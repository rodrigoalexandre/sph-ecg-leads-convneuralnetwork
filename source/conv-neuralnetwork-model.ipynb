{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47353fcc-74e3-426e-a724-6b1d1bf32b2f",
   "metadata": {},
   "source": [
    "#### Train a Convolutional Neural Network model for predicting cardiac arrhythmias."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdbe460f",
   "metadata": {},
   "source": [
    "#### Import necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "077b6c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import time\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import scipy.signal as signal\n",
    "from collections import Counter\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import Sequential\n",
    "from keras.layers import BatchNormalization, Dense, Flatten, Input, Conv1D, MaxPooling1D\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from tensorflow.keras.optimizers import Adam\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42175028",
   "metadata": {},
   "source": [
    "#### Define a function to load the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b43ccbdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_dataset(lead_name):\n",
    "    # Loads the dataset using only the chosen lead.\n",
    "    # Parameters:\n",
    "    #    lead_name: Lead to be used.\n",
    "    # Return:\n",
    "    #    Dataframe loaded (Pandas dataframe)\n",
    "    df = None\n",
    "    column_names = [\"idx\", \"ecg_id\", lead_name, \"arrhythmia_code\"]\n",
    "    dtypes = {\"ecg_id\": \"str\", lead_name : \"float16\", \"arrhythmia_code\" : \"int16\"}\n",
    "    try:\n",
    "        print(\"\\nStart loading CSV file...\")\n",
    "        df = pd.read_csv(\"../dataset/csv_files/ecg_sph_dataset.csv\", sep=\"|\", dtype = dtypes, usecols = column_names)\n",
    "        print(\"Finish loading CSV file.\")\n",
    "    except Exception as e:\n",
    "        print(\"\\nFail to load CSV file.\")\n",
    "        print(\"Error: {}\".format(e))\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3579e10b",
   "metadata": {},
   "source": [
    "#### Build a helper function to convert the records to the required format to perform a time series processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "859f6e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "number_of_steps = 1250\n",
    "number_of_features = 1\n",
    "number_of_classes = 32\n",
    "\n",
    "def get_new_columns_order(column_names_array):\n",
    "    column_idx_count = 0\n",
    "    new_array = np.zeros(len(column_names_array), dtype = int)\n",
    "    for column_idx in range(0, (number_of_steps * 4)):\n",
    "        for column_idx_2 in range(0, number_of_features):\n",
    "            new_array[column_idx + column_idx_2 * (number_of_steps * 4)] = column_idx_count\n",
    "            column_idx_count += 1\n",
    "    return new_array\n",
    "\n",
    "def build_time_window_structure(df, lead_name):\n",
    "    # Splits the dataset into \"time windows\" to be used as a time series.\n",
    "    # The function groups each 125 dataset records (CSV lines) into one record.\n",
    "    # Parameters:\n",
    "    #    df: Dataframe to be splitted.\n",
    "    #    lead_name: Lead to be used.\n",
    "    # Return:\n",
    "    #    All time windows (np.array)\n",
    "    #    All target values (np.array)\n",
    "    print(\"\\nStarting build_time_window_structure function...\")\n",
    "    df[\"idx\"] = df[\"idx\"] % (number_of_steps * 4)\n",
    "    df_aux = df.pivot_table(index = \"ecg_id\", columns = \"idx\", values = [lead_name], aggfunc = \"sum\")\n",
    "    new_columns = get_new_columns_order(df_aux.columns.values)\n",
    "    df_aux.columns = list(new_columns)\n",
    "    sorted_columns = sorted(df_aux.columns)\n",
    "    df_modified = df_aux[sorted_columns]\n",
    "    X_array = df_modified.values\n",
    "    y_array = df[\"arrhythmia_code\"].values\n",
    "    y_array = y_array[::(number_of_steps * 4)]\n",
    "    # Resample sample frequency to 125 hz.\n",
    "    fs_original = 500 # Original frequency (Hz)\n",
    "    fs_new = 125 # New frequency (Hz)\n",
    "    downsampling_factor = int(fs_original / fs_new)\n",
    "    nyquist_rate = fs_original / 2.0  # Nyquist rate\n",
    "    cutoff_freq = fs_new / 2.0  # Cut off rate\n",
    "    b, a = signal.butter(4, cutoff_freq / nyquist_rate, btype = \"low\")\n",
    "    X_array_filtered = signal.filtfilt(b, a, X_array, axis = 1)\n",
    "    X_array_125hz = X_array_filtered[:, ::downsampling_factor]\n",
    "    print(\"\\nShape of features: \", X_array_125hz.shape)\n",
    "    print(\"Quantity os samples (labels): \", len(y_array))\n",
    "    print(\"\\nFinishing build_time_window_structure function.\")\n",
    "    return X_array_125hz, y_array\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d50c0a6-624d-4e06-b5c3-438e3909aea8",
   "metadata": {},
   "source": [
    "#### Define a function to remove classes with less than 6 samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4b49334-6e2d-4a4e-899f-6ab4aceed629",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def remove_classes_with_less_samples(X_array, y_array):\n",
    "    # Remove classes with less than 6 samples.\n",
    "    # Parameters:\n",
    "    #    X_array: array of features.\n",
    "    #    y_array: array of targets.\n",
    "    # Return:\n",
    "    #    Array of features (np.array)\n",
    "    #    Array of targets (np.array)\n",
    "\n",
    "    # Remove samples belonging to diagnostics 31, 37, 84, 87, 102, 143, 148, and 152 because these classes have less than 6 samples (SMOTE restriction).\n",
    "    print(\"\\nRemove classes with less than 6 samples.\")\n",
    "    removed_idx = np.where(np.isin(y_array, [31, 37, 84, 87, 102, 143, 148, 152]))[0]\n",
    "    X_array = np.delete(X_array, removed_idx, axis = 0)\n",
    "    y_array = np.delete(y_array, removed_idx, axis = 0)\n",
    "    number_of_classes = 32\n",
    "    # Generate a class number for each diagnostic code and replace y_array values.\n",
    "    sorted_codes = sorted(set(y_array))\n",
    "    dict_aux = {}\n",
    "    for classes_idx in range(0, number_of_classes):\n",
    "        dict_aux[classes_idx] = sorted_codes[classes_idx]\n",
    "        y_array = [classes_idx if elem == sorted_codes[classes_idx] else elem for elem in y_array]\n",
    "    y_array = np.array(y_array)\n",
    "    print(\"\\nShow classes identification:\")\n",
    "    for key, value in dict_aux.items():\n",
    "        print(f\"Class: {key} - Arrhythmia code: {value}\")\n",
    "    # Check for dataset balance.\n",
    "    diagnostic_classes, count = np.unique(y_array, return_counts = True)\n",
    "    percentage_by_class = [(i * 100 / np.sum(count)) for i in count]\n",
    "    category_count = list(zip(diagnostic_classes, count, percentage_by_class))\n",
    "    category_count.sort(key = lambda x: x[1], reverse = True)\n",
    "    print(\"\\nCheck for dataset balance:\")\n",
    "    for diagnostic_classes, count, percentage_by_class in category_count:\n",
    "        print(f\"Class = {diagnostic_classes:3.0f}   Qty = {count:8.0f}   Percentage = {percentage_by_class:2.2f} %\")\n",
    "    return X_array, y_array\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf2c820a-3ac1-4da5-ae09-dd378637c4be",
   "metadata": {},
   "source": [
    "#### Define a function for training a CNN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19d8f38d-7cd0-4ddc-80d9-b11e94edb868",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def apply_upsampling(X_array, y_array):\n",
    "    # Apply SMOTE to balance the dataset.\n",
    "    # Parameters:\n",
    "    #    X_array (np.array): array of features values.\n",
    "    #    y_array (np.array): array of target values.\n",
    "    # Return:\n",
    "    #    X_array (np.array): array of features values.\n",
    "    #    y_array (np.array): array of target values.\n",
    "    print(\"\\nGenerating upsampling using SMOTE...\")\n",
    "    min_samples = min([sum(y_array == c) for c in set(y_array)])\n",
    "    k_neighbors = min(5, min_samples - 1)\n",
    "    smote = SMOTE(sampling_strategy = \"auto\", k_neighbors = k_neighbors, random_state = 42)\n",
    "    X_array_res, y_array_res = smote.fit_resample(X_array, y_array)\n",
    "    # Check for balance.\n",
    "    diagnostic_codes, count = np.unique(y_array, return_counts = True)\n",
    "    percentage_by_codes = [(i * 100 / np.sum(count)) for i in count]\n",
    "    category_count = list(zip(diagnostic_codes, count, percentage_by_codes))\n",
    "    category_count.sort(key = lambda x: x[1], reverse = True)\n",
    "    # Use 6250 samples of each class for training.\n",
    "    dict_samples_per_class = {}\n",
    "    nr_samples_per_class = 6250\n",
    "    for category_ids, count, percentage_of_categories in category_count:\n",
    "        dict_samples_per_class[category_ids] = nr_samples_per_class\n",
    "    rus = RandomUnderSampler(sampling_strategy = dict_samples_per_class, random_state = 42)\n",
    "    X_train, y_train = rus.fit_resample(X_array_res, y_array_res)\n",
    "    print(\"{} samples after upsampling.\".format(len(y_train)))\n",
    "    print(f\"Class distribution for training after upsampling: {Counter(y_train)}\")\n",
    "    print(\"Finishing upsampling.\\n\")\n",
    "    return X_train, y_train\n",
    "\n",
    "def train_cnn_model(cnn_model, X, y, num_epochs, batch_size, validation_split, model_cfg_file):\n",
    "    # Train a CNN model.\n",
    "    # Parameters:\n",
    "    #    cnn_model (Sequential): model to be trained.\n",
    "    #    X (np.array): array of features values.\n",
    "    #    y (np.array): array of target values.\n",
    "    #    nun_folds (int): number of folds.\n",
    "    #    num_epochs (int): number of epochs of training.\n",
    "    #    batch_size (int): batch size.\n",
    "    #    validation_split (float): percentage of instances for validation set.\n",
    "    #    model_cfg_file (str): file to save the configuration model.\n",
    "    # Returns:\n",
    "    #    history (History object): history of training metrics.\n",
    "\n",
    "    # Defining the number of folds (k-Fold).\n",
    "    skf = StratifiedKFold(n_splits = 10, shuffle = True, random_state = 42)\n",
    "    start_time = time.time()\n",
    "    X_train = X\n",
    "    y_train = y\n",
    "    es = EarlyStopping(monitor = \"val_loss\", mode = \"min\", verbose = 1, patience = 10, restore_best_weights = True)\n",
    "    train_accuracy_by_fold = []\n",
    "    test_accuracy_by_fold = []\n",
    "    fold_number = 1\n",
    "    history_by_fold = []\n",
    "    y_predclass_for_report = []\n",
    "    y_testclass_for_report = []\n",
    "    print(\"\\nStarting training...\")\n",
    "    rb_scaler = PowerTransformer()\n",
    "    for train_index, test_index in skf.split(X_train, y_train):\n",
    "        print(\"\\nTraining fold {}\".format(fold_number))\n",
    "        X_train_fold, y_train_fold = apply_upsampling(X_train[train_index], y_train[train_index])\n",
    "        X_train_fold = rb_scaler.fit_transform(X_train_fold)\n",
    "        X_train_fold = X_train_fold.reshape((X_train_fold.shape[0], number_of_steps, number_of_features))\n",
    "        history = cnn_model.fit(X_train_fold, y_train_fold, validation_split = validation_split,\n",
    "                                epochs = num_epochs, batch_size = batch_size, \n",
    "                                verbose = 1, callbacks = [es])\n",
    "        _, train_accuracy = cnn_model.evaluate(X_train_fold, y_train_fold, verbose = 0)\n",
    "        X_test_fold = rb_scaler.transform(X_train[test_index])\n",
    "        X_test_reshaped = X_test_fold.reshape((X_test_fold.shape[0], number_of_steps, number_of_features))\n",
    "        _, test_accuracy = cnn_model.evaluate(X_test_reshaped, y_train[test_index], verbose = 0)\n",
    "        train_accuracy_by_fold.append(train_accuracy)\n",
    "        test_accuracy_by_fold.append(test_accuracy)\n",
    "        y_predclass_for_report.extend(np.argmax(cnn_model.predict(X_test_reshaped), axis = 1))\n",
    "        y_testclass_for_report.extend(y_train[test_index])\n",
    "        history_by_fold.append(history)\n",
    "        fold_number += 1\n",
    "    cnn_model.save(\"../modelconfig/\" + model_cfg_file)\n",
    "    elapsed_seconds = time.time() - start_time\n",
    "    print(\"\\nTime taken for training: \", time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_seconds)))\n",
    "    print(\"\\n\")\n",
    "    # Show metrics.\n",
    "    for i in range(len(train_accuracy_by_fold)):\n",
    "        print(\"Fold {} - Train Accuracy {:.4f} - Test Accuracy {:.4f}\".format((i + 1), train_accuracy_by_fold[i],\n",
    "                                                                              test_accuracy_by_fold[i]))\n",
    "    print(\"\\nMean Train Accuracy: {:.4f} - Std: {:.4f} \".format(np.mean(train_accuracy_by_fold),\n",
    "                                                                np.std(train_accuracy_by_fold)))\n",
    "    print(\"Mean Test Accuracy: {:.4f} - Std: {:.4f} \".format(np.mean(test_accuracy_by_fold),\n",
    "                                                             np.std(test_accuracy_by_fold)))\n",
    "    print(\"\\nEvaluate other metrics:\")\n",
    "    print(classification_report(y_testclass_for_report, y_predclass_for_report, zero_division = 0))\n",
    "    return history_by_fold\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a8dbf24-0ece-49d0-9644-a3a90865f0dd",
   "metadata": {},
   "source": [
    "#### Define a function to build a version 1 of CNN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "acff693b-6d32-4b69-8133-0fca13f2b035",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_v1():\n",
    "    act_fuction = \"relu\"\n",
    "    k_init = \"he_uniform\"\n",
    "    model = Sequential()\n",
    "    model.add(Input((number_of_steps, number_of_features)))\n",
    "    model.add(Conv1D(filters = 8, kernel_size = 3, activation = act_fuction,\n",
    "                     kernel_initializer = k_init))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv1D(filters = 8, kernel_size = 3, activation = act_fuction, \n",
    "                     kernel_initializer = k_init))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling1D(pool_size = 2))\n",
    "    model.add(Conv1D(filters = 16, kernel_size = 5, activation = act_fuction, \n",
    "                     kernel_initializer = k_init))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv1D(filters = 16, kernel_size = 5, activation = act_fuction, \n",
    "                     kernel_initializer = k_init))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling1D(pool_size = 2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation = act_fuction, kernel_initializer = k_init))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(number_of_classes, activation = 'softmax'))\n",
    "    opt = Adam(learning_rate = 0.001)\n",
    "    model.summary()\n",
    "    model.compile(loss = \"sparse_categorical_crossentropy\", optimizer = opt, metrics = [\"accuracy\"])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d861ca2b-79bb-4d8a-8216-3f1d80e985ba",
   "metadata": {},
   "source": [
    "#### Lead I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b3ad224-b9d5-4158-ba39-3f046d59ecdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Start loading CSV file...\n",
      "Finish loading CSV file.\n",
      "\n",
      "Starting build_time_window_structure function...\n",
      "\n",
      "Shape of features:  (25770, 1250)\n",
      "Quantity os samples (labels):  25770\n",
      "\n",
      "Finishing build_time_window_structure function.\n",
      "\n",
      "Remove classes with less than 6 samples.\n",
      "\n",
      "Show classes identification:\n",
      "Class: 0 - Arrhythmia code: 1\n",
      "Class: 1 - Arrhythmia code: 21\n",
      "Class: 2 - Arrhythmia code: 22\n",
      "Class: 3 - Arrhythmia code: 23\n",
      "Class: 4 - Arrhythmia code: 30\n",
      "Class: 5 - Arrhythmia code: 36\n",
      "Class: 6 - Arrhythmia code: 50\n",
      "Class: 7 - Arrhythmia code: 51\n",
      "Class: 8 - Arrhythmia code: 54\n",
      "Class: 9 - Arrhythmia code: 60\n",
      "Class: 10 - Arrhythmia code: 80\n",
      "Class: 11 - Arrhythmia code: 82\n",
      "Class: 12 - Arrhythmia code: 83\n",
      "Class: 13 - Arrhythmia code: 88\n",
      "Class: 14 - Arrhythmia code: 101\n",
      "Class: 15 - Arrhythmia code: 104\n",
      "Class: 16 - Arrhythmia code: 105\n",
      "Class: 17 - Arrhythmia code: 106\n",
      "Class: 18 - Arrhythmia code: 108\n",
      "Class: 19 - Arrhythmia code: 120\n",
      "Class: 20 - Arrhythmia code: 121\n",
      "Class: 21 - Arrhythmia code: 125\n",
      "Class: 22 - Arrhythmia code: 140\n",
      "Class: 23 - Arrhythmia code: 142\n",
      "Class: 24 - Arrhythmia code: 145\n",
      "Class: 25 - Arrhythmia code: 146\n",
      "Class: 26 - Arrhythmia code: 147\n",
      "Class: 27 - Arrhythmia code: 155\n",
      "Class: 28 - Arrhythmia code: 160\n",
      "Class: 29 - Arrhythmia code: 161\n",
      "Class: 30 - Arrhythmia code: 165\n",
      "Class: 31 - Arrhythmia code: 166\n",
      "\n",
      "Check for dataset balance:\n",
      "Class =   0   Qty =    13905   Percentage = 54.01 %\n",
      "Class =   2   Qty =     2659   Percentage = 10.33 %\n",
      "Class =  26   Qty =     1334   Percentage = 5.18 %\n",
      "Class =   3   Qty =     1123   Percentage = 4.36 %\n",
      "Class =  24   Qty =     1045   Percentage = 4.06 %\n",
      "Class =  16   Qty =      917   Percentage = 3.56 %\n",
      "Class =   9   Qty =      786   Percentage = 3.05 %\n",
      "Class =   1   Qty =      723   Percentage = 2.81 %\n",
      "Class =   6   Qty =      663   Percentage = 2.58 %\n",
      "Class =  25   Qty =      540   Percentage = 2.10 %\n",
      "Class =  17   Qty =      473   Percentage = 1.84 %\n",
      "Class =   4   Qty =      384   Percentage = 1.49 %\n",
      "Class =  21   Qty =      201   Percentage = 0.78 %\n",
      "Class =  19   Qty =      122   Percentage = 0.47 %\n",
      "Class =  20   Qty =      111   Percentage = 0.43 %\n",
      "Class =  11   Qty =       98   Percentage = 0.38 %\n",
      "Class =  23   Qty =       96   Percentage = 0.37 %\n",
      "Class =   7   Qty =       94   Percentage = 0.37 %\n",
      "Class =  14   Qty =       77   Percentage = 0.30 %\n",
      "Class =  29   Qty =       77   Percentage = 0.30 %\n",
      "Class =  30   Qty =       64   Percentage = 0.25 %\n",
      "Class =  15   Qty =       62   Percentage = 0.24 %\n",
      "Class =   5   Qty =       44   Percentage = 0.17 %\n",
      "Class =  28   Qty =       35   Percentage = 0.14 %\n",
      "Class =  27   Qty =       28   Percentage = 0.11 %\n",
      "Class =  18   Qty =       22   Percentage = 0.09 %\n",
      "Class =  13   Qty =       20   Percentage = 0.08 %\n",
      "Class =   8   Qty =       12   Percentage = 0.05 %\n",
      "Class =  10   Qty =        9   Percentage = 0.03 %\n",
      "Class =  12   Qty =        8   Percentage = 0.03 %\n",
      "Class =  22   Qty =        7   Percentage = 0.03 %\n",
      "Class =  31   Qty =        7   Percentage = 0.03 %\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1248</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)        │            <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1248</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)        │            <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1246</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)        │           <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1246</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)        │            <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">623</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">619</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)        │           <span style=\"color: #00af00; text-decoration-color: #00af00\">656</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_2           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">619</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)        │            <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">615</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)        │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,296</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_3           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">615</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)        │            <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">307</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4912</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">628,864</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_4           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,128</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv1d (\u001b[38;5;33mConv1D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1248\u001b[0m, \u001b[38;5;34m8\u001b[0m)        │            \u001b[38;5;34m32\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1248\u001b[0m, \u001b[38;5;34m8\u001b[0m)        │            \u001b[38;5;34m32\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_1 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1246\u001b[0m, \u001b[38;5;34m8\u001b[0m)        │           \u001b[38;5;34m200\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_1           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1246\u001b[0m, \u001b[38;5;34m8\u001b[0m)        │            \u001b[38;5;34m32\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d (\u001b[38;5;33mMaxPooling1D\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m623\u001b[0m, \u001b[38;5;34m8\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_2 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m619\u001b[0m, \u001b[38;5;34m16\u001b[0m)        │           \u001b[38;5;34m656\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_2           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m619\u001b[0m, \u001b[38;5;34m16\u001b[0m)        │            \u001b[38;5;34m64\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_3 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m615\u001b[0m, \u001b[38;5;34m16\u001b[0m)        │         \u001b[38;5;34m1,296\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_3           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m615\u001b[0m, \u001b[38;5;34m16\u001b[0m)        │            \u001b[38;5;34m64\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_1 (\u001b[38;5;33mMaxPooling1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m307\u001b[0m, \u001b[38;5;34m16\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4912\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │       \u001b[38;5;34m628,864\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_4           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │           \u001b[38;5;34m512\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │         \u001b[38;5;34m4,128\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">635,880</span> (2.43 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m635,880\u001b[0m (2.43 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">635,528</span> (2.42 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m635,528\u001b[0m (2.42 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">352</span> (1.38 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m352\u001b[0m (1.38 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting training...\n",
      "\n",
      "Training fold 1\n",
      "\n",
      "Generating upsampling using SMOTE...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\DeveloperTools\\python\\3.11.0\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:737: UserWarning: The least populated class in y has only 7 members, which is less than n_splits=10.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200000 samples after upsampling.\n",
      "Class distribution for training after upsampling: Counter({np.int64(0): 6250, np.int64(1): 6250, np.int64(2): 6250, np.int64(3): 6250, np.int64(4): 6250, np.int64(5): 6250, np.int64(6): 6250, np.int64(7): 6250, np.int64(8): 6250, np.int64(9): 6250, np.int64(10): 6250, np.int64(11): 6250, np.int64(12): 6250, np.int64(13): 6250, np.int64(14): 6250, np.int64(15): 6250, np.int64(16): 6250, np.int64(17): 6250, np.int64(18): 6250, np.int64(19): 6250, np.int64(20): 6250, np.int64(21): 6250, np.int64(22): 6250, np.int64(23): 6250, np.int64(24): 6250, np.int64(25): 6250, np.int64(26): 6250, np.int64(27): 6250, np.int64(28): 6250, np.int64(29): 6250, np.int64(30): 6250, np.int64(31): 6250})\n",
      "Finishing upsampling.\n",
      "\n",
      "Epoch 1/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m113s\u001b[0m 17ms/step - accuracy: 0.8512 - loss: 0.5808 - val_accuracy: 1.0000 - val_loss: 1.0015e-04\n",
      "Epoch 2/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 17ms/step - accuracy: 0.9741 - loss: 0.0834 - val_accuracy: 1.0000 - val_loss: 2.6412e-05\n",
      "Epoch 3/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m112s\u001b[0m 18ms/step - accuracy: 0.9874 - loss: 0.0405 - val_accuracy: 1.0000 - val_loss: 1.2815e-04\n",
      "Epoch 4/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m116s\u001b[0m 19ms/step - accuracy: 0.9917 - loss: 0.0269 - val_accuracy: 1.0000 - val_loss: 1.3747e-06\n",
      "Epoch 5/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m116s\u001b[0m 19ms/step - accuracy: 0.9940 - loss: 0.0190 - val_accuracy: 1.0000 - val_loss: 4.3434e-07\n",
      "Epoch 6/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m110s\u001b[0m 18ms/step - accuracy: 0.9945 - loss: 0.0167 - val_accuracy: 1.0000 - val_loss: 3.1560e-07\n",
      "Epoch 7/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m116s\u001b[0m 19ms/step - accuracy: 0.9950 - loss: 0.0159 - val_accuracy: 1.0000 - val_loss: 6.9439e-08\n",
      "Epoch 8/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m119s\u001b[0m 19ms/step - accuracy: 0.9959 - loss: 0.0121 - val_accuracy: 1.0000 - val_loss: 5.3769e-06\n",
      "Epoch 9/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m126s\u001b[0m 20ms/step - accuracy: 0.9967 - loss: 0.0102 - val_accuracy: 1.0000 - val_loss: 2.6037e-06\n",
      "Epoch 10/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m126s\u001b[0m 20ms/step - accuracy: 0.9968 - loss: 0.0100 - val_accuracy: 1.0000 - val_loss: 9.5248e-08\n",
      "Epoch 11/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m117s\u001b[0m 19ms/step - accuracy: 0.9970 - loss: 0.0094 - val_accuracy: 1.0000 - val_loss: 2.8264e-07\n",
      "Epoch 12/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m122s\u001b[0m 20ms/step - accuracy: 0.9970 - loss: 0.0094 - val_accuracy: 1.0000 - val_loss: 9.3659e-06\n",
      "Epoch 13/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m121s\u001b[0m 20ms/step - accuracy: 0.9973 - loss: 0.0086 - val_accuracy: 1.0000 - val_loss: 2.4438e-09\n",
      "Epoch 14/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m116s\u001b[0m 19ms/step - accuracy: 0.9974 - loss: 0.0079 - val_accuracy: 1.0000 - val_loss: 2.0283e-07\n",
      "Epoch 15/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m120s\u001b[0m 19ms/step - accuracy: 0.9975 - loss: 0.0079 - val_accuracy: 1.0000 - val_loss: 1.1325e-09\n",
      "Epoch 16/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m113s\u001b[0m 18ms/step - accuracy: 0.9977 - loss: 0.0070 - val_accuracy: 1.0000 - val_loss: 9.5149e-07\n",
      "Epoch 17/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m110s\u001b[0m 18ms/step - accuracy: 0.9983 - loss: 0.0062 - val_accuracy: 1.0000 - val_loss: 9.7752e-09\n",
      "Epoch 18/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m115s\u001b[0m 19ms/step - accuracy: 0.9978 - loss: 0.0069 - val_accuracy: 1.0000 - val_loss: 1.3709e-09\n",
      "Epoch 19/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m119s\u001b[0m 19ms/step - accuracy: 0.9982 - loss: 0.0058 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 20/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m122s\u001b[0m 20ms/step - accuracy: 0.9983 - loss: 0.0057 - val_accuracy: 1.0000 - val_loss: 9.8347e-09\n",
      "Epoch 21/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m130s\u001b[0m 21ms/step - accuracy: 0.9986 - loss: 0.0046 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 22/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m131s\u001b[0m 21ms/step - accuracy: 0.9981 - loss: 0.0064 - val_accuracy: 1.0000 - val_loss: 4.1723e-10\n",
      "Epoch 23/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m124s\u001b[0m 20ms/step - accuracy: 0.9985 - loss: 0.0048 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 24/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m147s\u001b[0m 24ms/step - accuracy: 0.9983 - loss: 0.0055 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 25/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m161s\u001b[0m 26ms/step - accuracy: 0.9985 - loss: 0.0050 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 26/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m124s\u001b[0m 20ms/step - accuracy: 0.9985 - loss: 0.0047 - val_accuracy: 1.0000 - val_loss: 4.1723e-10\n",
      "Epoch 27/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m128s\u001b[0m 21ms/step - accuracy: 0.9987 - loss: 0.0046 - val_accuracy: 1.0000 - val_loss: 3.5167e-08\n",
      "Epoch 28/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m138s\u001b[0m 22ms/step - accuracy: 0.9986 - loss: 0.0051 - val_accuracy: 1.0000 - val_loss: 1.1921e-10\n",
      "Epoch 29/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m128s\u001b[0m 21ms/step - accuracy: 0.9986 - loss: 0.0046 - val_accuracy: 1.0000 - val_loss: 1.1921e-10\n",
      "Epoch 29: early stopping\n",
      "Restoring model weights from the end of the best epoch: 19.\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step\n",
      "\n",
      "Training fold 2\n",
      "\n",
      "Generating upsampling using SMOTE...\n",
      "200000 samples after upsampling.\n",
      "Class distribution for training after upsampling: Counter({np.int64(0): 6250, np.int64(1): 6250, np.int64(2): 6250, np.int64(3): 6250, np.int64(4): 6250, np.int64(5): 6250, np.int64(6): 6250, np.int64(7): 6250, np.int64(8): 6250, np.int64(9): 6250, np.int64(10): 6250, np.int64(11): 6250, np.int64(12): 6250, np.int64(13): 6250, np.int64(14): 6250, np.int64(15): 6250, np.int64(16): 6250, np.int64(17): 6250, np.int64(18): 6250, np.int64(19): 6250, np.int64(20): 6250, np.int64(21): 6250, np.int64(22): 6250, np.int64(23): 6250, np.int64(24): 6250, np.int64(25): 6250, np.int64(26): 6250, np.int64(27): 6250, np.int64(28): 6250, np.int64(29): 6250, np.int64(30): 6250, np.int64(31): 6250})\n",
      "Finishing upsampling.\n",
      "\n",
      "Epoch 1/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m130s\u001b[0m 21ms/step - accuracy: 0.9738 - loss: 0.1530 - val_accuracy: 1.0000 - val_loss: 1.6498e-07\n",
      "Epoch 2/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m126s\u001b[0m 20ms/step - accuracy: 0.9948 - loss: 0.0153 - val_accuracy: 1.0000 - val_loss: 7.1526e-10\n",
      "Epoch 3/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m134s\u001b[0m 22ms/step - accuracy: 0.9971 - loss: 0.0092 - val_accuracy: 1.0000 - val_loss: 5.6028e-09\n",
      "Epoch 4/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m121s\u001b[0m 20ms/step - accuracy: 0.9978 - loss: 0.0073 - val_accuracy: 1.0000 - val_loss: 8.7440e-08\n",
      "Epoch 5/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m121s\u001b[0m 20ms/step - accuracy: 0.9979 - loss: 0.0072 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 6/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m130s\u001b[0m 21ms/step - accuracy: 0.9982 - loss: 0.0056 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 7/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m125s\u001b[0m 20ms/step - accuracy: 0.9985 - loss: 0.0057 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 8/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m120s\u001b[0m 19ms/step - accuracy: 0.9983 - loss: 0.0055 - val_accuracy: 1.0000 - val_loss: 4.1723e-10\n",
      "Epoch 9/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m126s\u001b[0m 20ms/step - accuracy: 0.9983 - loss: 0.0054 - val_accuracy: 1.0000 - val_loss: 6.6226e-05\n",
      "Epoch 10/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m133s\u001b[0m 21ms/step - accuracy: 0.9984 - loss: 0.0056 - val_accuracy: 1.0000 - val_loss: 2.6226e-08\n",
      "Epoch 11/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m121s\u001b[0m 20ms/step - accuracy: 0.9988 - loss: 0.0046 - val_accuracy: 1.0000 - val_loss: 4.7505e-08\n",
      "Epoch 12/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m123s\u001b[0m 20ms/step - accuracy: 0.9988 - loss: 0.0044 - val_accuracy: 1.0000 - val_loss: 1.1921e-10\n",
      "Epoch 13/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m130s\u001b[0m 21ms/step - accuracy: 0.9988 - loss: 0.0038 - val_accuracy: 1.0000 - val_loss: 1.7941e-08\n",
      "Epoch 14/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m136s\u001b[0m 22ms/step - accuracy: 0.9989 - loss: 0.0040 - val_accuracy: 1.0000 - val_loss: 1.8477e-09\n",
      "Epoch 15/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m121s\u001b[0m 20ms/step - accuracy: 0.9988 - loss: 0.0044 - val_accuracy: 1.0000 - val_loss: 2.2650e-09\n",
      "Epoch 15: early stopping\n",
      "Restoring model weights from the end of the best epoch: 5.\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\n",
      "Training fold 3\n",
      "\n",
      "Generating upsampling using SMOTE...\n",
      "200000 samples after upsampling.\n",
      "Class distribution for training after upsampling: Counter({np.int64(0): 6250, np.int64(1): 6250, np.int64(2): 6250, np.int64(3): 6250, np.int64(4): 6250, np.int64(5): 6250, np.int64(6): 6250, np.int64(7): 6250, np.int64(8): 6250, np.int64(9): 6250, np.int64(10): 6250, np.int64(11): 6250, np.int64(12): 6250, np.int64(13): 6250, np.int64(14): 6250, np.int64(15): 6250, np.int64(16): 6250, np.int64(17): 6250, np.int64(18): 6250, np.int64(19): 6250, np.int64(20): 6250, np.int64(21): 6250, np.int64(22): 6250, np.int64(23): 6250, np.int64(24): 6250, np.int64(25): 6250, np.int64(26): 6250, np.int64(27): 6250, np.int64(28): 6250, np.int64(29): 6250, np.int64(30): 6250, np.int64(31): 6250})\n",
      "Finishing upsampling.\n",
      "\n",
      "Epoch 1/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m128s\u001b[0m 21ms/step - accuracy: 0.9906 - loss: 0.0374 - val_accuracy: 1.0000 - val_loss: 4.2736e-08\n",
      "Epoch 2/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m131s\u001b[0m 21ms/step - accuracy: 0.9970 - loss: 0.0110 - val_accuracy: 1.0000 - val_loss: 1.2115e-06\n",
      "Epoch 3/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m133s\u001b[0m 22ms/step - accuracy: 0.9981 - loss: 0.0069 - val_accuracy: 1.0000 - val_loss: 6.6161e-09\n",
      "Epoch 4/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m122s\u001b[0m 20ms/step - accuracy: 0.9984 - loss: 0.0061 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 5/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m129s\u001b[0m 21ms/step - accuracy: 0.9984 - loss: 0.0053 - val_accuracy: 1.0000 - val_loss: 2.6566e-06\n",
      "Epoch 6/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m136s\u001b[0m 22ms/step - accuracy: 0.9987 - loss: 0.0049 - val_accuracy: 1.0000 - val_loss: 1.5497e-08\n",
      "Epoch 7/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m137s\u001b[0m 22ms/step - accuracy: 0.9986 - loss: 0.0051 - val_accuracy: 1.0000 - val_loss: 2.9802e-10\n",
      "Epoch 8/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m128s\u001b[0m 21ms/step - accuracy: 0.9989 - loss: 0.0040 - val_accuracy: 1.0000 - val_loss: 8.3714e-07\n",
      "Epoch 9/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m134s\u001b[0m 22ms/step - accuracy: 0.9988 - loss: 0.0040 - val_accuracy: 1.0000 - val_loss: 3.0398e-09\n",
      "Epoch 10/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 23ms/step - accuracy: 0.9988 - loss: 0.0044 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 11/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m126s\u001b[0m 20ms/step - accuracy: 0.9989 - loss: 0.0040 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 12/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m134s\u001b[0m 22ms/step - accuracy: 0.9986 - loss: 0.0048 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 13/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 23ms/step - accuracy: 0.9989 - loss: 0.0040 - val_accuracy: 1.0000 - val_loss: 1.9670e-09\n",
      "Epoch 14/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m128s\u001b[0m 21ms/step - accuracy: 0.9988 - loss: 0.0034 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 14: early stopping\n",
      "Restoring model weights from the end of the best epoch: 4.\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\n",
      "Training fold 4\n",
      "\n",
      "Generating upsampling using SMOTE...\n",
      "200000 samples after upsampling.\n",
      "Class distribution for training after upsampling: Counter({np.int64(0): 6250, np.int64(1): 6250, np.int64(2): 6250, np.int64(3): 6250, np.int64(4): 6250, np.int64(5): 6250, np.int64(6): 6250, np.int64(7): 6250, np.int64(8): 6250, np.int64(9): 6250, np.int64(10): 6250, np.int64(11): 6250, np.int64(12): 6250, np.int64(13): 6250, np.int64(14): 6250, np.int64(15): 6250, np.int64(16): 6250, np.int64(17): 6250, np.int64(18): 6250, np.int64(19): 6250, np.int64(20): 6250, np.int64(21): 6250, np.int64(22): 6250, np.int64(23): 6250, np.int64(24): 6250, np.int64(25): 6250, np.int64(26): 6250, np.int64(27): 6250, np.int64(28): 6250, np.int64(29): 6250, np.int64(30): 6250, np.int64(31): 6250})\n",
      "Finishing upsampling.\n",
      "\n",
      "Epoch 1/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m133s\u001b[0m 21ms/step - accuracy: 0.9939 - loss: 0.0219 - val_accuracy: 1.0000 - val_loss: 3.5763e-10\n",
      "Epoch 2/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m137s\u001b[0m 22ms/step - accuracy: 0.9979 - loss: 0.0067 - val_accuracy: 1.0000 - val_loss: 1.4894e-06\n",
      "Epoch 3/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 23ms/step - accuracy: 0.9986 - loss: 0.0053 - val_accuracy: 1.0000 - val_loss: 3.9339e-08\n",
      "Epoch 4/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m131s\u001b[0m 21ms/step - accuracy: 0.9986 - loss: 0.0048 - val_accuracy: 1.0000 - val_loss: 6.1393e-09\n",
      "Epoch 5/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m135s\u001b[0m 22ms/step - accuracy: 0.9987 - loss: 0.0047 - val_accuracy: 1.0000 - val_loss: 2.8789e-08\n",
      "Epoch 6/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 23ms/step - accuracy: 0.9986 - loss: 0.0048 - val_accuracy: 1.0000 - val_loss: 5.9605e-11\n",
      "Epoch 7/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 23ms/step - accuracy: 0.9989 - loss: 0.0039 - val_accuracy: 1.0000 - val_loss: 3.1590e-09\n",
      "Epoch 8/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m134s\u001b[0m 22ms/step - accuracy: 0.9989 - loss: 0.0039 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 9/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 23ms/step - accuracy: 0.9989 - loss: 0.0039 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 10/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m134s\u001b[0m 22ms/step - accuracy: 0.9985 - loss: 0.0046 - val_accuracy: 1.0000 - val_loss: 1.7124e-07\n",
      "Epoch 11/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m136s\u001b[0m 22ms/step - accuracy: 0.9989 - loss: 0.0039 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 12/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 23ms/step - accuracy: 0.9988 - loss: 0.0044 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 13/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m153s\u001b[0m 25ms/step - accuracy: 0.9991 - loss: 0.0037 - val_accuracy: 1.0000 - val_loss: 1.9193e-08\n",
      "Epoch 14/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m136s\u001b[0m 22ms/step - accuracy: 0.9990 - loss: 0.0043 - val_accuracy: 1.0000 - val_loss: 2.3842e-10\n",
      "Epoch 15/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 23ms/step - accuracy: 0.9990 - loss: 0.0032 - val_accuracy: 1.0000 - val_loss: 9.5367e-10\n",
      "Epoch 16/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m160s\u001b[0m 26ms/step - accuracy: 0.9989 - loss: 0.0031 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 17/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 23ms/step - accuracy: 0.9990 - loss: 0.0036 - val_accuracy: 1.0000 - val_loss: 9.0539e-08\n",
      "Epoch 18/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m146s\u001b[0m 24ms/step - accuracy: 0.9993 - loss: 0.0027 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 18: early stopping\n",
      "Restoring model weights from the end of the best epoch: 8.\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step\n",
      "\n",
      "Training fold 5\n",
      "\n",
      "Generating upsampling using SMOTE...\n",
      "200000 samples after upsampling.\n",
      "Class distribution for training after upsampling: Counter({np.int64(0): 6250, np.int64(1): 6250, np.int64(2): 6250, np.int64(3): 6250, np.int64(4): 6250, np.int64(5): 6250, np.int64(6): 6250, np.int64(7): 6250, np.int64(8): 6250, np.int64(9): 6250, np.int64(10): 6250, np.int64(11): 6250, np.int64(12): 6250, np.int64(13): 6250, np.int64(14): 6250, np.int64(15): 6250, np.int64(16): 6250, np.int64(17): 6250, np.int64(18): 6250, np.int64(19): 6250, np.int64(20): 6250, np.int64(21): 6250, np.int64(22): 6250, np.int64(23): 6250, np.int64(24): 6250, np.int64(25): 6250, np.int64(26): 6250, np.int64(27): 6250, np.int64(28): 6250, np.int64(29): 6250, np.int64(30): 6250, np.int64(31): 6250})\n",
      "Finishing upsampling.\n",
      "\n",
      "Epoch 1/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m151s\u001b[0m 24ms/step - accuracy: 0.9940 - loss: 0.0221 - val_accuracy: 1.0000 - val_loss: 3.0875e-08\n",
      "Epoch 2/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m154s\u001b[0m 25ms/step - accuracy: 0.9978 - loss: 0.0072 - val_accuracy: 1.0000 - val_loss: 5.3644e-10\n",
      "Epoch 3/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m159s\u001b[0m 26ms/step - accuracy: 0.9986 - loss: 0.0048 - val_accuracy: 1.0000 - val_loss: 6.1989e-09\n",
      "Epoch 4/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m175s\u001b[0m 28ms/step - accuracy: 0.9988 - loss: 0.0045 - val_accuracy: 1.0000 - val_loss: 1.1921e-09\n",
      "Epoch 5/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m154s\u001b[0m 25ms/step - accuracy: 0.9988 - loss: 0.0046 - val_accuracy: 1.0000 - val_loss: 2.4080e-08\n",
      "Epoch 6/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m163s\u001b[0m 26ms/step - accuracy: 0.9988 - loss: 0.0038 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 7/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m173s\u001b[0m 28ms/step - accuracy: 0.9990 - loss: 0.0034 - val_accuracy: 1.0000 - val_loss: 1.7881e-09\n",
      "Epoch 8/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m158s\u001b[0m 25ms/step - accuracy: 0.9990 - loss: 0.0039 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 9/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m172s\u001b[0m 28ms/step - accuracy: 0.9989 - loss: 0.0038 - val_accuracy: 1.0000 - val_loss: 8.9358e-05\n",
      "Epoch 10/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m179s\u001b[0m 29ms/step - accuracy: 0.9992 - loss: 0.0028 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 11/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m167s\u001b[0m 27ms/step - accuracy: 0.9990 - loss: 0.0035 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 12/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m184s\u001b[0m 30ms/step - accuracy: 0.9991 - loss: 0.0030 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 13/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m167s\u001b[0m 27ms/step - accuracy: 0.9991 - loss: 0.0032 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 14/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m184s\u001b[0m 30ms/step - accuracy: 0.9993 - loss: 0.0027 - val_accuracy: 1.0000 - val_loss: 2.9802e-10\n",
      "Epoch 15/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m180s\u001b[0m 29ms/step - accuracy: 0.9990 - loss: 0.0033 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 16/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m177s\u001b[0m 29ms/step - accuracy: 0.9990 - loss: 0.0038 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 16: early stopping\n",
      "Restoring model weights from the end of the best epoch: 6.\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step\n",
      "\n",
      "Training fold 6\n",
      "\n",
      "Generating upsampling using SMOTE...\n",
      "200000 samples after upsampling.\n",
      "Class distribution for training after upsampling: Counter({np.int64(0): 6250, np.int64(1): 6250, np.int64(2): 6250, np.int64(3): 6250, np.int64(4): 6250, np.int64(5): 6250, np.int64(6): 6250, np.int64(7): 6250, np.int64(8): 6250, np.int64(9): 6250, np.int64(10): 6250, np.int64(11): 6250, np.int64(12): 6250, np.int64(13): 6250, np.int64(14): 6250, np.int64(15): 6250, np.int64(16): 6250, np.int64(17): 6250, np.int64(18): 6250, np.int64(19): 6250, np.int64(20): 6250, np.int64(21): 6250, np.int64(22): 6250, np.int64(23): 6250, np.int64(24): 6250, np.int64(25): 6250, np.int64(26): 6250, np.int64(27): 6250, np.int64(28): 6250, np.int64(29): 6250, np.int64(30): 6250, np.int64(31): 6250})\n",
      "Finishing upsampling.\n",
      "\n",
      "Epoch 1/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m190s\u001b[0m 31ms/step - accuracy: 0.9959 - loss: 0.0135 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 2/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m169s\u001b[0m 27ms/step - accuracy: 0.9985 - loss: 0.0052 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 3/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m191s\u001b[0m 31ms/step - accuracy: 0.9988 - loss: 0.0040 - val_accuracy: 1.0000 - val_loss: 5.9605e-09\n",
      "Epoch 4/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m197s\u001b[0m 32ms/step - accuracy: 0.9990 - loss: 0.0049 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 5/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m203s\u001b[0m 33ms/step - accuracy: 0.9990 - loss: 0.0037 - val_accuracy: 1.0000 - val_loss: 5.6922e-08\n",
      "Epoch 6/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m201s\u001b[0m 32ms/step - accuracy: 0.9989 - loss: 0.0045 - val_accuracy: 1.0000 - val_loss: 1.7881e-10\n",
      "Epoch 7/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m180s\u001b[0m 29ms/step - accuracy: 0.9991 - loss: 0.0036 - val_accuracy: 1.0000 - val_loss: 7.5102e-09\n",
      "Epoch 8/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m190s\u001b[0m 31ms/step - accuracy: 0.9993 - loss: 0.0027 - val_accuracy: 1.0000 - val_loss: 3.1650e-08\n",
      "Epoch 9/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m193s\u001b[0m 31ms/step - accuracy: 0.9988 - loss: 0.0045 - val_accuracy: 1.0000 - val_loss: 6.2423e-07\n",
      "Epoch 10/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m188s\u001b[0m 30ms/step - accuracy: 0.9990 - loss: 0.0036 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 11/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m203s\u001b[0m 33ms/step - accuracy: 0.9991 - loss: 0.0036 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 11: early stopping\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step\n",
      "\n",
      "Training fold 7\n",
      "\n",
      "Generating upsampling using SMOTE...\n",
      "200000 samples after upsampling.\n",
      "Class distribution for training after upsampling: Counter({np.int64(0): 6250, np.int64(1): 6250, np.int64(2): 6250, np.int64(3): 6250, np.int64(4): 6250, np.int64(5): 6250, np.int64(6): 6250, np.int64(7): 6250, np.int64(8): 6250, np.int64(9): 6250, np.int64(10): 6250, np.int64(11): 6250, np.int64(12): 6250, np.int64(13): 6250, np.int64(14): 6250, np.int64(15): 6250, np.int64(16): 6250, np.int64(17): 6250, np.int64(18): 6250, np.int64(19): 6250, np.int64(20): 6250, np.int64(21): 6250, np.int64(22): 6250, np.int64(23): 6250, np.int64(24): 6250, np.int64(25): 6250, np.int64(26): 6250, np.int64(27): 6250, np.int64(28): 6250, np.int64(29): 6250, np.int64(30): 6250, np.int64(31): 6250})\n",
      "Finishing upsampling.\n",
      "\n",
      "Epoch 1/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m196s\u001b[0m 32ms/step - accuracy: 0.9971 - loss: 0.0102 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 2/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m207s\u001b[0m 33ms/step - accuracy: 0.9985 - loss: 0.0049 - val_accuracy: 1.0000 - val_loss: 1.1191e-05\n",
      "Epoch 3/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m213s\u001b[0m 34ms/step - accuracy: 0.9990 - loss: 0.0042 - val_accuracy: 1.0000 - val_loss: 1.5229e-07\n",
      "Epoch 4/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m198s\u001b[0m 32ms/step - accuracy: 0.9991 - loss: 0.0032 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 5/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m194s\u001b[0m 31ms/step - accuracy: 0.9988 - loss: 0.0038 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 6/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m196s\u001b[0m 32ms/step - accuracy: 0.9989 - loss: 0.0040 - val_accuracy: 1.0000 - val_loss: 1.1730e-07\n",
      "Epoch 7/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m211s\u001b[0m 34ms/step - accuracy: 0.9991 - loss: 0.0035 - val_accuracy: 1.0000 - val_loss: 9.3221e-08\n",
      "Epoch 8/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m196s\u001b[0m 32ms/step - accuracy: 0.9991 - loss: 0.0030 - val_accuracy: 1.0000 - val_loss: 2.4259e-08\n",
      "Epoch 9/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m203s\u001b[0m 33ms/step - accuracy: 0.9992 - loss: 0.0025 - val_accuracy: 1.0000 - val_loss: 4.7684e-10\n",
      "Epoch 10/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m194s\u001b[0m 31ms/step - accuracy: 0.9993 - loss: 0.0030 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 11/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m205s\u001b[0m 33ms/step - accuracy: 0.9991 - loss: 0.0029 - val_accuracy: 1.0000 - val_loss: 4.1723e-10\n",
      "Epoch 11: early stopping\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step\n",
      "\n",
      "Training fold 8\n",
      "\n",
      "Generating upsampling using SMOTE...\n",
      "200000 samples after upsampling.\n",
      "Class distribution for training after upsampling: Counter({np.int64(0): 6250, np.int64(1): 6250, np.int64(2): 6250, np.int64(3): 6250, np.int64(4): 6250, np.int64(5): 6250, np.int64(6): 6250, np.int64(7): 6250, np.int64(8): 6250, np.int64(9): 6250, np.int64(10): 6250, np.int64(11): 6250, np.int64(12): 6250, np.int64(13): 6250, np.int64(14): 6250, np.int64(15): 6250, np.int64(16): 6250, np.int64(17): 6250, np.int64(18): 6250, np.int64(19): 6250, np.int64(20): 6250, np.int64(21): 6250, np.int64(22): 6250, np.int64(23): 6250, np.int64(24): 6250, np.int64(25): 6250, np.int64(26): 6250, np.int64(27): 6250, np.int64(28): 6250, np.int64(29): 6250, np.int64(30): 6250, np.int64(31): 6250})\n",
      "Finishing upsampling.\n",
      "\n",
      "Epoch 1/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m194s\u001b[0m 31ms/step - accuracy: 0.9976 - loss: 0.0076 - val_accuracy: 1.0000 - val_loss: 4.6730e-08\n",
      "Epoch 2/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m203s\u001b[0m 33ms/step - accuracy: 0.9984 - loss: 0.0052 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 3/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m218s\u001b[0m 35ms/step - accuracy: 0.9988 - loss: 0.0042 - val_accuracy: 1.0000 - val_loss: 4.6492e-09\n",
      "Epoch 4/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m203s\u001b[0m 33ms/step - accuracy: 0.9989 - loss: 0.0043 - val_accuracy: 1.0000 - val_loss: 3.6620e-07\n",
      "Epoch 5/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m218s\u001b[0m 35ms/step - accuracy: 0.9990 - loss: 0.0039 - val_accuracy: 1.0000 - val_loss: 2.8610e-09\n",
      "Epoch 6/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m207s\u001b[0m 33ms/step - accuracy: 0.9991 - loss: 0.0033 - val_accuracy: 1.0000 - val_loss: 1.3709e-09\n",
      "Epoch 7/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m217s\u001b[0m 35ms/step - accuracy: 0.9992 - loss: 0.0030 - val_accuracy: 1.0000 - val_loss: 4.3284e-07\n",
      "Epoch 8/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m205s\u001b[0m 33ms/step - accuracy: 0.9991 - loss: 0.0033 - val_accuracy: 1.0000 - val_loss: 2.9206e-09\n",
      "Epoch 9/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m221s\u001b[0m 36ms/step - accuracy: 0.9991 - loss: 0.0037 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 10/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m209s\u001b[0m 34ms/step - accuracy: 0.9992 - loss: 0.0031 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 11/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m221s\u001b[0m 36ms/step - accuracy: 0.9993 - loss: 0.0030 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 12/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m208s\u001b[0m 34ms/step - accuracy: 0.9992 - loss: 0.0034 - val_accuracy: 1.0000 - val_loss: 7.1526e-10\n",
      "Epoch 12: early stopping\n",
      "Restoring model weights from the end of the best epoch: 2.\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step\n",
      "\n",
      "Training fold 9\n",
      "\n",
      "Generating upsampling using SMOTE...\n",
      "200000 samples after upsampling.\n",
      "Class distribution for training after upsampling: Counter({np.int64(0): 6250, np.int64(1): 6250, np.int64(2): 6250, np.int64(3): 6250, np.int64(4): 6250, np.int64(5): 6250, np.int64(6): 6250, np.int64(7): 6250, np.int64(8): 6250, np.int64(9): 6250, np.int64(10): 6250, np.int64(11): 6250, np.int64(12): 6250, np.int64(13): 6250, np.int64(14): 6250, np.int64(15): 6250, np.int64(16): 6250, np.int64(17): 6250, np.int64(18): 6250, np.int64(19): 6250, np.int64(20): 6250, np.int64(21): 6250, np.int64(22): 6250, np.int64(23): 6250, np.int64(24): 6250, np.int64(25): 6250, np.int64(26): 6250, np.int64(27): 6250, np.int64(28): 6250, np.int64(29): 6250, np.int64(30): 6250, np.int64(31): 6250})\n",
      "Finishing upsampling.\n",
      "\n",
      "Epoch 1/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m227s\u001b[0m 37ms/step - accuracy: 0.9971 - loss: 0.0099 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 2/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m218s\u001b[0m 35ms/step - accuracy: 0.9988 - loss: 0.0044 - val_accuracy: 1.0000 - val_loss: 5.3942e-08\n",
      "Epoch 3/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m220s\u001b[0m 36ms/step - accuracy: 0.9988 - loss: 0.0049 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 4/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m221s\u001b[0m 36ms/step - accuracy: 0.9991 - loss: 0.0036 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 5/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m223s\u001b[0m 36ms/step - accuracy: 0.9991 - loss: 0.0034 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 6/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m229s\u001b[0m 37ms/step - accuracy: 0.9990 - loss: 0.0034 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 7/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m224s\u001b[0m 36ms/step - accuracy: 0.9992 - loss: 0.0028 - val_accuracy: 1.0000 - val_loss: 1.1921e-10\n",
      "Epoch 8/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m219s\u001b[0m 35ms/step - accuracy: 0.9989 - loss: 0.0039 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 9/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m226s\u001b[0m 37ms/step - accuracy: 0.9992 - loss: 0.0033 - val_accuracy: 1.0000 - val_loss: 2.9206e-09\n",
      "Epoch 10/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m226s\u001b[0m 37ms/step - accuracy: 0.9991 - loss: 0.0036 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 11/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m228s\u001b[0m 37ms/step - accuracy: 0.9992 - loss: 0.0024 - val_accuracy: 1.0000 - val_loss: 3.6538e-08\n",
      "Epoch 11: early stopping\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step\n",
      "\n",
      "Training fold 10\n",
      "\n",
      "Generating upsampling using SMOTE...\n",
      "200000 samples after upsampling.\n",
      "Class distribution for training after upsampling: Counter({np.int64(0): 6250, np.int64(1): 6250, np.int64(2): 6250, np.int64(3): 6250, np.int64(4): 6250, np.int64(5): 6250, np.int64(6): 6250, np.int64(7): 6250, np.int64(8): 6250, np.int64(9): 6250, np.int64(10): 6250, np.int64(11): 6250, np.int64(12): 6250, np.int64(13): 6250, np.int64(14): 6250, np.int64(15): 6250, np.int64(16): 6250, np.int64(17): 6250, np.int64(18): 6250, np.int64(19): 6250, np.int64(20): 6250, np.int64(21): 6250, np.int64(22): 6250, np.int64(23): 6250, np.int64(24): 6250, np.int64(25): 6250, np.int64(26): 6250, np.int64(27): 6250, np.int64(28): 6250, np.int64(29): 6250, np.int64(30): 6250, np.int64(31): 6250})\n",
      "Finishing upsampling.\n",
      "\n",
      "Epoch 1/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m219s\u001b[0m 35ms/step - accuracy: 0.9977 - loss: 0.0079 - val_accuracy: 1.0000 - val_loss: 1.6809e-08\n",
      "Epoch 2/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m225s\u001b[0m 36ms/step - accuracy: 0.9987 - loss: 0.0041 - val_accuracy: 1.0000 - val_loss: 4.1127e-09\n",
      "Epoch 3/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m243s\u001b[0m 39ms/step - accuracy: 0.9992 - loss: 0.0033 - val_accuracy: 1.0000 - val_loss: 3.6955e-09\n",
      "Epoch 4/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m231s\u001b[0m 37ms/step - accuracy: 0.9991 - loss: 0.0032 - val_accuracy: 1.0000 - val_loss: 5.9605e-11\n",
      "Epoch 5/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m233s\u001b[0m 38ms/step - accuracy: 0.9993 - loss: 0.0027 - val_accuracy: 1.0000 - val_loss: 1.4901e-09\n",
      "Epoch 6/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m230s\u001b[0m 37ms/step - accuracy: 0.9992 - loss: 0.0031 - val_accuracy: 1.0000 - val_loss: 1.2159e-08\n",
      "Epoch 7/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m234s\u001b[0m 38ms/step - accuracy: 0.9992 - loss: 0.0028 - val_accuracy: 0.9940 - val_loss: 0.0110\n",
      "Epoch 8/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m234s\u001b[0m 38ms/step - accuracy: 0.9992 - loss: 0.0027 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 9/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m240s\u001b[0m 39ms/step - accuracy: 0.9993 - loss: 0.0027 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 10/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m234s\u001b[0m 38ms/step - accuracy: 0.9994 - loss: 0.0022 - val_accuracy: 1.0000 - val_loss: 5.3644e-10\n",
      "Epoch 11/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m235s\u001b[0m 38ms/step - accuracy: 0.9993 - loss: 0.0025 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 12/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m240s\u001b[0m 39ms/step - accuracy: 0.9993 - loss: 0.0028 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 13/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m233s\u001b[0m 38ms/step - accuracy: 0.9993 - loss: 0.0023 - val_accuracy: 1.0000 - val_loss: 1.0729e-09\n",
      "Epoch 14/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m243s\u001b[0m 39ms/step - accuracy: 0.9994 - loss: 0.0027 - val_accuracy: 1.0000 - val_loss: 4.1322e-07\n",
      "Epoch 15/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m234s\u001b[0m 38ms/step - accuracy: 0.9995 - loss: 0.0018 - val_accuracy: 1.0000 - val_loss: 3.6359e-09\n",
      "Epoch 16/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m246s\u001b[0m 40ms/step - accuracy: 0.9993 - loss: 0.0023 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 17/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m233s\u001b[0m 38ms/step - accuracy: 0.9992 - loss: 0.0034 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 18/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m249s\u001b[0m 40ms/step - accuracy: 0.9992 - loss: 0.0029 - val_accuracy: 1.0000 - val_loss: 1.1921e-10\n",
      "Epoch 18: early stopping\n",
      "Restoring model weights from the end of the best epoch: 8.\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step\n",
      "\n",
      "Time taken for training:  08:17:49\n",
      "\n",
      "\n",
      "Fold 1 - Train Accuracy 0.9993 - Test Accuracy 0.5600\n",
      "Fold 2 - Train Accuracy 0.9996 - Test Accuracy 0.8835\n",
      "Fold 3 - Train Accuracy 0.9996 - Test Accuracy 0.9262\n",
      "Fold 4 - Train Accuracy 0.9996 - Test Accuracy 0.9099\n",
      "Fold 5 - Train Accuracy 0.9998 - Test Accuracy 0.9526\n",
      "Fold 6 - Train Accuracy 0.9994 - Test Accuracy 0.9794\n",
      "Fold 7 - Train Accuracy 0.9995 - Test Accuracy 0.9872\n",
      "Fold 8 - Train Accuracy 0.9997 - Test Accuracy 0.9841\n",
      "Fold 9 - Train Accuracy 0.9996 - Test Accuracy 0.9841\n",
      "Fold 10 - Train Accuracy 0.9998 - Test Accuracy 0.9670\n",
      "\n",
      "Mean Train Accuracy: 0.9996 - Std: 0.0001 \n",
      "Mean Test Accuracy: 0.9134 - Std: 0.1225 \n",
      "\n",
      "Evaluate other metrics:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.93      0.94     13905\n",
      "           1       0.90      0.97      0.93       723\n",
      "           2       0.95      0.97      0.96      2659\n",
      "           3       0.83      0.83      0.83      1123\n",
      "           4       0.90      0.86      0.88       384\n",
      "           5       0.97      0.84      0.90        44\n",
      "           6       0.86      0.89      0.87       663\n",
      "           7       0.83      0.88      0.86        94\n",
      "           8       0.91      0.83      0.87        12\n",
      "           9       0.87      0.84      0.86       786\n",
      "          10       0.73      0.89      0.80         9\n",
      "          11       0.89      0.87      0.88        98\n",
      "          12       0.50      0.75      0.60         8\n",
      "          13       0.90      0.90      0.90        20\n",
      "          14       0.94      0.88      0.91        77\n",
      "          15       0.98      0.94      0.96        62\n",
      "          16       0.83      0.84      0.84       917\n",
      "          17       0.94      0.95      0.94       473\n",
      "          18       0.73      0.86      0.79        22\n",
      "          19       0.85      0.86      0.85       122\n",
      "          20       0.89      0.84      0.87       111\n",
      "          21       0.93      0.86      0.89       201\n",
      "          22       0.88      1.00      0.93         7\n",
      "          23       0.81      0.92      0.86        96\n",
      "          24       0.81      0.86      0.84      1045\n",
      "          25       0.82      0.87      0.85       540\n",
      "          26       0.82      0.84      0.83      1334\n",
      "          27       0.76      0.89      0.82        28\n",
      "          28       0.89      0.94      0.92        35\n",
      "          29       0.84      0.83      0.84        77\n",
      "          30       0.93      0.89      0.91        64\n",
      "          31       0.75      0.86      0.80         7\n",
      "\n",
      "    accuracy                           0.91     25746\n",
      "   macro avg       0.86      0.88      0.87     25746\n",
      "weighted avg       0.91      0.91      0.91     25746\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Build a \"time window\" structure to handle the dataset as a time series.\n",
    "new_df = load_dataset(\"lead1\")\n",
    "X_array, y_array = build_time_window_structure(new_df, \"lead1\")\n",
    "X_array, y_array = remove_classes_with_less_samples(X_array, y_array)\n",
    "\n",
    "# Train the CNN model.\n",
    "v1_model = create_v1()\n",
    "v1_num_epochs = 300\n",
    "v1_batch_size = 32\n",
    "v1_validation_split = 0.01\n",
    "\n",
    "training_history_v1 = train_cnn_model(v1_model, X_array, y_array, v1_num_epochs, v1_batch_size, v1_validation_split, \"v1_model_lead1.keras\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a16aed-e2fe-44c0-a83d-30235fc2ec44",
   "metadata": {},
   "source": [
    "#### Lead II"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b2c21c2-ca1c-4ec4-b7c7-42c4f374d4a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Start loading CSV file...\n",
      "Finish loading CSV file.\n",
      "\n",
      "Starting build_time_window_structure function...\n",
      "\n",
      "Shape of features:  (25770, 1250)\n",
      "Quantity os samples (labels):  25770\n",
      "\n",
      "Finishing build_time_window_structure function.\n",
      "\n",
      "Remove classes with less than 6 samples.\n",
      "\n",
      "Show classes identification:\n",
      "Class: 0 - Arrhythmia code: 1\n",
      "Class: 1 - Arrhythmia code: 21\n",
      "Class: 2 - Arrhythmia code: 22\n",
      "Class: 3 - Arrhythmia code: 23\n",
      "Class: 4 - Arrhythmia code: 30\n",
      "Class: 5 - Arrhythmia code: 36\n",
      "Class: 6 - Arrhythmia code: 50\n",
      "Class: 7 - Arrhythmia code: 51\n",
      "Class: 8 - Arrhythmia code: 54\n",
      "Class: 9 - Arrhythmia code: 60\n",
      "Class: 10 - Arrhythmia code: 80\n",
      "Class: 11 - Arrhythmia code: 82\n",
      "Class: 12 - Arrhythmia code: 83\n",
      "Class: 13 - Arrhythmia code: 88\n",
      "Class: 14 - Arrhythmia code: 101\n",
      "Class: 15 - Arrhythmia code: 104\n",
      "Class: 16 - Arrhythmia code: 105\n",
      "Class: 17 - Arrhythmia code: 106\n",
      "Class: 18 - Arrhythmia code: 108\n",
      "Class: 19 - Arrhythmia code: 120\n",
      "Class: 20 - Arrhythmia code: 121\n",
      "Class: 21 - Arrhythmia code: 125\n",
      "Class: 22 - Arrhythmia code: 140\n",
      "Class: 23 - Arrhythmia code: 142\n",
      "Class: 24 - Arrhythmia code: 145\n",
      "Class: 25 - Arrhythmia code: 146\n",
      "Class: 26 - Arrhythmia code: 147\n",
      "Class: 27 - Arrhythmia code: 155\n",
      "Class: 28 - Arrhythmia code: 160\n",
      "Class: 29 - Arrhythmia code: 161\n",
      "Class: 30 - Arrhythmia code: 165\n",
      "Class: 31 - Arrhythmia code: 166\n",
      "\n",
      "Check for dataset balance:\n",
      "Class =   0   Qty =    13905   Percentage = 54.01 %\n",
      "Class =   2   Qty =     2659   Percentage = 10.33 %\n",
      "Class =  26   Qty =     1334   Percentage = 5.18 %\n",
      "Class =   3   Qty =     1123   Percentage = 4.36 %\n",
      "Class =  24   Qty =     1045   Percentage = 4.06 %\n",
      "Class =  16   Qty =      917   Percentage = 3.56 %\n",
      "Class =   9   Qty =      786   Percentage = 3.05 %\n",
      "Class =   1   Qty =      723   Percentage = 2.81 %\n",
      "Class =   6   Qty =      663   Percentage = 2.58 %\n",
      "Class =  25   Qty =      540   Percentage = 2.10 %\n",
      "Class =  17   Qty =      473   Percentage = 1.84 %\n",
      "Class =   4   Qty =      384   Percentage = 1.49 %\n",
      "Class =  21   Qty =      201   Percentage = 0.78 %\n",
      "Class =  19   Qty =      122   Percentage = 0.47 %\n",
      "Class =  20   Qty =      111   Percentage = 0.43 %\n",
      "Class =  11   Qty =       98   Percentage = 0.38 %\n",
      "Class =  23   Qty =       96   Percentage = 0.37 %\n",
      "Class =   7   Qty =       94   Percentage = 0.37 %\n",
      "Class =  14   Qty =       77   Percentage = 0.30 %\n",
      "Class =  29   Qty =       77   Percentage = 0.30 %\n",
      "Class =  30   Qty =       64   Percentage = 0.25 %\n",
      "Class =  15   Qty =       62   Percentage = 0.24 %\n",
      "Class =   5   Qty =       44   Percentage = 0.17 %\n",
      "Class =  28   Qty =       35   Percentage = 0.14 %\n",
      "Class =  27   Qty =       28   Percentage = 0.11 %\n",
      "Class =  18   Qty =       22   Percentage = 0.09 %\n",
      "Class =  13   Qty =       20   Percentage = 0.08 %\n",
      "Class =   8   Qty =       12   Percentage = 0.05 %\n",
      "Class =  10   Qty =        9   Percentage = 0.03 %\n",
      "Class =  12   Qty =        8   Percentage = 0.03 %\n",
      "Class =  22   Qty =        7   Percentage = 0.03 %\n",
      "Class =  31   Qty =        7   Percentage = 0.03 %\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1248</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)        │            <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1248</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)        │            <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1246</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)        │           <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1246</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)        │            <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">623</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">619</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)        │           <span style=\"color: #00af00; text-decoration-color: #00af00\">656</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_2           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">619</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)        │            <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">615</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)        │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,296</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_3           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">615</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)        │            <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">307</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4912</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">628,864</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_4           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,128</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv1d (\u001b[38;5;33mConv1D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1248\u001b[0m, \u001b[38;5;34m8\u001b[0m)        │            \u001b[38;5;34m32\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1248\u001b[0m, \u001b[38;5;34m8\u001b[0m)        │            \u001b[38;5;34m32\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_1 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1246\u001b[0m, \u001b[38;5;34m8\u001b[0m)        │           \u001b[38;5;34m200\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_1           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1246\u001b[0m, \u001b[38;5;34m8\u001b[0m)        │            \u001b[38;5;34m32\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d (\u001b[38;5;33mMaxPooling1D\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m623\u001b[0m, \u001b[38;5;34m8\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_2 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m619\u001b[0m, \u001b[38;5;34m16\u001b[0m)        │           \u001b[38;5;34m656\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_2           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m619\u001b[0m, \u001b[38;5;34m16\u001b[0m)        │            \u001b[38;5;34m64\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_3 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m615\u001b[0m, \u001b[38;5;34m16\u001b[0m)        │         \u001b[38;5;34m1,296\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_3           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m615\u001b[0m, \u001b[38;5;34m16\u001b[0m)        │            \u001b[38;5;34m64\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_1 (\u001b[38;5;33mMaxPooling1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m307\u001b[0m, \u001b[38;5;34m16\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4912\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │       \u001b[38;5;34m628,864\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_4           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │           \u001b[38;5;34m512\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │         \u001b[38;5;34m4,128\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">635,880</span> (2.43 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m635,880\u001b[0m (2.43 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">635,528</span> (2.42 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m635,528\u001b[0m (2.42 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">352</span> (1.38 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m352\u001b[0m (1.38 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting training...\n",
      "\n",
      "Training fold 1\n",
      "\n",
      "Generating upsampling using SMOTE...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\DeveloperTools\\python\\3.11.0\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:737: UserWarning: The least populated class in y has only 7 members, which is less than n_splits=10.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200000 samples after upsampling.\n",
      "Class distribution for training after upsampling: Counter({np.int64(0): 6250, np.int64(1): 6250, np.int64(2): 6250, np.int64(3): 6250, np.int64(4): 6250, np.int64(5): 6250, np.int64(6): 6250, np.int64(7): 6250, np.int64(8): 6250, np.int64(9): 6250, np.int64(10): 6250, np.int64(11): 6250, np.int64(12): 6250, np.int64(13): 6250, np.int64(14): 6250, np.int64(15): 6250, np.int64(16): 6250, np.int64(17): 6250, np.int64(18): 6250, np.int64(19): 6250, np.int64(20): 6250, np.int64(21): 6250, np.int64(22): 6250, np.int64(23): 6250, np.int64(24): 6250, np.int64(25): 6250, np.int64(26): 6250, np.int64(27): 6250, np.int64(28): 6250, np.int64(29): 6250, np.int64(30): 6250, np.int64(31): 6250})\n",
      "Finishing upsampling.\n",
      "\n",
      "Epoch 1/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m113s\u001b[0m 18ms/step - accuracy: 0.8666 - loss: 0.5191 - val_accuracy: 1.0000 - val_loss: 7.0585e-05\n",
      "Epoch 2/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 17ms/step - accuracy: 0.9785 - loss: 0.0685 - val_accuracy: 1.0000 - val_loss: 2.9964e-05\n",
      "Epoch 3/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 18ms/step - accuracy: 0.9893 - loss: 0.0334 - val_accuracy: 1.0000 - val_loss: 5.6041e-06\n",
      "Epoch 4/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 18ms/step - accuracy: 0.9926 - loss: 0.0230 - val_accuracy: 1.0000 - val_loss: 1.7777e-06\n",
      "Epoch 5/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 18ms/step - accuracy: 0.9944 - loss: 0.0168 - val_accuracy: 1.0000 - val_loss: 1.7953e-07\n",
      "Epoch 6/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m110s\u001b[0m 18ms/step - accuracy: 0.9955 - loss: 0.0145 - val_accuracy: 1.0000 - val_loss: 1.0069e-05\n",
      "Epoch 7/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 18ms/step - accuracy: 0.9964 - loss: 0.0122 - val_accuracy: 1.0000 - val_loss: 1.5962e-07\n",
      "Epoch 8/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m111s\u001b[0m 18ms/step - accuracy: 0.9965 - loss: 0.0108 - val_accuracy: 1.0000 - val_loss: 4.6682e-06\n",
      "Epoch 9/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m111s\u001b[0m 18ms/step - accuracy: 0.9967 - loss: 0.0099 - val_accuracy: 1.0000 - val_loss: 6.3073e-07\n",
      "Epoch 10/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m111s\u001b[0m 18ms/step - accuracy: 0.9974 - loss: 0.0081 - val_accuracy: 1.0000 - val_loss: 3.9935e-09\n",
      "Epoch 11/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m113s\u001b[0m 18ms/step - accuracy: 0.9975 - loss: 0.0078 - val_accuracy: 1.0000 - val_loss: 5.3929e-07\n",
      "Epoch 12/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m112s\u001b[0m 18ms/step - accuracy: 0.9975 - loss: 0.0075 - val_accuracy: 1.0000 - val_loss: 5.4764e-07\n",
      "Epoch 13/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m113s\u001b[0m 18ms/step - accuracy: 0.9977 - loss: 0.0073 - val_accuracy: 1.0000 - val_loss: 4.6492e-09\n",
      "Epoch 14/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m114s\u001b[0m 18ms/step - accuracy: 0.9979 - loss: 0.0067 - val_accuracy: 1.0000 - val_loss: 2.2054e-09\n",
      "Epoch 15/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m114s\u001b[0m 18ms/step - accuracy: 0.9981 - loss: 0.0056 - val_accuracy: 1.0000 - val_loss: 6.8247e-08\n",
      "Epoch 16/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m114s\u001b[0m 18ms/step - accuracy: 0.9978 - loss: 0.0065 - val_accuracy: 1.0000 - val_loss: 5.3644e-09\n",
      "Epoch 17/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m116s\u001b[0m 19ms/step - accuracy: 0.9983 - loss: 0.0054 - val_accuracy: 1.0000 - val_loss: 4.9829e-08\n",
      "Epoch 18/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m115s\u001b[0m 19ms/step - accuracy: 0.9983 - loss: 0.0059 - val_accuracy: 1.0000 - val_loss: 3.2306e-08\n",
      "Epoch 19/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m116s\u001b[0m 19ms/step - accuracy: 0.9985 - loss: 0.0053 - val_accuracy: 1.0000 - val_loss: 1.1802e-08\n",
      "Epoch 20/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m116s\u001b[0m 19ms/step - accuracy: 0.9985 - loss: 0.0047 - val_accuracy: 1.0000 - val_loss: 4.1604e-08\n",
      "Epoch 21/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m117s\u001b[0m 19ms/step - accuracy: 0.9985 - loss: 0.0047 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 22/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m118s\u001b[0m 19ms/step - accuracy: 0.9984 - loss: 0.0052 - val_accuracy: 1.0000 - val_loss: 2.7072e-07\n",
      "Epoch 23/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m118s\u001b[0m 19ms/step - accuracy: 0.9987 - loss: 0.0043 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 24/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m119s\u001b[0m 19ms/step - accuracy: 0.9988 - loss: 0.0038 - val_accuracy: 1.0000 - val_loss: 5.9605e-11\n",
      "Epoch 25/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m119s\u001b[0m 19ms/step - accuracy: 0.9989 - loss: 0.0037 - val_accuracy: 1.0000 - val_loss: 2.8038e-07\n",
      "Epoch 26/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m120s\u001b[0m 19ms/step - accuracy: 0.9987 - loss: 0.0041 - val_accuracy: 1.0000 - val_loss: 3.0816e-08\n",
      "Epoch 27/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m120s\u001b[0m 19ms/step - accuracy: 0.9987 - loss: 0.0042 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 28/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m119s\u001b[0m 19ms/step - accuracy: 0.9989 - loss: 0.0037 - val_accuracy: 1.0000 - val_loss: 8.0345e-08\n",
      "Epoch 29/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m121s\u001b[0m 20ms/step - accuracy: 0.9988 - loss: 0.0036 - val_accuracy: 1.0000 - val_loss: 5.4836e-09\n",
      "Epoch 30/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m122s\u001b[0m 20ms/step - accuracy: 0.9988 - loss: 0.0038 - val_accuracy: 1.0000 - val_loss: 1.1510e-07\n",
      "Epoch 31/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m122s\u001b[0m 20ms/step - accuracy: 0.9988 - loss: 0.0038 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 31: early stopping\n",
      "Restoring model weights from the end of the best epoch: 21.\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step\n",
      "\n",
      "Training fold 2\n",
      "\n",
      "Generating upsampling using SMOTE...\n",
      "200000 samples after upsampling.\n",
      "Class distribution for training after upsampling: Counter({np.int64(0): 6250, np.int64(1): 6250, np.int64(2): 6250, np.int64(3): 6250, np.int64(4): 6250, np.int64(5): 6250, np.int64(6): 6250, np.int64(7): 6250, np.int64(8): 6250, np.int64(9): 6250, np.int64(10): 6250, np.int64(11): 6250, np.int64(12): 6250, np.int64(13): 6250, np.int64(14): 6250, np.int64(15): 6250, np.int64(16): 6250, np.int64(17): 6250, np.int64(18): 6250, np.int64(19): 6250, np.int64(20): 6250, np.int64(21): 6250, np.int64(22): 6250, np.int64(23): 6250, np.int64(24): 6250, np.int64(25): 6250, np.int64(26): 6250, np.int64(27): 6250, np.int64(28): 6250, np.int64(29): 6250, np.int64(30): 6250, np.int64(31): 6250})\n",
      "Finishing upsampling.\n",
      "\n",
      "Epoch 1/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m125s\u001b[0m 20ms/step - accuracy: 0.9742 - loss: 0.1671 - val_accuracy: 1.0000 - val_loss: 9.3885e-05\n",
      "Epoch 2/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m123s\u001b[0m 20ms/step - accuracy: 0.9955 - loss: 0.0147 - val_accuracy: 1.0000 - val_loss: 1.9848e-08\n",
      "Epoch 3/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m124s\u001b[0m 20ms/step - accuracy: 0.9976 - loss: 0.0077 - val_accuracy: 1.0000 - val_loss: 5.9605e-09\n",
      "Epoch 4/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m123s\u001b[0m 20ms/step - accuracy: 0.9985 - loss: 0.0054 - val_accuracy: 1.0000 - val_loss: 7.0333e-09\n",
      "Epoch 5/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m124s\u001b[0m 20ms/step - accuracy: 0.9982 - loss: 0.0059 - val_accuracy: 1.0000 - val_loss: 3.1626e-07\n",
      "Epoch 6/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m124s\u001b[0m 20ms/step - accuracy: 0.9987 - loss: 0.0044 - val_accuracy: 1.0000 - val_loss: 1.7023e-07\n",
      "Epoch 7/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m125s\u001b[0m 20ms/step - accuracy: 0.9985 - loss: 0.0057 - val_accuracy: 1.0000 - val_loss: 2.0862e-09\n",
      "Epoch 8/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m125s\u001b[0m 20ms/step - accuracy: 0.9986 - loss: 0.0050 - val_accuracy: 1.0000 - val_loss: 1.3238e-07\n",
      "Epoch 9/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m126s\u001b[0m 20ms/step - accuracy: 0.9988 - loss: 0.0045 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 10/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m128s\u001b[0m 21ms/step - accuracy: 0.9990 - loss: 0.0033 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 11/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m128s\u001b[0m 21ms/step - accuracy: 0.9988 - loss: 0.0040 - val_accuracy: 1.0000 - val_loss: 7.7486e-10\n",
      "Epoch 12/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m130s\u001b[0m 21ms/step - accuracy: 0.9988 - loss: 0.0040 - val_accuracy: 1.0000 - val_loss: 4.1723e-10\n",
      "Epoch 13/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m129s\u001b[0m 21ms/step - accuracy: 0.9989 - loss: 0.0037 - val_accuracy: 1.0000 - val_loss: 1.7881e-10\n",
      "Epoch 14/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m129s\u001b[0m 21ms/step - accuracy: 0.9988 - loss: 0.0037 - val_accuracy: 1.0000 - val_loss: 5.9605e-10\n",
      "Epoch 15/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m130s\u001b[0m 21ms/step - accuracy: 0.9992 - loss: 0.0027 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 16/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m132s\u001b[0m 21ms/step - accuracy: 0.9988 - loss: 0.0036 - val_accuracy: 1.0000 - val_loss: 4.4167e-08\n",
      "Epoch 17/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m131s\u001b[0m 21ms/step - accuracy: 0.9989 - loss: 0.0034 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 18/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m131s\u001b[0m 21ms/step - accuracy: 0.9991 - loss: 0.0028 - val_accuracy: 1.0000 - val_loss: 2.9802e-10\n",
      "Epoch 19/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m131s\u001b[0m 21ms/step - accuracy: 0.9988 - loss: 0.0034 - val_accuracy: 1.0000 - val_loss: 5.3644e-10\n",
      "Epoch 19: early stopping\n",
      "Restoring model weights from the end of the best epoch: 9.\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step\n",
      "\n",
      "Training fold 3\n",
      "\n",
      "Generating upsampling using SMOTE...\n",
      "200000 samples after upsampling.\n",
      "Class distribution for training after upsampling: Counter({np.int64(0): 6250, np.int64(1): 6250, np.int64(2): 6250, np.int64(3): 6250, np.int64(4): 6250, np.int64(5): 6250, np.int64(6): 6250, np.int64(7): 6250, np.int64(8): 6250, np.int64(9): 6250, np.int64(10): 6250, np.int64(11): 6250, np.int64(12): 6250, np.int64(13): 6250, np.int64(14): 6250, np.int64(15): 6250, np.int64(16): 6250, np.int64(17): 6250, np.int64(18): 6250, np.int64(19): 6250, np.int64(20): 6250, np.int64(21): 6250, np.int64(22): 6250, np.int64(23): 6250, np.int64(24): 6250, np.int64(25): 6250, np.int64(26): 6250, np.int64(27): 6250, np.int64(28): 6250, np.int64(29): 6250, np.int64(30): 6250, np.int64(31): 6250})\n",
      "Finishing upsampling.\n",
      "\n",
      "Epoch 1/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m136s\u001b[0m 22ms/step - accuracy: 0.9909 - loss: 0.0398 - val_accuracy: 1.0000 - val_loss: 9.3534e-07\n",
      "Epoch 2/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m133s\u001b[0m 22ms/step - accuracy: 0.9977 - loss: 0.0084 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 3/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m132s\u001b[0m 21ms/step - accuracy: 0.9986 - loss: 0.0052 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 4/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m135s\u001b[0m 22ms/step - accuracy: 0.9986 - loss: 0.0042 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 5/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m133s\u001b[0m 21ms/step - accuracy: 0.9987 - loss: 0.0046 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 6/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m133s\u001b[0m 22ms/step - accuracy: 0.9987 - loss: 0.0046 - val_accuracy: 1.0000 - val_loss: 1.2577e-08\n",
      "Epoch 7/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m134s\u001b[0m 22ms/step - accuracy: 0.9985 - loss: 0.0046 - val_accuracy: 1.0000 - val_loss: 3.1650e-08\n",
      "Epoch 8/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m134s\u001b[0m 22ms/step - accuracy: 0.9988 - loss: 0.0044 - val_accuracy: 1.0000 - val_loss: 1.4842e-08\n",
      "Epoch 9/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m135s\u001b[0m 22ms/step - accuracy: 0.9990 - loss: 0.0038 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 10/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m134s\u001b[0m 22ms/step - accuracy: 0.9988 - loss: 0.0044 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 11/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m135s\u001b[0m 22ms/step - accuracy: 0.9993 - loss: 0.0035 - val_accuracy: 1.0000 - val_loss: 4.7257e-05\n",
      "Epoch 12/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m137s\u001b[0m 22ms/step - accuracy: 0.9989 - loss: 0.0039 - val_accuracy: 1.0000 - val_loss: 3.4928e-08\n",
      "Epoch 12: early stopping\n",
      "Restoring model weights from the end of the best epoch: 2.\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step\n",
      "\n",
      "Training fold 4\n",
      "\n",
      "Generating upsampling using SMOTE...\n",
      "200000 samples after upsampling.\n",
      "Class distribution for training after upsampling: Counter({np.int64(0): 6250, np.int64(1): 6250, np.int64(2): 6250, np.int64(3): 6250, np.int64(4): 6250, np.int64(5): 6250, np.int64(6): 6250, np.int64(7): 6250, np.int64(8): 6250, np.int64(9): 6250, np.int64(10): 6250, np.int64(11): 6250, np.int64(12): 6250, np.int64(13): 6250, np.int64(14): 6250, np.int64(15): 6250, np.int64(16): 6250, np.int64(17): 6250, np.int64(18): 6250, np.int64(19): 6250, np.int64(20): 6250, np.int64(21): 6250, np.int64(22): 6250, np.int64(23): 6250, np.int64(24): 6250, np.int64(25): 6250, np.int64(26): 6250, np.int64(27): 6250, np.int64(28): 6250, np.int64(29): 6250, np.int64(30): 6250, np.int64(31): 6250})\n",
      "Finishing upsampling.\n",
      "\n",
      "Epoch 1/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m169s\u001b[0m 27ms/step - accuracy: 0.9953 - loss: 0.0187 - val_accuracy: 1.0000 - val_loss: 2.1636e-07\n",
      "Epoch 2/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m160s\u001b[0m 26ms/step - accuracy: 0.9981 - loss: 0.0062 - val_accuracy: 1.0000 - val_loss: 3.1650e-08\n",
      "Epoch 3/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m155s\u001b[0m 25ms/step - accuracy: 0.9986 - loss: 0.0052 - val_accuracy: 1.0000 - val_loss: 2.3842e-10\n",
      "Epoch 4/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m155s\u001b[0m 25ms/step - accuracy: 0.9988 - loss: 0.0039 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 5/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m157s\u001b[0m 25ms/step - accuracy: 0.9989 - loss: 0.0038 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 6/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m158s\u001b[0m 26ms/step - accuracy: 0.9992 - loss: 0.0032 - val_accuracy: 1.0000 - val_loss: 5.9307e-08\n",
      "Epoch 7/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m159s\u001b[0m 26ms/step - accuracy: 0.9989 - loss: 0.0041 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 8/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m161s\u001b[0m 26ms/step - accuracy: 0.9992 - loss: 0.0031 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 9/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m162s\u001b[0m 26ms/step - accuracy: 0.9993 - loss: 0.0030 - val_accuracy: 1.0000 - val_loss: 1.1921e-09\n",
      "Epoch 10/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m161s\u001b[0m 26ms/step - accuracy: 0.9991 - loss: 0.0036 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 11/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m163s\u001b[0m 26ms/step - accuracy: 0.9992 - loss: 0.0037 - val_accuracy: 1.0000 - val_loss: 3.6418e-08\n",
      "Epoch 12/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m163s\u001b[0m 26ms/step - accuracy: 0.9990 - loss: 0.0038 - val_accuracy: 1.0000 - val_loss: 1.7285e-09\n",
      "Epoch 13/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m165s\u001b[0m 27ms/step - accuracy: 0.9990 - loss: 0.0033 - val_accuracy: 1.0000 - val_loss: 6.9099e-05\n",
      "Epoch 14/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m166s\u001b[0m 27ms/step - accuracy: 0.9993 - loss: 0.0028 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 14: early stopping\n",
      "Restoring model weights from the end of the best epoch: 4.\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step\n",
      "\n",
      "Training fold 5\n",
      "\n",
      "Generating upsampling using SMOTE...\n",
      "200000 samples after upsampling.\n",
      "Class distribution for training after upsampling: Counter({np.int64(0): 6250, np.int64(1): 6250, np.int64(2): 6250, np.int64(3): 6250, np.int64(4): 6250, np.int64(5): 6250, np.int64(6): 6250, np.int64(7): 6250, np.int64(8): 6250, np.int64(9): 6250, np.int64(10): 6250, np.int64(11): 6250, np.int64(12): 6250, np.int64(13): 6250, np.int64(14): 6250, np.int64(15): 6250, np.int64(16): 6250, np.int64(17): 6250, np.int64(18): 6250, np.int64(19): 6250, np.int64(20): 6250, np.int64(21): 6250, np.int64(22): 6250, np.int64(23): 6250, np.int64(24): 6250, np.int64(25): 6250, np.int64(26): 6250, np.int64(27): 6250, np.int64(28): 6250, np.int64(29): 6250, np.int64(30): 6250, np.int64(31): 6250})\n",
      "Finishing upsampling.\n",
      "\n",
      "Epoch 1/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m177s\u001b[0m 29ms/step - accuracy: 0.9957 - loss: 0.0146 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 2/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m173s\u001b[0m 28ms/step - accuracy: 0.9987 - loss: 0.0047 - val_accuracy: 1.0000 - val_loss: 1.9193e-08\n",
      "Epoch 3/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m170s\u001b[0m 27ms/step - accuracy: 0.9990 - loss: 0.0035 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 4/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m172s\u001b[0m 28ms/step - accuracy: 0.9987 - loss: 0.0045 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 5/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m173s\u001b[0m 28ms/step - accuracy: 0.9989 - loss: 0.0034 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 6/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m173s\u001b[0m 28ms/step - accuracy: 0.9992 - loss: 0.0025 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 7/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m176s\u001b[0m 28ms/step - accuracy: 0.9991 - loss: 0.0032 - val_accuracy: 1.0000 - val_loss: 9.5367e-10\n",
      "Epoch 8/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m175s\u001b[0m 28ms/step - accuracy: 0.9992 - loss: 0.0033 - val_accuracy: 1.0000 - val_loss: 2.3842e-09\n",
      "Epoch 9/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m175s\u001b[0m 28ms/step - accuracy: 0.9991 - loss: 0.0033 - val_accuracy: 1.0000 - val_loss: 9.5367e-09\n",
      "Epoch 10/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m179s\u001b[0m 29ms/step - accuracy: 0.9992 - loss: 0.0030 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 11/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m180s\u001b[0m 29ms/step - accuracy: 0.9994 - loss: 0.0024 - val_accuracy: 1.0000 - val_loss: 9.3936e-08\n",
      "Epoch 11: early stopping\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step\n",
      "\n",
      "Training fold 6\n",
      "\n",
      "Generating upsampling using SMOTE...\n",
      "200000 samples after upsampling.\n",
      "Class distribution for training after upsampling: Counter({np.int64(0): 6250, np.int64(1): 6250, np.int64(2): 6250, np.int64(3): 6250, np.int64(4): 6250, np.int64(5): 6250, np.int64(6): 6250, np.int64(7): 6250, np.int64(8): 6250, np.int64(9): 6250, np.int64(10): 6250, np.int64(11): 6250, np.int64(12): 6250, np.int64(13): 6250, np.int64(14): 6250, np.int64(15): 6250, np.int64(16): 6250, np.int64(17): 6250, np.int64(18): 6250, np.int64(19): 6250, np.int64(20): 6250, np.int64(21): 6250, np.int64(22): 6250, np.int64(23): 6250, np.int64(24): 6250, np.int64(25): 6250, np.int64(26): 6250, np.int64(27): 6250, np.int64(28): 6250, np.int64(29): 6250, np.int64(30): 6250, np.int64(31): 6250})\n",
      "Finishing upsampling.\n",
      "\n",
      "Epoch 1/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m188s\u001b[0m 30ms/step - accuracy: 0.9972 - loss: 0.0096 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 2/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m186s\u001b[0m 30ms/step - accuracy: 0.9984 - loss: 0.0051 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 3/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m187s\u001b[0m 30ms/step - accuracy: 0.9990 - loss: 0.0042 - val_accuracy: 1.0000 - val_loss: 7.1526e-10\n",
      "Epoch 4/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m186s\u001b[0m 30ms/step - accuracy: 0.9989 - loss: 0.0045 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 5/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m186s\u001b[0m 30ms/step - accuracy: 0.9990 - loss: 0.0032 - val_accuracy: 1.0000 - val_loss: 5.0664e-09\n",
      "Epoch 6/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m188s\u001b[0m 30ms/step - accuracy: 0.9991 - loss: 0.0040 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 7/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m188s\u001b[0m 30ms/step - accuracy: 0.9992 - loss: 0.0031 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 8/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m190s\u001b[0m 31ms/step - accuracy: 0.9992 - loss: 0.0028 - val_accuracy: 1.0000 - val_loss: 1.1751e-06\n",
      "Epoch 9/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m191s\u001b[0m 31ms/step - accuracy: 0.9992 - loss: 0.0029 - val_accuracy: 1.0000 - val_loss: 2.0266e-09\n",
      "Epoch 10/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m192s\u001b[0m 31ms/step - accuracy: 0.9990 - loss: 0.0046 - val_accuracy: 1.0000 - val_loss: 4.0531e-09\n",
      "Epoch 11/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m192s\u001b[0m 31ms/step - accuracy: 0.9993 - loss: 0.0029 - val_accuracy: 1.0000 - val_loss: 1.3113e-09\n",
      "Epoch 11: early stopping\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step\n",
      "\n",
      "Training fold 7\n",
      "\n",
      "Generating upsampling using SMOTE...\n",
      "200000 samples after upsampling.\n",
      "Class distribution for training after upsampling: Counter({np.int64(0): 6250, np.int64(1): 6250, np.int64(2): 6250, np.int64(3): 6250, np.int64(4): 6250, np.int64(5): 6250, np.int64(6): 6250, np.int64(7): 6250, np.int64(8): 6250, np.int64(9): 6250, np.int64(10): 6250, np.int64(11): 6250, np.int64(12): 6250, np.int64(13): 6250, np.int64(14): 6250, np.int64(15): 6250, np.int64(16): 6250, np.int64(17): 6250, np.int64(18): 6250, np.int64(19): 6250, np.int64(20): 6250, np.int64(21): 6250, np.int64(22): 6250, np.int64(23): 6250, np.int64(24): 6250, np.int64(25): 6250, np.int64(26): 6250, np.int64(27): 6250, np.int64(28): 6250, np.int64(29): 6250, np.int64(30): 6250, np.int64(31): 6250})\n",
      "Finishing upsampling.\n",
      "\n",
      "Epoch 1/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m200s\u001b[0m 32ms/step - accuracy: 0.9975 - loss: 0.0081 - val_accuracy: 1.0000 - val_loss: 4.2021e-08\n",
      "Epoch 2/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m200s\u001b[0m 32ms/step - accuracy: 0.9986 - loss: 0.0048 - val_accuracy: 1.0000 - val_loss: 2.8193e-08\n",
      "Epoch 3/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m198s\u001b[0m 32ms/step - accuracy: 0.9988 - loss: 0.0042 - val_accuracy: 1.0000 - val_loss: 1.7285e-09\n",
      "Epoch 4/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m199s\u001b[0m 32ms/step - accuracy: 0.9990 - loss: 0.0035 - val_accuracy: 1.0000 - val_loss: 2.0266e-09\n",
      "Epoch 5/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m199s\u001b[0m 32ms/step - accuracy: 0.9992 - loss: 0.0029 - val_accuracy: 1.0000 - val_loss: 7.1526e-10\n",
      "Epoch 6/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m200s\u001b[0m 32ms/step - accuracy: 0.9990 - loss: 0.0033 - val_accuracy: 1.0000 - val_loss: 1.5140e-08\n",
      "Epoch 7/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m199s\u001b[0m 32ms/step - accuracy: 0.9992 - loss: 0.0028 - val_accuracy: 1.0000 - val_loss: 1.8477e-09\n",
      "Epoch 8/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m201s\u001b[0m 33ms/step - accuracy: 0.9992 - loss: 0.0028 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 9/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m200s\u001b[0m 32ms/step - accuracy: 0.9991 - loss: 0.0034 - val_accuracy: 1.0000 - val_loss: 2.3842e-10\n",
      "Epoch 10/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m201s\u001b[0m 32ms/step - accuracy: 0.9992 - loss: 0.0025 - val_accuracy: 1.0000 - val_loss: 5.9605e-11\n",
      "Epoch 11/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m202s\u001b[0m 33ms/step - accuracy: 0.9993 - loss: 0.0027 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 12/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m209s\u001b[0m 34ms/step - accuracy: 0.9993 - loss: 0.0024 - val_accuracy: 1.0000 - val_loss: 7.7486e-10\n",
      "Epoch 13/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m203s\u001b[0m 33ms/step - accuracy: 0.9994 - loss: 0.0021 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 14/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m204s\u001b[0m 33ms/step - accuracy: 0.9992 - loss: 0.0032 - val_accuracy: 1.0000 - val_loss: 1.3709e-09\n",
      "Epoch 15/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m207s\u001b[0m 33ms/step - accuracy: 0.9992 - loss: 0.0033 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 16/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m208s\u001b[0m 34ms/step - accuracy: 0.9992 - loss: 0.0023 - val_accuracy: 1.0000 - val_loss: 3.5763e-10\n",
      "Epoch 17/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m210s\u001b[0m 34ms/step - accuracy: 0.9992 - loss: 0.0029 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 18/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m210s\u001b[0m 34ms/step - accuracy: 0.9992 - loss: 0.0027 - val_accuracy: 1.0000 - val_loss: 2.9206e-08\n",
      "Epoch 18: early stopping\n",
      "Restoring model weights from the end of the best epoch: 8.\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step\n",
      "\n",
      "Training fold 8\n",
      "\n",
      "Generating upsampling using SMOTE...\n",
      "200000 samples after upsampling.\n",
      "Class distribution for training after upsampling: Counter({np.int64(0): 6250, np.int64(1): 6250, np.int64(2): 6250, np.int64(3): 6250, np.int64(4): 6250, np.int64(5): 6250, np.int64(6): 6250, np.int64(7): 6250, np.int64(8): 6250, np.int64(9): 6250, np.int64(10): 6250, np.int64(11): 6250, np.int64(12): 6250, np.int64(13): 6250, np.int64(14): 6250, np.int64(15): 6250, np.int64(16): 6250, np.int64(17): 6250, np.int64(18): 6250, np.int64(19): 6250, np.int64(20): 6250, np.int64(21): 6250, np.int64(22): 6250, np.int64(23): 6250, np.int64(24): 6250, np.int64(25): 6250, np.int64(26): 6250, np.int64(27): 6250, np.int64(28): 6250, np.int64(29): 6250, np.int64(30): 6250, np.int64(31): 6250})\n",
      "Finishing upsampling.\n",
      "\n",
      "Epoch 1/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m245s\u001b[0m 39ms/step - accuracy: 0.9966 - loss: 0.0118 - val_accuracy: 1.0000 - val_loss: 2.2054e-09\n",
      "Epoch 2/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m247s\u001b[0m 40ms/step - accuracy: 0.9989 - loss: 0.0039 - val_accuracy: 1.0000 - val_loss: 1.3113e-09\n",
      "Epoch 3/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m237s\u001b[0m 38ms/step - accuracy: 0.9988 - loss: 0.0041 - val_accuracy: 1.0000 - val_loss: 1.0133e-09\n",
      "Epoch 4/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m218s\u001b[0m 35ms/step - accuracy: 0.9990 - loss: 0.0038 - val_accuracy: 1.0000 - val_loss: 7.1526e-10\n",
      "Epoch 5/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m202s\u001b[0m 33ms/step - accuracy: 0.9992 - loss: 0.0033 - val_accuracy: 1.0000 - val_loss: 3.4571e-09\n",
      "Epoch 6/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m217s\u001b[0m 35ms/step - accuracy: 0.9992 - loss: 0.0029 - val_accuracy: 1.0000 - val_loss: 1.7881e-10\n",
      "Epoch 7/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m219s\u001b[0m 35ms/step - accuracy: 0.9994 - loss: 0.0035 - val_accuracy: 1.0000 - val_loss: 1.2636e-08\n",
      "Epoch 8/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m220s\u001b[0m 35ms/step - accuracy: 0.9992 - loss: 0.0037 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 9/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m224s\u001b[0m 36ms/step - accuracy: 0.9993 - loss: 0.0024 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 10/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m240s\u001b[0m 39ms/step - accuracy: 0.9994 - loss: 0.0023 - val_accuracy: 1.0000 - val_loss: 3.9294e-06\n",
      "Epoch 11/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m307s\u001b[0m 50ms/step - accuracy: 0.9993 - loss: 0.0026 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 12/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m247s\u001b[0m 40ms/step - accuracy: 0.9994 - loss: 0.0030 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 13/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m260s\u001b[0m 42ms/step - accuracy: 0.9992 - loss: 0.0028 - val_accuracy: 1.0000 - val_loss: 1.6093e-09\n",
      "Epoch 14/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m235s\u001b[0m 38ms/step - accuracy: 0.9993 - loss: 0.0027 - val_accuracy: 1.0000 - val_loss: 7.7486e-10\n",
      "Epoch 15/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m234s\u001b[0m 38ms/step - accuracy: 0.9995 - loss: 0.0019 - val_accuracy: 1.0000 - val_loss: 2.0635e-07\n",
      "Epoch 16/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m240s\u001b[0m 39ms/step - accuracy: 0.9992 - loss: 0.0030 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 17/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m244s\u001b[0m 39ms/step - accuracy: 0.9994 - loss: 0.0024 - val_accuracy: 1.0000 - val_loss: 1.3113e-09\n",
      "Epoch 18/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m243s\u001b[0m 39ms/step - accuracy: 0.9993 - loss: 0.0024 - val_accuracy: 1.0000 - val_loss: 5.9605e-10\n",
      "Epoch 18: early stopping\n",
      "Restoring model weights from the end of the best epoch: 8.\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step\n",
      "\n",
      "Training fold 9\n",
      "\n",
      "Generating upsampling using SMOTE...\n",
      "200000 samples after upsampling.\n",
      "Class distribution for training after upsampling: Counter({np.int64(0): 6250, np.int64(1): 6250, np.int64(2): 6250, np.int64(3): 6250, np.int64(4): 6250, np.int64(5): 6250, np.int64(6): 6250, np.int64(7): 6250, np.int64(8): 6250, np.int64(9): 6250, np.int64(10): 6250, np.int64(11): 6250, np.int64(12): 6250, np.int64(13): 6250, np.int64(14): 6250, np.int64(15): 6250, np.int64(16): 6250, np.int64(17): 6250, np.int64(18): 6250, np.int64(19): 6250, np.int64(20): 6250, np.int64(21): 6250, np.int64(22): 6250, np.int64(23): 6250, np.int64(24): 6250, np.int64(25): 6250, np.int64(26): 6250, np.int64(27): 6250, np.int64(28): 6250, np.int64(29): 6250, np.int64(30): 6250, np.int64(31): 6250})\n",
      "Finishing upsampling.\n",
      "\n",
      "Epoch 1/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m233s\u001b[0m 38ms/step - accuracy: 0.9970 - loss: 0.0106 - val_accuracy: 1.0000 - val_loss: 2.0862e-09\n",
      "Epoch 2/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m265s\u001b[0m 43ms/step - accuracy: 0.9986 - loss: 0.0046 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 3/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m256s\u001b[0m 41ms/step - accuracy: 0.9991 - loss: 0.0040 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 4/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m243s\u001b[0m 39ms/step - accuracy: 0.9993 - loss: 0.0026 - val_accuracy: 1.0000 - val_loss: 5.2512e-08\n",
      "Epoch 5/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m246s\u001b[0m 40ms/step - accuracy: 0.9992 - loss: 0.0029 - val_accuracy: 1.0000 - val_loss: 9.9540e-09\n",
      "Epoch 6/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m245s\u001b[0m 40ms/step - accuracy: 0.9993 - loss: 0.0026 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 7/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m256s\u001b[0m 41ms/step - accuracy: 0.9994 - loss: 0.0025 - val_accuracy: 1.0000 - val_loss: 5.9605e-11\n",
      "Epoch 8/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m249s\u001b[0m 40ms/step - accuracy: 0.9994 - loss: 0.0022 - val_accuracy: 1.0000 - val_loss: 6.5312e-05\n",
      "Epoch 9/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m247s\u001b[0m 40ms/step - accuracy: 0.9993 - loss: 0.0024 - val_accuracy: 1.0000 - val_loss: 1.8299e-08\n",
      "Epoch 10/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m269s\u001b[0m 43ms/step - accuracy: 0.9992 - loss: 0.0033 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 11/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m376s\u001b[0m 61ms/step - accuracy: 0.9994 - loss: 0.0019 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 12/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m338s\u001b[0m 55ms/step - accuracy: 0.9993 - loss: 0.0028 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 12: early stopping\n",
      "Restoring model weights from the end of the best epoch: 2.\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step\n",
      "\n",
      "Training fold 10\n",
      "\n",
      "Generating upsampling using SMOTE...\n",
      "200000 samples after upsampling.\n",
      "Class distribution for training after upsampling: Counter({np.int64(0): 6250, np.int64(1): 6250, np.int64(2): 6250, np.int64(3): 6250, np.int64(4): 6250, np.int64(5): 6250, np.int64(6): 6250, np.int64(7): 6250, np.int64(8): 6250, np.int64(9): 6250, np.int64(10): 6250, np.int64(11): 6250, np.int64(12): 6250, np.int64(13): 6250, np.int64(14): 6250, np.int64(15): 6250, np.int64(16): 6250, np.int64(17): 6250, np.int64(18): 6250, np.int64(19): 6250, np.int64(20): 6250, np.int64(21): 6250, np.int64(22): 6250, np.int64(23): 6250, np.int64(24): 6250, np.int64(25): 6250, np.int64(26): 6250, np.int64(27): 6250, np.int64(28): 6250, np.int64(29): 6250, np.int64(30): 6250, np.int64(31): 6250})\n",
      "Finishing upsampling.\n",
      "\n",
      "Epoch 1/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m249s\u001b[0m 40ms/step - accuracy: 0.9977 - loss: 0.0087 - val_accuracy: 1.0000 - val_loss: 4.7684e-09\n",
      "Epoch 2/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m222s\u001b[0m 36ms/step - accuracy: 0.9991 - loss: 0.0036 - val_accuracy: 1.0000 - val_loss: 2.8610e-09\n",
      "Epoch 3/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m233s\u001b[0m 38ms/step - accuracy: 0.9991 - loss: 0.0031 - val_accuracy: 1.0000 - val_loss: 3.6359e-09\n",
      "Epoch 4/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m244s\u001b[0m 39ms/step - accuracy: 0.9993 - loss: 0.0027 - val_accuracy: 1.0000 - val_loss: 7.9870e-09\n",
      "Epoch 5/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m256s\u001b[0m 41ms/step - accuracy: 0.9992 - loss: 0.0026 - val_accuracy: 1.0000 - val_loss: 7.1526e-10\n",
      "Epoch 6/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m252s\u001b[0m 41ms/step - accuracy: 0.9992 - loss: 0.0025 - val_accuracy: 1.0000 - val_loss: 1.0669e-08\n",
      "Epoch 7/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m274s\u001b[0m 44ms/step - accuracy: 0.9993 - loss: 0.0025 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 8/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m262s\u001b[0m 42ms/step - accuracy: 0.9994 - loss: 0.0025 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 9/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m285s\u001b[0m 46ms/step - accuracy: 0.9993 - loss: 0.0026 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 10/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m271s\u001b[0m 44ms/step - accuracy: 0.9993 - loss: 0.0023 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 11/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m273s\u001b[0m 44ms/step - accuracy: 0.9995 - loss: 0.0025 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 12/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m281s\u001b[0m 45ms/step - accuracy: 0.9994 - loss: 0.0021 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 13/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m265s\u001b[0m 43ms/step - accuracy: 0.9995 - loss: 0.0018 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 14/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m276s\u001b[0m 45ms/step - accuracy: 0.9994 - loss: 0.0025 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 15/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m275s\u001b[0m 44ms/step - accuracy: 0.9994 - loss: 0.0018 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 16/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m268s\u001b[0m 43ms/step - accuracy: 0.9995 - loss: 0.0023 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 17/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m276s\u001b[0m 45ms/step - accuracy: 0.9995 - loss: 0.0017 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 17: early stopping\n",
      "Restoring model weights from the end of the best epoch: 7.\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step\n",
      "\n",
      "Time taken for training:  09:30:12\n",
      "\n",
      "\n",
      "Fold 1 - Train Accuracy 0.9997 - Test Accuracy 0.6120\n",
      "Fold 2 - Train Accuracy 0.9996 - Test Accuracy 0.8439\n",
      "Fold 3 - Train Accuracy 0.9993 - Test Accuracy 0.9565\n",
      "Fold 4 - Train Accuracy 0.9997 - Test Accuracy 0.9542\n",
      "Fold 5 - Train Accuracy 0.9995 - Test Accuracy 0.9763\n",
      "Fold 6 - Train Accuracy 0.9994 - Test Accuracy 0.9802\n",
      "Fold 7 - Train Accuracy 0.9994 - Test Accuracy 0.9398\n",
      "Fold 8 - Train Accuracy 0.9999 - Test Accuracy 0.9670\n",
      "Fold 9 - Train Accuracy 0.9994 - Test Accuracy 0.9747\n",
      "Fold 10 - Train Accuracy 0.9998 - Test Accuracy 0.9806\n",
      "\n",
      "Mean Train Accuracy: 0.9996 - Std: 0.0002 \n",
      "Mean Test Accuracy: 0.9185 - Std: 0.1092 \n",
      "\n",
      "Evaluate other metrics:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.94      0.94     13905\n",
      "           1       0.93      0.96      0.95       723\n",
      "           2       0.95      0.98      0.96      2659\n",
      "           3       0.83      0.81      0.82      1123\n",
      "           4       0.91      0.84      0.87       384\n",
      "           5       0.90      0.84      0.87        44\n",
      "           6       0.94      0.89      0.91       663\n",
      "           7       0.96      0.91      0.93        94\n",
      "           8       0.83      0.83      0.83        12\n",
      "           9       0.91      0.89      0.90       786\n",
      "          10       0.80      0.89      0.84         9\n",
      "          11       0.89      0.87      0.88        98\n",
      "          12       0.88      0.88      0.88         8\n",
      "          13       0.90      0.90      0.90        20\n",
      "          14       0.85      0.91      0.88        77\n",
      "          15       0.91      0.84      0.87        62\n",
      "          16       0.82      0.84      0.83       917\n",
      "          17       0.87      0.89      0.88       473\n",
      "          18       0.95      0.91      0.93        22\n",
      "          19       0.90      0.88      0.89       122\n",
      "          20       0.79      0.89      0.84       111\n",
      "          21       0.88      0.86      0.87       201\n",
      "          22       0.88      1.00      0.93         7\n",
      "          23       0.84      0.84      0.84        96\n",
      "          24       0.83      0.84      0.83      1045\n",
      "          25       0.88      0.86      0.87       540\n",
      "          26       0.85      0.86      0.85      1334\n",
      "          27       0.96      0.89      0.93        28\n",
      "          28       0.89      0.94      0.92        35\n",
      "          29       0.93      0.88      0.91        77\n",
      "          30       0.90      0.86      0.88        64\n",
      "          31       1.00      0.86      0.92         7\n",
      "\n",
      "    accuracy                           0.92     25746\n",
      "   macro avg       0.89      0.88      0.89     25746\n",
      "weighted avg       0.92      0.92      0.92     25746\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Build a \"time window\" structure to handle the dataset as a time series.\n",
    "new_df = load_dataset(\"lead2\")\n",
    "X_array, y_array = build_time_window_structure(new_df, \"lead2\")\n",
    "X_array, y_array = remove_classes_with_less_samples(X_array, y_array)\n",
    "\n",
    "# Train the CNN model.\n",
    "v1_model = create_v1()\n",
    "v1_num_epochs = 300\n",
    "v1_batch_size = 32\n",
    "v1_validation_split = 0.01\n",
    "\n",
    "training_history_v1 = train_cnn_model(v1_model, X_array, y_array, v1_num_epochs, v1_batch_size, v1_validation_split, \"v1_model_lead2.keras\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba6dabf-e8cc-4899-aa7a-db4ad6b0cd7a",
   "metadata": {},
   "source": [
    "#### Lead III"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "47199c32-f584-4be7-9a65-47b703645bde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Start loading CSV file...\n",
      "Finish loading CSV file.\n",
      "\n",
      "Starting build_time_window_structure function...\n",
      "\n",
      "Shape of features:  (25770, 1250)\n",
      "Quantity os samples (labels):  25770\n",
      "\n",
      "Finishing build_time_window_structure function.\n",
      "\n",
      "Remove classes with less than 6 samples.\n",
      "\n",
      "Show classes identification:\n",
      "Class: 0 - Arrhythmia code: 1\n",
      "Class: 1 - Arrhythmia code: 21\n",
      "Class: 2 - Arrhythmia code: 22\n",
      "Class: 3 - Arrhythmia code: 23\n",
      "Class: 4 - Arrhythmia code: 30\n",
      "Class: 5 - Arrhythmia code: 36\n",
      "Class: 6 - Arrhythmia code: 50\n",
      "Class: 7 - Arrhythmia code: 51\n",
      "Class: 8 - Arrhythmia code: 54\n",
      "Class: 9 - Arrhythmia code: 60\n",
      "Class: 10 - Arrhythmia code: 80\n",
      "Class: 11 - Arrhythmia code: 82\n",
      "Class: 12 - Arrhythmia code: 83\n",
      "Class: 13 - Arrhythmia code: 88\n",
      "Class: 14 - Arrhythmia code: 101\n",
      "Class: 15 - Arrhythmia code: 104\n",
      "Class: 16 - Arrhythmia code: 105\n",
      "Class: 17 - Arrhythmia code: 106\n",
      "Class: 18 - Arrhythmia code: 108\n",
      "Class: 19 - Arrhythmia code: 120\n",
      "Class: 20 - Arrhythmia code: 121\n",
      "Class: 21 - Arrhythmia code: 125\n",
      "Class: 22 - Arrhythmia code: 140\n",
      "Class: 23 - Arrhythmia code: 142\n",
      "Class: 24 - Arrhythmia code: 145\n",
      "Class: 25 - Arrhythmia code: 146\n",
      "Class: 26 - Arrhythmia code: 147\n",
      "Class: 27 - Arrhythmia code: 155\n",
      "Class: 28 - Arrhythmia code: 160\n",
      "Class: 29 - Arrhythmia code: 161\n",
      "Class: 30 - Arrhythmia code: 165\n",
      "Class: 31 - Arrhythmia code: 166\n",
      "\n",
      "Check for dataset balance:\n",
      "Class =   0   Qty =    13905   Percentage = 54.01 %\n",
      "Class =   2   Qty =     2659   Percentage = 10.33 %\n",
      "Class =  26   Qty =     1334   Percentage = 5.18 %\n",
      "Class =   3   Qty =     1123   Percentage = 4.36 %\n",
      "Class =  24   Qty =     1045   Percentage = 4.06 %\n",
      "Class =  16   Qty =      917   Percentage = 3.56 %\n",
      "Class =   9   Qty =      786   Percentage = 3.05 %\n",
      "Class =   1   Qty =      723   Percentage = 2.81 %\n",
      "Class =   6   Qty =      663   Percentage = 2.58 %\n",
      "Class =  25   Qty =      540   Percentage = 2.10 %\n",
      "Class =  17   Qty =      473   Percentage = 1.84 %\n",
      "Class =   4   Qty =      384   Percentage = 1.49 %\n",
      "Class =  21   Qty =      201   Percentage = 0.78 %\n",
      "Class =  19   Qty =      122   Percentage = 0.47 %\n",
      "Class =  20   Qty =      111   Percentage = 0.43 %\n",
      "Class =  11   Qty =       98   Percentage = 0.38 %\n",
      "Class =  23   Qty =       96   Percentage = 0.37 %\n",
      "Class =   7   Qty =       94   Percentage = 0.37 %\n",
      "Class =  14   Qty =       77   Percentage = 0.30 %\n",
      "Class =  29   Qty =       77   Percentage = 0.30 %\n",
      "Class =  30   Qty =       64   Percentage = 0.25 %\n",
      "Class =  15   Qty =       62   Percentage = 0.24 %\n",
      "Class =   5   Qty =       44   Percentage = 0.17 %\n",
      "Class =  28   Qty =       35   Percentage = 0.14 %\n",
      "Class =  27   Qty =       28   Percentage = 0.11 %\n",
      "Class =  18   Qty =       22   Percentage = 0.09 %\n",
      "Class =  13   Qty =       20   Percentage = 0.08 %\n",
      "Class =   8   Qty =       12   Percentage = 0.05 %\n",
      "Class =  10   Qty =        9   Percentage = 0.03 %\n",
      "Class =  12   Qty =        8   Percentage = 0.03 %\n",
      "Class =  22   Qty =        7   Percentage = 0.03 %\n",
      "Class =  31   Qty =        7   Percentage = 0.03 %\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1248</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)        │            <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1248</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)        │            <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1246</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)        │           <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1246</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)        │            <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">623</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">619</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)        │           <span style=\"color: #00af00; text-decoration-color: #00af00\">656</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_2           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">619</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)        │            <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">615</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)        │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,296</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_3           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">615</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)        │            <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">307</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4912</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">628,864</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_4           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,128</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv1d (\u001b[38;5;33mConv1D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1248\u001b[0m, \u001b[38;5;34m8\u001b[0m)        │            \u001b[38;5;34m32\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1248\u001b[0m, \u001b[38;5;34m8\u001b[0m)        │            \u001b[38;5;34m32\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_1 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1246\u001b[0m, \u001b[38;5;34m8\u001b[0m)        │           \u001b[38;5;34m200\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_1           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1246\u001b[0m, \u001b[38;5;34m8\u001b[0m)        │            \u001b[38;5;34m32\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d (\u001b[38;5;33mMaxPooling1D\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m623\u001b[0m, \u001b[38;5;34m8\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_2 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m619\u001b[0m, \u001b[38;5;34m16\u001b[0m)        │           \u001b[38;5;34m656\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_2           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m619\u001b[0m, \u001b[38;5;34m16\u001b[0m)        │            \u001b[38;5;34m64\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_3 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m615\u001b[0m, \u001b[38;5;34m16\u001b[0m)        │         \u001b[38;5;34m1,296\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_3           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m615\u001b[0m, \u001b[38;5;34m16\u001b[0m)        │            \u001b[38;5;34m64\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_1 (\u001b[38;5;33mMaxPooling1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m307\u001b[0m, \u001b[38;5;34m16\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4912\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │       \u001b[38;5;34m628,864\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_4           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │           \u001b[38;5;34m512\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │         \u001b[38;5;34m4,128\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">635,880</span> (2.43 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m635,880\u001b[0m (2.43 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">635,528</span> (2.42 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m635,528\u001b[0m (2.42 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">352</span> (1.38 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m352\u001b[0m (1.38 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting training...\n",
      "\n",
      "Training fold 1\n",
      "\n",
      "Generating upsampling using SMOTE...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\DeveloperTools\\python\\3.11.0\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:737: UserWarning: The least populated class in y has only 7 members, which is less than n_splits=10.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200000 samples after upsampling.\n",
      "Class distribution for training after upsampling: Counter({np.int64(0): 6250, np.int64(1): 6250, np.int64(2): 6250, np.int64(3): 6250, np.int64(4): 6250, np.int64(5): 6250, np.int64(6): 6250, np.int64(7): 6250, np.int64(8): 6250, np.int64(9): 6250, np.int64(10): 6250, np.int64(11): 6250, np.int64(12): 6250, np.int64(13): 6250, np.int64(14): 6250, np.int64(15): 6250, np.int64(16): 6250, np.int64(17): 6250, np.int64(18): 6250, np.int64(19): 6250, np.int64(20): 6250, np.int64(21): 6250, np.int64(22): 6250, np.int64(23): 6250, np.int64(24): 6250, np.int64(25): 6250, np.int64(26): 6250, np.int64(27): 6250, np.int64(28): 6250, np.int64(29): 6250, np.int64(30): 6250, np.int64(31): 6250})\n",
      "Finishing upsampling.\n",
      "\n",
      "Epoch 1/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 17ms/step - accuracy: 0.8478 - loss: 0.5997 - val_accuracy: 1.0000 - val_loss: 2.2989e-04\n",
      "Epoch 2/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m101s\u001b[0m 16ms/step - accuracy: 0.9748 - loss: 0.0810 - val_accuracy: 1.0000 - val_loss: 1.2628e-05\n",
      "Epoch 3/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m101s\u001b[0m 16ms/step - accuracy: 0.9867 - loss: 0.0426 - val_accuracy: 1.0000 - val_loss: 3.6582e-06\n",
      "Epoch 4/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m101s\u001b[0m 16ms/step - accuracy: 0.9914 - loss: 0.0263 - val_accuracy: 1.0000 - val_loss: 1.2025e-05\n",
      "Epoch 5/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m101s\u001b[0m 16ms/step - accuracy: 0.9931 - loss: 0.0213 - val_accuracy: 1.0000 - val_loss: 3.8570e-07\n",
      "Epoch 6/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m101s\u001b[0m 16ms/step - accuracy: 0.9949 - loss: 0.0156 - val_accuracy: 1.0000 - val_loss: 3.7418e-06\n",
      "Epoch 7/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m102s\u001b[0m 16ms/step - accuracy: 0.9956 - loss: 0.0145 - val_accuracy: 1.0000 - val_loss: 3.0518e-08\n",
      "Epoch 8/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m102s\u001b[0m 17ms/step - accuracy: 0.9962 - loss: 0.0124 - val_accuracy: 1.0000 - val_loss: 1.6034e-08\n",
      "Epoch 9/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m102s\u001b[0m 16ms/step - accuracy: 0.9962 - loss: 0.0115 - val_accuracy: 1.0000 - val_loss: 8.1092e-07\n",
      "Epoch 10/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m103s\u001b[0m 17ms/step - accuracy: 0.9964 - loss: 0.0115 - val_accuracy: 1.0000 - val_loss: 6.6042e-08\n",
      "Epoch 11/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m102s\u001b[0m 17ms/step - accuracy: 0.9973 - loss: 0.0085 - val_accuracy: 1.0000 - val_loss: 4.1366e-08\n",
      "Epoch 12/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 17ms/step - accuracy: 0.9971 - loss: 0.0093 - val_accuracy: 1.0000 - val_loss: 1.4800e-07\n",
      "Epoch 13/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m103s\u001b[0m 17ms/step - accuracy: 0.9973 - loss: 0.0087 - val_accuracy: 1.0000 - val_loss: 6.7353e-09\n",
      "Epoch 14/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m103s\u001b[0m 17ms/step - accuracy: 0.9974 - loss: 0.0081 - val_accuracy: 1.0000 - val_loss: 2.0444e-08\n",
      "Epoch 15/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m137s\u001b[0m 16ms/step - accuracy: 0.9979 - loss: 0.0072 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 16/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 17ms/step - accuracy: 0.9979 - loss: 0.0075 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 17/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m103s\u001b[0m 17ms/step - accuracy: 0.9978 - loss: 0.0072 - val_accuracy: 1.0000 - val_loss: 6.9797e-08\n",
      "Epoch 18/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 17ms/step - accuracy: 0.9981 - loss: 0.0061 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 19/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 17ms/step - accuracy: 0.9979 - loss: 0.0065 - val_accuracy: 1.0000 - val_loss: 4.8280e-09\n",
      "Epoch 20/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 17ms/step - accuracy: 0.9980 - loss: 0.0065 - val_accuracy: 1.0000 - val_loss: 1.4131e-06\n",
      "Epoch 21/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 17ms/step - accuracy: 0.9983 - loss: 0.0056 - val_accuracy: 1.0000 - val_loss: 1.7834e-07\n",
      "Epoch 22/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 17ms/step - accuracy: 0.9983 - loss: 0.0056 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 23/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 17ms/step - accuracy: 0.9984 - loss: 0.0057 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 24/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 17ms/step - accuracy: 0.9985 - loss: 0.0053 - val_accuracy: 1.0000 - val_loss: 5.9605e-11\n",
      "Epoch 25/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 17ms/step - accuracy: 0.9986 - loss: 0.0048 - val_accuracy: 1.0000 - val_loss: 5.3644e-10\n",
      "Epoch 25: early stopping\n",
      "Restoring model weights from the end of the best epoch: 15.\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step\n",
      "\n",
      "Training fold 2\n",
      "\n",
      "Generating upsampling using SMOTE...\n",
      "200000 samples after upsampling.\n",
      "Class distribution for training after upsampling: Counter({np.int64(0): 6250, np.int64(1): 6250, np.int64(2): 6250, np.int64(3): 6250, np.int64(4): 6250, np.int64(5): 6250, np.int64(6): 6250, np.int64(7): 6250, np.int64(8): 6250, np.int64(9): 6250, np.int64(10): 6250, np.int64(11): 6250, np.int64(12): 6250, np.int64(13): 6250, np.int64(14): 6250, np.int64(15): 6250, np.int64(16): 6250, np.int64(17): 6250, np.int64(18): 6250, np.int64(19): 6250, np.int64(20): 6250, np.int64(21): 6250, np.int64(22): 6250, np.int64(23): 6250, np.int64(24): 6250, np.int64(25): 6250, np.int64(26): 6250, np.int64(27): 6250, np.int64(28): 6250, np.int64(29): 6250, np.int64(30): 6250, np.int64(31): 6250})\n",
      "Finishing upsampling.\n",
      "\n",
      "Epoch 1/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 17ms/step - accuracy: 0.9704 - loss: 0.1682 - val_accuracy: 1.0000 - val_loss: 2.3842e-10\n",
      "Epoch 2/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 17ms/step - accuracy: 0.9948 - loss: 0.0163 - val_accuracy: 1.0000 - val_loss: 2.2054e-08\n",
      "Epoch 3/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 17ms/step - accuracy: 0.9972 - loss: 0.0090 - val_accuracy: 1.0000 - val_loss: 4.4190e-07\n",
      "Epoch 4/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 17ms/step - accuracy: 0.9975 - loss: 0.0082 - val_accuracy: 1.0000 - val_loss: 4.9889e-08\n",
      "Epoch 5/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 17ms/step - accuracy: 0.9979 - loss: 0.0077 - val_accuracy: 1.0000 - val_loss: 2.0993e-07\n",
      "Epoch 6/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 17ms/step - accuracy: 0.9980 - loss: 0.0063 - val_accuracy: 1.0000 - val_loss: 7.2384e-06\n",
      "Epoch 7/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 17ms/step - accuracy: 0.9983 - loss: 0.0057 - val_accuracy: 1.0000 - val_loss: 3.2783e-09\n",
      "Epoch 8/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 17ms/step - accuracy: 0.9984 - loss: 0.0053 - val_accuracy: 1.0000 - val_loss: 9.3579e-09\n",
      "Epoch 9/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 17ms/step - accuracy: 0.9981 - loss: 0.0060 - val_accuracy: 1.0000 - val_loss: 5.8593e-05\n",
      "Epoch 10/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 17ms/step - accuracy: 0.9983 - loss: 0.0059 - val_accuracy: 1.0000 - val_loss: 8.3446e-10\n",
      "Epoch 11/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 17ms/step - accuracy: 0.9985 - loss: 0.0051 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 12/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 17ms/step - accuracy: 0.9984 - loss: 0.0049 - val_accuracy: 1.0000 - val_loss: 1.0104e-05\n",
      "Epoch 13/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 17ms/step - accuracy: 0.9987 - loss: 0.0051 - val_accuracy: 1.0000 - val_loss: 5.2869e-08\n",
      "Epoch 14/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 17ms/step - accuracy: 0.9986 - loss: 0.0045 - val_accuracy: 1.0000 - val_loss: 1.1325e-08\n",
      "Epoch 15/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 17ms/step - accuracy: 0.9986 - loss: 0.0046 - val_accuracy: 1.0000 - val_loss: 1.7524e-08\n",
      "Epoch 16/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 18ms/step - accuracy: 0.9987 - loss: 0.0049 - val_accuracy: 1.0000 - val_loss: 2.4438e-09\n",
      "Epoch 17/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 18ms/step - accuracy: 0.9987 - loss: 0.0040 - val_accuracy: 1.0000 - val_loss: 1.2338e-08\n",
      "Epoch 18/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 17ms/step - accuracy: 0.9987 - loss: 0.0048 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 19/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 17ms/step - accuracy: 0.9987 - loss: 0.0044 - val_accuracy: 1.0000 - val_loss: 2.5153e-08\n",
      "Epoch 20/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 17ms/step - accuracy: 0.9987 - loss: 0.0043 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 21/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 18ms/step - accuracy: 0.9988 - loss: 0.0042 - val_accuracy: 1.0000 - val_loss: 1.9812e-07\n",
      "Epoch 21: early stopping\n",
      "Restoring model weights from the end of the best epoch: 11.\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\n",
      "Training fold 3\n",
      "\n",
      "Generating upsampling using SMOTE...\n",
      "200000 samples after upsampling.\n",
      "Class distribution for training after upsampling: Counter({np.int64(0): 6250, np.int64(1): 6250, np.int64(2): 6250, np.int64(3): 6250, np.int64(4): 6250, np.int64(5): 6250, np.int64(6): 6250, np.int64(7): 6250, np.int64(8): 6250, np.int64(9): 6250, np.int64(10): 6250, np.int64(11): 6250, np.int64(12): 6250, np.int64(13): 6250, np.int64(14): 6250, np.int64(15): 6250, np.int64(16): 6250, np.int64(17): 6250, np.int64(18): 6250, np.int64(19): 6250, np.int64(20): 6250, np.int64(21): 6250, np.int64(22): 6250, np.int64(23): 6250, np.int64(24): 6250, np.int64(25): 6250, np.int64(26): 6250, np.int64(27): 6250, np.int64(28): 6250, np.int64(29): 6250, np.int64(30): 6250, np.int64(31): 6250})\n",
      "Finishing upsampling.\n",
      "\n",
      "Epoch 1/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m112s\u001b[0m 18ms/step - accuracy: 0.9870 - loss: 0.0569 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 2/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 18ms/step - accuracy: 0.9969 - loss: 0.0101 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 3/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 18ms/step - accuracy: 0.9980 - loss: 0.0074 - val_accuracy: 1.0000 - val_loss: 1.9729e-08\n",
      "Epoch 4/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 18ms/step - accuracy: 0.9982 - loss: 0.0068 - val_accuracy: 1.0000 - val_loss: 1.9670e-09\n",
      "Epoch 5/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m110s\u001b[0m 18ms/step - accuracy: 0.9985 - loss: 0.0062 - val_accuracy: 1.0000 - val_loss: 3.6001e-08\n",
      "Epoch 6/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m111s\u001b[0m 18ms/step - accuracy: 0.9987 - loss: 0.0052 - val_accuracy: 1.0000 - val_loss: 2.9206e-09\n",
      "Epoch 7/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m111s\u001b[0m 18ms/step - accuracy: 0.9986 - loss: 0.0049 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 8/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m111s\u001b[0m 18ms/step - accuracy: 0.9989 - loss: 0.0044 - val_accuracy: 1.0000 - val_loss: 5.9605e-10\n",
      "Epoch 9/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m111s\u001b[0m 18ms/step - accuracy: 0.9987 - loss: 0.0044 - val_accuracy: 1.0000 - val_loss: 1.8895e-07\n",
      "Epoch 10/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m111s\u001b[0m 18ms/step - accuracy: 0.9986 - loss: 0.0045 - val_accuracy: 1.0000 - val_loss: 5.6982e-08\n",
      "Epoch 11/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m110s\u001b[0m 18ms/step - accuracy: 0.9988 - loss: 0.0050 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 11: early stopping\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\n",
      "Training fold 4\n",
      "\n",
      "Generating upsampling using SMOTE...\n",
      "200000 samples after upsampling.\n",
      "Class distribution for training after upsampling: Counter({np.int64(0): 6250, np.int64(1): 6250, np.int64(2): 6250, np.int64(3): 6250, np.int64(4): 6250, np.int64(5): 6250, np.int64(6): 6250, np.int64(7): 6250, np.int64(8): 6250, np.int64(9): 6250, np.int64(10): 6250, np.int64(11): 6250, np.int64(12): 6250, np.int64(13): 6250, np.int64(14): 6250, np.int64(15): 6250, np.int64(16): 6250, np.int64(17): 6250, np.int64(18): 6250, np.int64(19): 6250, np.int64(20): 6250, np.int64(21): 6250, np.int64(22): 6250, np.int64(23): 6250, np.int64(24): 6250, np.int64(25): 6250, np.int64(26): 6250, np.int64(27): 6250, np.int64(28): 6250, np.int64(29): 6250, np.int64(30): 6250, np.int64(31): 6250})\n",
      "Finishing upsampling.\n",
      "\n",
      "Epoch 1/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m119s\u001b[0m 19ms/step - accuracy: 0.9934 - loss: 0.0237 - val_accuracy: 1.0000 - val_loss: 1.1706e-07\n",
      "Epoch 2/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m114s\u001b[0m 18ms/step - accuracy: 0.9976 - loss: 0.0089 - val_accuracy: 1.0000 - val_loss: 2.4892e-04\n",
      "Epoch 3/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m114s\u001b[0m 18ms/step - accuracy: 0.9981 - loss: 0.0068 - val_accuracy: 1.0000 - val_loss: 8.0764e-08\n",
      "Epoch 4/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m113s\u001b[0m 18ms/step - accuracy: 0.9987 - loss: 0.0048 - val_accuracy: 1.0000 - val_loss: 4.9293e-08\n",
      "Epoch 5/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m113s\u001b[0m 18ms/step - accuracy: 0.9985 - loss: 0.0054 - val_accuracy: 1.0000 - val_loss: 9.8944e-09\n",
      "Epoch 6/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m114s\u001b[0m 18ms/step - accuracy: 0.9987 - loss: 0.0046 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 7/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m114s\u001b[0m 18ms/step - accuracy: 0.9987 - loss: 0.0048 - val_accuracy: 1.0000 - val_loss: 6.2537e-06\n",
      "Epoch 8/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m115s\u001b[0m 19ms/step - accuracy: 0.9988 - loss: 0.0052 - val_accuracy: 1.0000 - val_loss: 4.4703e-09\n",
      "Epoch 9/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m114s\u001b[0m 18ms/step - accuracy: 0.9990 - loss: 0.0039 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 10/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m115s\u001b[0m 19ms/step - accuracy: 0.9988 - loss: 0.0039 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 11/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m115s\u001b[0m 19ms/step - accuracy: 0.9987 - loss: 0.0043 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 12/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m115s\u001b[0m 19ms/step - accuracy: 0.9989 - loss: 0.0041 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 13/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m115s\u001b[0m 19ms/step - accuracy: 0.9990 - loss: 0.0036 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 14/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m116s\u001b[0m 19ms/step - accuracy: 0.9987 - loss: 0.0042 - val_accuracy: 1.0000 - val_loss: 1.7881e-10\n",
      "Epoch 15/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m117s\u001b[0m 19ms/step - accuracy: 0.9991 - loss: 0.0032 - val_accuracy: 1.0000 - val_loss: 2.6822e-09\n",
      "Epoch 16/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m117s\u001b[0m 19ms/step - accuracy: 0.9988 - loss: 0.0044 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 16: early stopping\n",
      "Restoring model weights from the end of the best epoch: 6.\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\n",
      "Training fold 5\n",
      "\n",
      "Generating upsampling using SMOTE...\n",
      "200000 samples after upsampling.\n",
      "Class distribution for training after upsampling: Counter({np.int64(0): 6250, np.int64(1): 6250, np.int64(2): 6250, np.int64(3): 6250, np.int64(4): 6250, np.int64(5): 6250, np.int64(6): 6250, np.int64(7): 6250, np.int64(8): 6250, np.int64(9): 6250, np.int64(10): 6250, np.int64(11): 6250, np.int64(12): 6250, np.int64(13): 6250, np.int64(14): 6250, np.int64(15): 6250, np.int64(16): 6250, np.int64(17): 6250, np.int64(18): 6250, np.int64(19): 6250, np.int64(20): 6250, np.int64(21): 6250, np.int64(22): 6250, np.int64(23): 6250, np.int64(24): 6250, np.int64(25): 6250, np.int64(26): 6250, np.int64(27): 6250, np.int64(28): 6250, np.int64(29): 6250, np.int64(30): 6250, np.int64(31): 6250})\n",
      "Finishing upsampling.\n",
      "\n",
      "Epoch 1/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m112s\u001b[0m 18ms/step - accuracy: 0.9942 - loss: 0.0211 - val_accuracy: 1.0000 - val_loss: 4.8995e-07\n",
      "Epoch 2/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m110s\u001b[0m 18ms/step - accuracy: 0.9976 - loss: 0.0078 - val_accuracy: 1.0000 - val_loss: 1.9550e-08\n",
      "Epoch 3/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 17ms/step - accuracy: 0.9985 - loss: 0.0049 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 4/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m110s\u001b[0m 18ms/step - accuracy: 0.9987 - loss: 0.0048 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 5/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m111s\u001b[0m 18ms/step - accuracy: 0.9985 - loss: 0.0053 - val_accuracy: 1.0000 - val_loss: 9.7156e-09\n",
      "Epoch 6/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 18ms/step - accuracy: 0.9989 - loss: 0.0037 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 7/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m111s\u001b[0m 18ms/step - accuracy: 0.9990 - loss: 0.0035 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 8/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m111s\u001b[0m 18ms/step - accuracy: 0.9990 - loss: 0.0040 - val_accuracy: 1.0000 - val_loss: 7.0154e-08\n",
      "Epoch 9/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m110s\u001b[0m 18ms/step - accuracy: 0.9988 - loss: 0.0051 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 10/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m110s\u001b[0m 18ms/step - accuracy: 0.9989 - loss: 0.0041 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 11/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m111s\u001b[0m 18ms/step - accuracy: 0.9989 - loss: 0.0045 - val_accuracy: 1.0000 - val_loss: 4.1641e-05\n",
      "Epoch 12/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m112s\u001b[0m 18ms/step - accuracy: 0.9990 - loss: 0.0044 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 13/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m115s\u001b[0m 19ms/step - accuracy: 0.9992 - loss: 0.0036 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 13: early stopping\n",
      "Restoring model weights from the end of the best epoch: 3.\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\n",
      "Training fold 6\n",
      "\n",
      "Generating upsampling using SMOTE...\n",
      "200000 samples after upsampling.\n",
      "Class distribution for training after upsampling: Counter({np.int64(0): 6250, np.int64(1): 6250, np.int64(2): 6250, np.int64(3): 6250, np.int64(4): 6250, np.int64(5): 6250, np.int64(6): 6250, np.int64(7): 6250, np.int64(8): 6250, np.int64(9): 6250, np.int64(10): 6250, np.int64(11): 6250, np.int64(12): 6250, np.int64(13): 6250, np.int64(14): 6250, np.int64(15): 6250, np.int64(16): 6250, np.int64(17): 6250, np.int64(18): 6250, np.int64(19): 6250, np.int64(20): 6250, np.int64(21): 6250, np.int64(22): 6250, np.int64(23): 6250, np.int64(24): 6250, np.int64(25): 6250, np.int64(26): 6250, np.int64(27): 6250, np.int64(28): 6250, np.int64(29): 6250, np.int64(30): 6250, np.int64(31): 6250})\n",
      "Finishing upsampling.\n",
      "\n",
      "Epoch 1/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m115s\u001b[0m 19ms/step - accuracy: 0.9962 - loss: 0.0130 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 2/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m113s\u001b[0m 18ms/step - accuracy: 0.9983 - loss: 0.0060 - val_accuracy: 1.0000 - val_loss: 6.5565e-10\n",
      "Epoch 3/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m113s\u001b[0m 18ms/step - accuracy: 0.9985 - loss: 0.0056 - val_accuracy: 1.0000 - val_loss: 4.9196e-07\n",
      "Epoch 4/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m114s\u001b[0m 18ms/step - accuracy: 0.9985 - loss: 0.0052 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 5/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m124s\u001b[0m 20ms/step - accuracy: 0.9990 - loss: 0.0039 - val_accuracy: 1.0000 - val_loss: 2.1219e-08\n",
      "Epoch 6/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m114s\u001b[0m 18ms/step - accuracy: 0.9989 - loss: 0.0045 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 7/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m115s\u001b[0m 19ms/step - accuracy: 0.9988 - loss: 0.0043 - val_accuracy: 1.0000 - val_loss: 1.0133e-09\n",
      "Epoch 8/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m115s\u001b[0m 19ms/step - accuracy: 0.9990 - loss: 0.0040 - val_accuracy: 1.0000 - val_loss: 2.8014e-09\n",
      "Epoch 9/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m115s\u001b[0m 19ms/step - accuracy: 0.9990 - loss: 0.0039 - val_accuracy: 1.0000 - val_loss: 1.5859e-06\n",
      "Epoch 10/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m115s\u001b[0m 19ms/step - accuracy: 0.9990 - loss: 0.0039 - val_accuracy: 1.0000 - val_loss: 1.7881e-10\n",
      "Epoch 11/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m116s\u001b[0m 19ms/step - accuracy: 0.9989 - loss: 0.0041 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 11: early stopping\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\n",
      "Training fold 7\n",
      "\n",
      "Generating upsampling using SMOTE...\n",
      "200000 samples after upsampling.\n",
      "Class distribution for training after upsampling: Counter({np.int64(0): 6250, np.int64(1): 6250, np.int64(2): 6250, np.int64(3): 6250, np.int64(4): 6250, np.int64(5): 6250, np.int64(6): 6250, np.int64(7): 6250, np.int64(8): 6250, np.int64(9): 6250, np.int64(10): 6250, np.int64(11): 6250, np.int64(12): 6250, np.int64(13): 6250, np.int64(14): 6250, np.int64(15): 6250, np.int64(16): 6250, np.int64(17): 6250, np.int64(18): 6250, np.int64(19): 6250, np.int64(20): 6250, np.int64(21): 6250, np.int64(22): 6250, np.int64(23): 6250, np.int64(24): 6250, np.int64(25): 6250, np.int64(26): 6250, np.int64(27): 6250, np.int64(28): 6250, np.int64(29): 6250, np.int64(30): 6250, np.int64(31): 6250})\n",
      "Finishing upsampling.\n",
      "\n",
      "Epoch 1/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m119s\u001b[0m 19ms/step - accuracy: 0.9965 - loss: 0.0114 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 2/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m118s\u001b[0m 19ms/step - accuracy: 0.9986 - loss: 0.0051 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 3/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m117s\u001b[0m 19ms/step - accuracy: 0.9987 - loss: 0.0056 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 4/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m117s\u001b[0m 19ms/step - accuracy: 0.9987 - loss: 0.0048 - val_accuracy: 1.0000 - val_loss: 8.3446e-10\n",
      "Epoch 5/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m120s\u001b[0m 19ms/step - accuracy: 0.9989 - loss: 0.0040 - val_accuracy: 1.0000 - val_loss: 9.5963e-09\n",
      "Epoch 6/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m119s\u001b[0m 19ms/step - accuracy: 0.9990 - loss: 0.0037 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 7/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m120s\u001b[0m 19ms/step - accuracy: 0.9988 - loss: 0.0047 - val_accuracy: 1.0000 - val_loss: 1.7881e-10\n",
      "Epoch 8/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m120s\u001b[0m 19ms/step - accuracy: 0.9990 - loss: 0.0041 - val_accuracy: 1.0000 - val_loss: 2.2054e-08\n",
      "Epoch 9/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m120s\u001b[0m 19ms/step - accuracy: 0.9984 - loss: 0.0050 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 10/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m119s\u001b[0m 19ms/step - accuracy: 0.9990 - loss: 0.0042 - val_accuracy: 1.0000 - val_loss: 1.5616e-08\n",
      "Epoch 11/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m120s\u001b[0m 19ms/step - accuracy: 0.9988 - loss: 0.0046 - val_accuracy: 1.0000 - val_loss: 2.3842e-09\n",
      "Epoch 11: early stopping\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\n",
      "Training fold 8\n",
      "\n",
      "Generating upsampling using SMOTE...\n",
      "200000 samples after upsampling.\n",
      "Class distribution for training after upsampling: Counter({np.int64(0): 6250, np.int64(1): 6250, np.int64(2): 6250, np.int64(3): 6250, np.int64(4): 6250, np.int64(5): 6250, np.int64(6): 6250, np.int64(7): 6250, np.int64(8): 6250, np.int64(9): 6250, np.int64(10): 6250, np.int64(11): 6250, np.int64(12): 6250, np.int64(13): 6250, np.int64(14): 6250, np.int64(15): 6250, np.int64(16): 6250, np.int64(17): 6250, np.int64(18): 6250, np.int64(19): 6250, np.int64(20): 6250, np.int64(21): 6250, np.int64(22): 6250, np.int64(23): 6250, np.int64(24): 6250, np.int64(25): 6250, np.int64(26): 6250, np.int64(27): 6250, np.int64(28): 6250, np.int64(29): 6250, np.int64(30): 6250, np.int64(31): 6250})\n",
      "Finishing upsampling.\n",
      "\n",
      "Epoch 1/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m122s\u001b[0m 20ms/step - accuracy: 0.9970 - loss: 0.0098 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 2/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m120s\u001b[0m 19ms/step - accuracy: 0.9984 - loss: 0.0056 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 3/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m120s\u001b[0m 19ms/step - accuracy: 0.9989 - loss: 0.0042 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 4/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m121s\u001b[0m 19ms/step - accuracy: 0.9989 - loss: 0.0044 - val_accuracy: 1.0000 - val_loss: 4.5598e-08\n",
      "Epoch 5/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m120s\u001b[0m 19ms/step - accuracy: 0.9989 - loss: 0.0041 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 6/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m120s\u001b[0m 19ms/step - accuracy: 0.9989 - loss: 0.0043 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 7/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m121s\u001b[0m 19ms/step - accuracy: 0.9991 - loss: 0.0033 - val_accuracy: 1.0000 - val_loss: 3.1769e-08\n",
      "Epoch 8/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m124s\u001b[0m 20ms/step - accuracy: 0.9992 - loss: 0.0031 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 9/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m121s\u001b[0m 20ms/step - accuracy: 0.9990 - loss: 0.0043 - val_accuracy: 1.0000 - val_loss: 2.9802e-10\n",
      "Epoch 10/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m121s\u001b[0m 20ms/step - accuracy: 0.9992 - loss: 0.0035 - val_accuracy: 1.0000 - val_loss: 1.0133e-09\n",
      "Epoch 11/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m122s\u001b[0m 20ms/step - accuracy: 0.9991 - loss: 0.0043 - val_accuracy: 1.0000 - val_loss: 1.3113e-09\n",
      "Epoch 11: early stopping\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\n",
      "Training fold 9\n",
      "\n",
      "Generating upsampling using SMOTE...\n",
      "200000 samples after upsampling.\n",
      "Class distribution for training after upsampling: Counter({np.int64(0): 6250, np.int64(1): 6250, np.int64(2): 6250, np.int64(3): 6250, np.int64(4): 6250, np.int64(5): 6250, np.int64(6): 6250, np.int64(7): 6250, np.int64(8): 6250, np.int64(9): 6250, np.int64(10): 6250, np.int64(11): 6250, np.int64(12): 6250, np.int64(13): 6250, np.int64(14): 6250, np.int64(15): 6250, np.int64(16): 6250, np.int64(17): 6250, np.int64(18): 6250, np.int64(19): 6250, np.int64(20): 6250, np.int64(21): 6250, np.int64(22): 6250, np.int64(23): 6250, np.int64(24): 6250, np.int64(25): 6250, np.int64(26): 6250, np.int64(27): 6250, np.int64(28): 6250, np.int64(29): 6250, np.int64(30): 6250, np.int64(31): 6250})\n",
      "Finishing upsampling.\n",
      "\n",
      "Epoch 1/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m126s\u001b[0m 20ms/step - accuracy: 0.9971 - loss: 0.0095 - val_accuracy: 1.0000 - val_loss: 8.4161e-08\n",
      "Epoch 2/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m124s\u001b[0m 20ms/step - accuracy: 0.9987 - loss: 0.0045 - val_accuracy: 1.0000 - val_loss: 3.5301e-06\n",
      "Epoch 3/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m122s\u001b[0m 20ms/step - accuracy: 0.9988 - loss: 0.0039 - val_accuracy: 1.0000 - val_loss: 2.0403e-07\n",
      "Epoch 4/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m124s\u001b[0m 20ms/step - accuracy: 0.9990 - loss: 0.0039 - val_accuracy: 1.0000 - val_loss: 5.9605e-11\n",
      "Epoch 5/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m122s\u001b[0m 20ms/step - accuracy: 0.9989 - loss: 0.0043 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 6/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m124s\u001b[0m 20ms/step - accuracy: 0.9990 - loss: 0.0035 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 7/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m122s\u001b[0m 20ms/step - accuracy: 0.9990 - loss: 0.0031 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 8/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m124s\u001b[0m 20ms/step - accuracy: 0.9991 - loss: 0.0034 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 9/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m125s\u001b[0m 20ms/step - accuracy: 0.9992 - loss: 0.0030 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 10/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m124s\u001b[0m 20ms/step - accuracy: 0.9993 - loss: 0.0024 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 11/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m124s\u001b[0m 20ms/step - accuracy: 0.9992 - loss: 0.0031 - val_accuracy: 1.0000 - val_loss: 1.6093e-09\n",
      "Epoch 12/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m123s\u001b[0m 20ms/step - accuracy: 0.9992 - loss: 0.0033 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 13/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m123s\u001b[0m 20ms/step - accuracy: 0.9990 - loss: 0.0033 - val_accuracy: 1.0000 - val_loss: 1.1921e-10\n",
      "Epoch 14/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m123s\u001b[0m 20ms/step - accuracy: 0.9991 - loss: 0.0028 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 15/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m126s\u001b[0m 20ms/step - accuracy: 0.9990 - loss: 0.0028 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 15: early stopping\n",
      "Restoring model weights from the end of the best epoch: 5.\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\n",
      "Training fold 10\n",
      "\n",
      "Generating upsampling using SMOTE...\n",
      "200000 samples after upsampling.\n",
      "Class distribution for training after upsampling: Counter({np.int64(0): 6250, np.int64(1): 6250, np.int64(2): 6250, np.int64(3): 6250, np.int64(4): 6250, np.int64(5): 6250, np.int64(6): 6250, np.int64(7): 6250, np.int64(8): 6250, np.int64(9): 6250, np.int64(10): 6250, np.int64(11): 6250, np.int64(12): 6250, np.int64(13): 6250, np.int64(14): 6250, np.int64(15): 6250, np.int64(16): 6250, np.int64(17): 6250, np.int64(18): 6250, np.int64(19): 6250, np.int64(20): 6250, np.int64(21): 6250, np.int64(22): 6250, np.int64(23): 6250, np.int64(24): 6250, np.int64(25): 6250, np.int64(26): 6250, np.int64(27): 6250, np.int64(28): 6250, np.int64(29): 6250, np.int64(30): 6250, np.int64(31): 6250})\n",
      "Finishing upsampling.\n",
      "\n",
      "Epoch 1/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m130s\u001b[0m 21ms/step - accuracy: 0.9967 - loss: 0.0110 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 2/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m131s\u001b[0m 21ms/step - accuracy: 0.9984 - loss: 0.0055 - val_accuracy: 1.0000 - val_loss: 9.5367e-10\n",
      "Epoch 3/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m126s\u001b[0m 20ms/step - accuracy: 0.9988 - loss: 0.0051 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 4/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m124s\u001b[0m 20ms/step - accuracy: 0.9988 - loss: 0.0045 - val_accuracy: 1.0000 - val_loss: 9.2983e-09\n",
      "Epoch 5/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m127s\u001b[0m 21ms/step - accuracy: 0.9990 - loss: 0.0037 - val_accuracy: 1.0000 - val_loss: 1.5497e-09\n",
      "Epoch 6/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m127s\u001b[0m 21ms/step - accuracy: 0.9989 - loss: 0.0044 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 7/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m126s\u001b[0m 20ms/step - accuracy: 0.9989 - loss: 0.0037 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 8/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m129s\u001b[0m 21ms/step - accuracy: 0.9991 - loss: 0.0034 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 9/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m127s\u001b[0m 21ms/step - accuracy: 0.9990 - loss: 0.0039 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 10/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m128s\u001b[0m 21ms/step - accuracy: 0.9993 - loss: 0.0025 - val_accuracy: 1.0000 - val_loss: 1.5497e-09\n",
      "Epoch 11/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m131s\u001b[0m 21ms/step - accuracy: 0.9991 - loss: 0.0033 - val_accuracy: 1.0000 - val_loss: 1.7881e-10\n",
      "Epoch 11: early stopping\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step\n",
      "\n",
      "Time taken for training:  05:32:54\n",
      "\n",
      "\n",
      "Fold 1 - Train Accuracy 0.9990 - Test Accuracy 0.4555\n",
      "Fold 2 - Train Accuracy 0.9996 - Test Accuracy 0.7654\n",
      "Fold 3 - Train Accuracy 0.9986 - Test Accuracy 0.9421\n",
      "Fold 4 - Train Accuracy 0.9996 - Test Accuracy 0.9122\n",
      "Fold 5 - Train Accuracy 0.9995 - Test Accuracy 0.9639\n",
      "Fold 6 - Train Accuracy 0.9990 - Test Accuracy 0.9775\n",
      "Fold 7 - Train Accuracy 0.9996 - Test Accuracy 0.9860\n",
      "Fold 8 - Train Accuracy 0.9995 - Test Accuracy 0.9860\n",
      "Fold 9 - Train Accuracy 0.9998 - Test Accuracy 0.9736\n",
      "Fold 10 - Train Accuracy 0.9996 - Test Accuracy 0.9891\n",
      "\n",
      "Mean Train Accuracy: 0.9994 - Std: 0.0004 \n",
      "Mean Test Accuracy: 0.8951 - Std: 0.1599 \n",
      "\n",
      "Evaluate other metrics:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.91      0.92     13905\n",
      "           1       0.93      0.96      0.95       723\n",
      "           2       0.93      0.96      0.94      2659\n",
      "           3       0.79      0.84      0.81      1123\n",
      "           4       0.85      0.85      0.85       384\n",
      "           5       0.97      0.86      0.92        44\n",
      "           6       0.89      0.88      0.88       663\n",
      "           7       0.82      0.88      0.85        94\n",
      "           8       0.65      0.92      0.76        12\n",
      "           9       0.86      0.90      0.88       786\n",
      "          10       0.70      0.78      0.74         9\n",
      "          11       0.90      0.86      0.88        98\n",
      "          12       0.67      0.75      0.71         8\n",
      "          13       0.86      0.90      0.88        20\n",
      "          14       0.84      0.90      0.87        77\n",
      "          15       0.89      0.89      0.89        62\n",
      "          16       0.77      0.82      0.79       917\n",
      "          17       0.88      0.90      0.89       473\n",
      "          18       0.95      0.82      0.88        22\n",
      "          19       0.82      0.84      0.83       122\n",
      "          20       0.83      0.88      0.86       111\n",
      "          21       0.88      0.83      0.85       201\n",
      "          22       0.70      1.00      0.82         7\n",
      "          23       0.85      0.82      0.84        96\n",
      "          24       0.78      0.86      0.82      1045\n",
      "          25       0.81      0.85      0.83       540\n",
      "          26       0.75      0.82      0.78      1334\n",
      "          27       0.74      0.82      0.78        28\n",
      "          28       0.94      0.91      0.93        35\n",
      "          29       0.83      0.87      0.85        77\n",
      "          30       0.89      0.86      0.87        64\n",
      "          31       0.75      0.86      0.80         7\n",
      "\n",
      "    accuracy                           0.90     25746\n",
      "   macro avg       0.83      0.87      0.85     25746\n",
      "weighted avg       0.90      0.90      0.90     25746\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Build a \"time window\" structure to handle the dataset as a time series.\n",
    "new_df = load_dataset(\"lead3\")\n",
    "X_array, y_array = build_time_window_structure(new_df, \"lead3\")\n",
    "X_array, y_array = remove_classes_with_less_samples(X_array, y_array)\n",
    "\n",
    "# Train the CNN model.\n",
    "v1_model = create_v1()\n",
    "v1_num_epochs = 300\n",
    "v1_batch_size = 32\n",
    "v1_validation_split = 0.01\n",
    "\n",
    "training_history_v1 = train_cnn_model(v1_model, X_array, y_array, v1_num_epochs, v1_batch_size, v1_validation_split, \"v1_model_lead3.keras\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54846ed-31bc-430c-85e0-6d212656e48b",
   "metadata": {},
   "source": [
    "#### aVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e435817e-25fc-4477-a72c-f5c760204725",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Start loading CSV file...\n",
      "Finish loading CSV file.\n",
      "\n",
      "Starting build_time_window_structure function...\n",
      "\n",
      "Shape of features:  (25770, 1250)\n",
      "Quantity os samples (labels):  25770\n",
      "\n",
      "Finishing build_time_window_structure function.\n",
      "\n",
      "Remove classes with less than 6 samples.\n",
      "\n",
      "Show classes identification:\n",
      "Class: 0 - Arrhythmia code: 1\n",
      "Class: 1 - Arrhythmia code: 21\n",
      "Class: 2 - Arrhythmia code: 22\n",
      "Class: 3 - Arrhythmia code: 23\n",
      "Class: 4 - Arrhythmia code: 30\n",
      "Class: 5 - Arrhythmia code: 36\n",
      "Class: 6 - Arrhythmia code: 50\n",
      "Class: 7 - Arrhythmia code: 51\n",
      "Class: 8 - Arrhythmia code: 54\n",
      "Class: 9 - Arrhythmia code: 60\n",
      "Class: 10 - Arrhythmia code: 80\n",
      "Class: 11 - Arrhythmia code: 82\n",
      "Class: 12 - Arrhythmia code: 83\n",
      "Class: 13 - Arrhythmia code: 88\n",
      "Class: 14 - Arrhythmia code: 101\n",
      "Class: 15 - Arrhythmia code: 104\n",
      "Class: 16 - Arrhythmia code: 105\n",
      "Class: 17 - Arrhythmia code: 106\n",
      "Class: 18 - Arrhythmia code: 108\n",
      "Class: 19 - Arrhythmia code: 120\n",
      "Class: 20 - Arrhythmia code: 121\n",
      "Class: 21 - Arrhythmia code: 125\n",
      "Class: 22 - Arrhythmia code: 140\n",
      "Class: 23 - Arrhythmia code: 142\n",
      "Class: 24 - Arrhythmia code: 145\n",
      "Class: 25 - Arrhythmia code: 146\n",
      "Class: 26 - Arrhythmia code: 147\n",
      "Class: 27 - Arrhythmia code: 155\n",
      "Class: 28 - Arrhythmia code: 160\n",
      "Class: 29 - Arrhythmia code: 161\n",
      "Class: 30 - Arrhythmia code: 165\n",
      "Class: 31 - Arrhythmia code: 166\n",
      "\n",
      "Check for dataset balance:\n",
      "Class =   0   Qty =    13905   Percentage = 54.01 %\n",
      "Class =   2   Qty =     2659   Percentage = 10.33 %\n",
      "Class =  26   Qty =     1334   Percentage = 5.18 %\n",
      "Class =   3   Qty =     1123   Percentage = 4.36 %\n",
      "Class =  24   Qty =     1045   Percentage = 4.06 %\n",
      "Class =  16   Qty =      917   Percentage = 3.56 %\n",
      "Class =   9   Qty =      786   Percentage = 3.05 %\n",
      "Class =   1   Qty =      723   Percentage = 2.81 %\n",
      "Class =   6   Qty =      663   Percentage = 2.58 %\n",
      "Class =  25   Qty =      540   Percentage = 2.10 %\n",
      "Class =  17   Qty =      473   Percentage = 1.84 %\n",
      "Class =   4   Qty =      384   Percentage = 1.49 %\n",
      "Class =  21   Qty =      201   Percentage = 0.78 %\n",
      "Class =  19   Qty =      122   Percentage = 0.47 %\n",
      "Class =  20   Qty =      111   Percentage = 0.43 %\n",
      "Class =  11   Qty =       98   Percentage = 0.38 %\n",
      "Class =  23   Qty =       96   Percentage = 0.37 %\n",
      "Class =   7   Qty =       94   Percentage = 0.37 %\n",
      "Class =  14   Qty =       77   Percentage = 0.30 %\n",
      "Class =  29   Qty =       77   Percentage = 0.30 %\n",
      "Class =  30   Qty =       64   Percentage = 0.25 %\n",
      "Class =  15   Qty =       62   Percentage = 0.24 %\n",
      "Class =   5   Qty =       44   Percentage = 0.17 %\n",
      "Class =  28   Qty =       35   Percentage = 0.14 %\n",
      "Class =  27   Qty =       28   Percentage = 0.11 %\n",
      "Class =  18   Qty =       22   Percentage = 0.09 %\n",
      "Class =  13   Qty =       20   Percentage = 0.08 %\n",
      "Class =   8   Qty =       12   Percentage = 0.05 %\n",
      "Class =  10   Qty =        9   Percentage = 0.03 %\n",
      "Class =  12   Qty =        8   Percentage = 0.03 %\n",
      "Class =  22   Qty =        7   Percentage = 0.03 %\n",
      "Class =  31   Qty =        7   Percentage = 0.03 %\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1248</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)        │            <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1248</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)        │            <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1246</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)        │           <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1246</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)        │            <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">623</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">619</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)        │           <span style=\"color: #00af00; text-decoration-color: #00af00\">656</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_2           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">619</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)        │            <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">615</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)        │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,296</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_3           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">615</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)        │            <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">307</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4912</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">628,864</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_4           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,128</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv1d (\u001b[38;5;33mConv1D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1248\u001b[0m, \u001b[38;5;34m8\u001b[0m)        │            \u001b[38;5;34m32\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1248\u001b[0m, \u001b[38;5;34m8\u001b[0m)        │            \u001b[38;5;34m32\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_1 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1246\u001b[0m, \u001b[38;5;34m8\u001b[0m)        │           \u001b[38;5;34m200\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_1           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1246\u001b[0m, \u001b[38;5;34m8\u001b[0m)        │            \u001b[38;5;34m32\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d (\u001b[38;5;33mMaxPooling1D\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m623\u001b[0m, \u001b[38;5;34m8\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_2 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m619\u001b[0m, \u001b[38;5;34m16\u001b[0m)        │           \u001b[38;5;34m656\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_2           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m619\u001b[0m, \u001b[38;5;34m16\u001b[0m)        │            \u001b[38;5;34m64\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_3 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m615\u001b[0m, \u001b[38;5;34m16\u001b[0m)        │         \u001b[38;5;34m1,296\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_3           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m615\u001b[0m, \u001b[38;5;34m16\u001b[0m)        │            \u001b[38;5;34m64\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_1 (\u001b[38;5;33mMaxPooling1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m307\u001b[0m, \u001b[38;5;34m16\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4912\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │       \u001b[38;5;34m628,864\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_4           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │           \u001b[38;5;34m512\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │         \u001b[38;5;34m4,128\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">635,880</span> (2.43 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m635,880\u001b[0m (2.43 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">635,528</span> (2.42 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m635,528\u001b[0m (2.42 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">352</span> (1.38 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m352\u001b[0m (1.38 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting training...\n",
      "\n",
      "Training fold 1\n",
      "\n",
      "Generating upsampling using SMOTE...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\DeveloperTools\\python\\3.11.0\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:737: UserWarning: The least populated class in y has only 7 members, which is less than n_splits=10.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200000 samples after upsampling.\n",
      "Class distribution for training after upsampling: Counter({np.int64(0): 6250, np.int64(1): 6250, np.int64(2): 6250, np.int64(3): 6250, np.int64(4): 6250, np.int64(5): 6250, np.int64(6): 6250, np.int64(7): 6250, np.int64(8): 6250, np.int64(9): 6250, np.int64(10): 6250, np.int64(11): 6250, np.int64(12): 6250, np.int64(13): 6250, np.int64(14): 6250, np.int64(15): 6250, np.int64(16): 6250, np.int64(17): 6250, np.int64(18): 6250, np.int64(19): 6250, np.int64(20): 6250, np.int64(21): 6250, np.int64(22): 6250, np.int64(23): 6250, np.int64(24): 6250, np.int64(25): 6250, np.int64(26): 6250, np.int64(27): 6250, np.int64(28): 6250, np.int64(29): 6250, np.int64(30): 6250, np.int64(31): 6250})\n",
      "Finishing upsampling.\n",
      "\n",
      "Epoch 1/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 16ms/step - accuracy: 0.8709 - loss: 0.4969 - val_accuracy: 1.0000 - val_loss: 6.8303e-05\n",
      "Epoch 2/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m100s\u001b[0m 16ms/step - accuracy: 0.9780 - loss: 0.0697 - val_accuracy: 1.0000 - val_loss: 1.8531e-04\n",
      "Epoch 3/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m101s\u001b[0m 16ms/step - accuracy: 0.9898 - loss: 0.0342 - val_accuracy: 1.0000 - val_loss: 1.2819e-06\n",
      "Epoch 4/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m101s\u001b[0m 16ms/step - accuracy: 0.9935 - loss: 0.0204 - val_accuracy: 1.0000 - val_loss: 6.4867e-06\n",
      "Epoch 5/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m101s\u001b[0m 16ms/step - accuracy: 0.9942 - loss: 0.0181 - val_accuracy: 1.0000 - val_loss: 1.8287e-06\n",
      "Epoch 6/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m100s\u001b[0m 16ms/step - accuracy: 0.9953 - loss: 0.0140 - val_accuracy: 1.0000 - val_loss: 1.7637e-07\n",
      "Epoch 7/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m101s\u001b[0m 16ms/step - accuracy: 0.9966 - loss: 0.0103 - val_accuracy: 1.0000 - val_loss: 4.6413e-07\n",
      "Epoch 8/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m102s\u001b[0m 16ms/step - accuracy: 0.9968 - loss: 0.0099 - val_accuracy: 1.0000 - val_loss: 4.4744e-06\n",
      "Epoch 9/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m103s\u001b[0m 17ms/step - accuracy: 0.9970 - loss: 0.0091 - val_accuracy: 1.0000 - val_loss: 7.9063e-07\n",
      "Epoch 10/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m103s\u001b[0m 17ms/step - accuracy: 0.9976 - loss: 0.0079 - val_accuracy: 1.0000 - val_loss: 2.3130e-06\n",
      "Epoch 11/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m103s\u001b[0m 17ms/step - accuracy: 0.9971 - loss: 0.0091 - val_accuracy: 1.0000 - val_loss: 3.0577e-08\n",
      "Epoch 12/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m103s\u001b[0m 17ms/step - accuracy: 0.9982 - loss: 0.0064 - val_accuracy: 1.0000 - val_loss: 1.5914e-08\n",
      "Epoch 13/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 17ms/step - accuracy: 0.9977 - loss: 0.0071 - val_accuracy: 1.0000 - val_loss: 2.3824e-07\n",
      "Epoch 14/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 17ms/step - accuracy: 0.9980 - loss: 0.0063 - val_accuracy: 1.0000 - val_loss: 1.9491e-08\n",
      "Epoch 15/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 17ms/step - accuracy: 0.9982 - loss: 0.0058 - val_accuracy: 1.0000 - val_loss: 5.3644e-10\n",
      "Epoch 16/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 17ms/step - accuracy: 0.9982 - loss: 0.0060 - val_accuracy: 1.0000 - val_loss: 9.0003e-09\n",
      "Epoch 17/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 17ms/step - accuracy: 0.9986 - loss: 0.0049 - val_accuracy: 1.0000 - val_loss: 2.0492e-07\n",
      "Epoch 18/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 17ms/step - accuracy: 0.9984 - loss: 0.0052 - val_accuracy: 1.0000 - val_loss: 2.2232e-08\n",
      "Epoch 19/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 17ms/step - accuracy: 0.9985 - loss: 0.0050 - val_accuracy: 1.0000 - val_loss: 8.6427e-09\n",
      "Epoch 20/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 17ms/step - accuracy: 0.9985 - loss: 0.0050 - val_accuracy: 1.0000 - val_loss: 2.1577e-08\n",
      "Epoch 21/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 17ms/step - accuracy: 0.9988 - loss: 0.0042 - val_accuracy: 1.0000 - val_loss: 2.3842e-10\n",
      "Epoch 22/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 17ms/step - accuracy: 0.9986 - loss: 0.0041 - val_accuracy: 1.0000 - val_loss: 1.6570e-08\n",
      "Epoch 23/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 17ms/step - accuracy: 0.9989 - loss: 0.0039 - val_accuracy: 1.0000 - val_loss: 1.0133e-09\n",
      "Epoch 24/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 18ms/step - accuracy: 0.9989 - loss: 0.0039 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 25/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 18ms/step - accuracy: 0.9988 - loss: 0.0038 - val_accuracy: 1.0000 - val_loss: 2.7418e-09\n",
      "Epoch 26/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 18ms/step - accuracy: 0.9985 - loss: 0.0047 - val_accuracy: 1.0000 - val_loss: 1.0133e-09\n",
      "Epoch 27/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m110s\u001b[0m 18ms/step - accuracy: 0.9989 - loss: 0.0035 - val_accuracy: 1.0000 - val_loss: 5.2452e-09\n",
      "Epoch 28/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m110s\u001b[0m 18ms/step - accuracy: 0.9992 - loss: 0.0026 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 29/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 18ms/step - accuracy: 0.9989 - loss: 0.0030 - val_accuracy: 1.0000 - val_loss: 1.5497e-09\n",
      "Epoch 30/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 18ms/step - accuracy: 0.9989 - loss: 0.0034 - val_accuracy: 1.0000 - val_loss: 4.7684e-10\n",
      "Epoch 31/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m111s\u001b[0m 18ms/step - accuracy: 0.9991 - loss: 0.0028 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 32/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m110s\u001b[0m 18ms/step - accuracy: 0.9986 - loss: 0.0041 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 33/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m111s\u001b[0m 18ms/step - accuracy: 0.9991 - loss: 0.0033 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 34/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m112s\u001b[0m 18ms/step - accuracy: 0.9990 - loss: 0.0030 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 34: early stopping\n",
      "Restoring model weights from the end of the best epoch: 24.\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step\n",
      "\n",
      "Training fold 2\n",
      "\n",
      "Generating upsampling using SMOTE...\n",
      "200000 samples after upsampling.\n",
      "Class distribution for training after upsampling: Counter({np.int64(0): 6250, np.int64(1): 6250, np.int64(2): 6250, np.int64(3): 6250, np.int64(4): 6250, np.int64(5): 6250, np.int64(6): 6250, np.int64(7): 6250, np.int64(8): 6250, np.int64(9): 6250, np.int64(10): 6250, np.int64(11): 6250, np.int64(12): 6250, np.int64(13): 6250, np.int64(14): 6250, np.int64(15): 6250, np.int64(16): 6250, np.int64(17): 6250, np.int64(18): 6250, np.int64(19): 6250, np.int64(20): 6250, np.int64(21): 6250, np.int64(22): 6250, np.int64(23): 6250, np.int64(24): 6250, np.int64(25): 6250, np.int64(26): 6250, np.int64(27): 6250, np.int64(28): 6250, np.int64(29): 6250, np.int64(30): 6250, np.int64(31): 6250})\n",
      "Finishing upsampling.\n",
      "\n",
      "Epoch 1/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m117s\u001b[0m 19ms/step - accuracy: 0.9765 - loss: 0.1438 - val_accuracy: 1.0000 - val_loss: 3.3975e-09\n",
      "Epoch 2/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m112s\u001b[0m 18ms/step - accuracy: 0.9956 - loss: 0.0141 - val_accuracy: 1.0000 - val_loss: 1.3769e-08\n",
      "Epoch 3/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m112s\u001b[0m 18ms/step - accuracy: 0.9976 - loss: 0.0081 - val_accuracy: 1.0000 - val_loss: 3.2783e-09\n",
      "Epoch 4/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m111s\u001b[0m 18ms/step - accuracy: 0.9983 - loss: 0.0055 - val_accuracy: 1.0000 - val_loss: 1.9133e-08\n",
      "Epoch 5/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m113s\u001b[0m 18ms/step - accuracy: 0.9986 - loss: 0.0042 - val_accuracy: 1.0000 - val_loss: 4.0531e-08\n",
      "Epoch 6/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m114s\u001b[0m 18ms/step - accuracy: 0.9987 - loss: 0.0047 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 7/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m112s\u001b[0m 18ms/step - accuracy: 0.9988 - loss: 0.0038 - val_accuracy: 1.0000 - val_loss: 1.7881e-09\n",
      "Epoch 8/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m112s\u001b[0m 18ms/step - accuracy: 0.9990 - loss: 0.0033 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 9/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m113s\u001b[0m 18ms/step - accuracy: 0.9990 - loss: 0.0034 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 10/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m115s\u001b[0m 19ms/step - accuracy: 0.9989 - loss: 0.0041 - val_accuracy: 1.0000 - val_loss: 1.7285e-09\n",
      "Epoch 11/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m114s\u001b[0m 18ms/step - accuracy: 0.9990 - loss: 0.0031 - val_accuracy: 1.0000 - val_loss: 3.2187e-09\n",
      "Epoch 12/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m115s\u001b[0m 19ms/step - accuracy: 0.9989 - loss: 0.0032 - val_accuracy: 1.0000 - val_loss: 1.3709e-09\n",
      "Epoch 13/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m117s\u001b[0m 19ms/step - accuracy: 0.9989 - loss: 0.0034 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 14/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m115s\u001b[0m 19ms/step - accuracy: 0.9991 - loss: 0.0030 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 15/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m116s\u001b[0m 19ms/step - accuracy: 0.9992 - loss: 0.0027 - val_accuracy: 1.0000 - val_loss: 5.3644e-10\n",
      "Epoch 16/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m117s\u001b[0m 19ms/step - accuracy: 0.9991 - loss: 0.0031 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 16: early stopping\n",
      "Restoring model weights from the end of the best epoch: 6.\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step\n",
      "\n",
      "Training fold 3\n",
      "\n",
      "Generating upsampling using SMOTE...\n",
      "200000 samples after upsampling.\n",
      "Class distribution for training after upsampling: Counter({np.int64(0): 6250, np.int64(1): 6250, np.int64(2): 6250, np.int64(3): 6250, np.int64(4): 6250, np.int64(5): 6250, np.int64(6): 6250, np.int64(7): 6250, np.int64(8): 6250, np.int64(9): 6250, np.int64(10): 6250, np.int64(11): 6250, np.int64(12): 6250, np.int64(13): 6250, np.int64(14): 6250, np.int64(15): 6250, np.int64(16): 6250, np.int64(17): 6250, np.int64(18): 6250, np.int64(19): 6250, np.int64(20): 6250, np.int64(21): 6250, np.int64(22): 6250, np.int64(23): 6250, np.int64(24): 6250, np.int64(25): 6250, np.int64(26): 6250, np.int64(27): 6250, np.int64(28): 6250, np.int64(29): 6250, np.int64(30): 6250, np.int64(31): 6250})\n",
      "Finishing upsampling.\n",
      "\n",
      "Epoch 1/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m132s\u001b[0m 21ms/step - accuracy: 0.9920 - loss: 0.0322 - val_accuracy: 1.0000 - val_loss: 1.2040e-08\n",
      "Epoch 2/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m131s\u001b[0m 21ms/step - accuracy: 0.9978 - loss: 0.0076 - val_accuracy: 1.0000 - val_loss: 3.2783e-09\n",
      "Epoch 3/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m128s\u001b[0m 21ms/step - accuracy: 0.9986 - loss: 0.0055 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 4/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m128s\u001b[0m 21ms/step - accuracy: 0.9988 - loss: 0.0039 - val_accuracy: 1.0000 - val_loss: 6.6876e-08\n",
      "Epoch 5/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m129s\u001b[0m 21ms/step - accuracy: 0.9988 - loss: 0.0040 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 6/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m131s\u001b[0m 21ms/step - accuracy: 0.9990 - loss: 0.0036 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 7/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m132s\u001b[0m 21ms/step - accuracy: 0.9989 - loss: 0.0033 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 8/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m133s\u001b[0m 21ms/step - accuracy: 0.9993 - loss: 0.0026 - val_accuracy: 1.0000 - val_loss: 4.7088e-09\n",
      "Epoch 9/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m134s\u001b[0m 22ms/step - accuracy: 0.9991 - loss: 0.0037 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 10/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m133s\u001b[0m 21ms/step - accuracy: 0.9991 - loss: 0.0032 - val_accuracy: 1.0000 - val_loss: 2.8610e-08\n",
      "Epoch 11/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m135s\u001b[0m 22ms/step - accuracy: 0.9994 - loss: 0.0025 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 12/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m136s\u001b[0m 22ms/step - accuracy: 0.9992 - loss: 0.0030 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 13/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m136s\u001b[0m 22ms/step - accuracy: 0.9992 - loss: 0.0033 - val_accuracy: 1.0000 - val_loss: 7.2122e-09\n",
      "Epoch 13: early stopping\n",
      "Restoring model weights from the end of the best epoch: 3.\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "\n",
      "Training fold 4\n",
      "\n",
      "Generating upsampling using SMOTE...\n",
      "200000 samples after upsampling.\n",
      "Class distribution for training after upsampling: Counter({np.int64(0): 6250, np.int64(1): 6250, np.int64(2): 6250, np.int64(3): 6250, np.int64(4): 6250, np.int64(5): 6250, np.int64(6): 6250, np.int64(7): 6250, np.int64(8): 6250, np.int64(9): 6250, np.int64(10): 6250, np.int64(11): 6250, np.int64(12): 6250, np.int64(13): 6250, np.int64(14): 6250, np.int64(15): 6250, np.int64(16): 6250, np.int64(17): 6250, np.int64(18): 6250, np.int64(19): 6250, np.int64(20): 6250, np.int64(21): 6250, np.int64(22): 6250, np.int64(23): 6250, np.int64(24): 6250, np.int64(25): 6250, np.int64(26): 6250, np.int64(27): 6250, np.int64(28): 6250, np.int64(29): 6250, np.int64(30): 6250, np.int64(31): 6250})\n",
      "Finishing upsampling.\n",
      "\n",
      "Epoch 1/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 23ms/step - accuracy: 0.9958 - loss: 0.0152 - val_accuracy: 1.0000 - val_loss: 1.3804e-07\n",
      "Epoch 2/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 23ms/step - accuracy: 0.9985 - loss: 0.0055 - val_accuracy: 1.0000 - val_loss: 1.0133e-09\n",
      "Epoch 3/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m139s\u001b[0m 22ms/step - accuracy: 0.9987 - loss: 0.0041 - val_accuracy: 1.0000 - val_loss: 1.1861e-08\n",
      "Epoch 4/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 23ms/step - accuracy: 0.9988 - loss: 0.0039 - val_accuracy: 1.0000 - val_loss: 1.8763e-07\n",
      "Epoch 5/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 23ms/step - accuracy: 0.9990 - loss: 0.0032 - val_accuracy: 1.0000 - val_loss: 1.1921e-10\n",
      "Epoch 6/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 23ms/step - accuracy: 0.9991 - loss: 0.0032 - val_accuracy: 1.0000 - val_loss: 2.7478e-08\n",
      "Epoch 7/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 23ms/step - accuracy: 0.9990 - loss: 0.0037 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 8/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 23ms/step - accuracy: 0.9990 - loss: 0.0036 - val_accuracy: 1.0000 - val_loss: 4.1723e-10\n",
      "Epoch 9/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 23ms/step - accuracy: 0.9994 - loss: 0.0027 - val_accuracy: 1.0000 - val_loss: 2.8372e-08\n",
      "Epoch 10/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m145s\u001b[0m 23ms/step - accuracy: 0.9992 - loss: 0.0034 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 11/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 23ms/step - accuracy: 0.9993 - loss: 0.0029 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 12/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 23ms/step - accuracy: 0.9991 - loss: 0.0031 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 13/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m146s\u001b[0m 24ms/step - accuracy: 0.9994 - loss: 0.0036 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 14/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m145s\u001b[0m 24ms/step - accuracy: 0.9992 - loss: 0.0032 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 15/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m146s\u001b[0m 24ms/step - accuracy: 0.9992 - loss: 0.0026 - val_accuracy: 1.0000 - val_loss: 7.3910e-09\n",
      "Epoch 16/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m146s\u001b[0m 24ms/step - accuracy: 0.9993 - loss: 0.0023 - val_accuracy: 1.0000 - val_loss: 6.7710e-08\n",
      "Epoch 17/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m146s\u001b[0m 24ms/step - accuracy: 0.9993 - loss: 0.0027 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 17: early stopping\n",
      "Restoring model weights from the end of the best epoch: 7.\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step\n",
      "\n",
      "Training fold 5\n",
      "\n",
      "Generating upsampling using SMOTE...\n",
      "200000 samples after upsampling.\n",
      "Class distribution for training after upsampling: Counter({np.int64(0): 6250, np.int64(1): 6250, np.int64(2): 6250, np.int64(3): 6250, np.int64(4): 6250, np.int64(5): 6250, np.int64(6): 6250, np.int64(7): 6250, np.int64(8): 6250, np.int64(9): 6250, np.int64(10): 6250, np.int64(11): 6250, np.int64(12): 6250, np.int64(13): 6250, np.int64(14): 6250, np.int64(15): 6250, np.int64(16): 6250, np.int64(17): 6250, np.int64(18): 6250, np.int64(19): 6250, np.int64(20): 6250, np.int64(21): 6250, np.int64(22): 6250, np.int64(23): 6250, np.int64(24): 6250, np.int64(25): 6250, np.int64(26): 6250, np.int64(27): 6250, np.int64(28): 6250, np.int64(29): 6250, np.int64(30): 6250, np.int64(31): 6250})\n",
      "Finishing upsampling.\n",
      "\n",
      "Epoch 1/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m154s\u001b[0m 25ms/step - accuracy: 0.9955 - loss: 0.0165 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 2/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m151s\u001b[0m 24ms/step - accuracy: 0.9986 - loss: 0.0042 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 3/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m151s\u001b[0m 24ms/step - accuracy: 0.9987 - loss: 0.0041 - val_accuracy: 1.0000 - val_loss: 2.5034e-09\n",
      "Epoch 4/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m149s\u001b[0m 24ms/step - accuracy: 0.9991 - loss: 0.0045 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 5/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m150s\u001b[0m 24ms/step - accuracy: 0.9991 - loss: 0.0034 - val_accuracy: 1.0000 - val_loss: 7.7486e-10\n",
      "Epoch 6/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m152s\u001b[0m 25ms/step - accuracy: 0.9991 - loss: 0.0033 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 7/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m152s\u001b[0m 25ms/step - accuracy: 0.9992 - loss: 0.0030 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 8/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m152s\u001b[0m 25ms/step - accuracy: 0.9993 - loss: 0.0025 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 9/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m151s\u001b[0m 24ms/step - accuracy: 0.9990 - loss: 0.0034 - val_accuracy: 1.0000 - val_loss: 2.3842e-10\n",
      "Epoch 10/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m153s\u001b[0m 25ms/step - accuracy: 0.9993 - loss: 0.0027 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 11/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m153s\u001b[0m 25ms/step - accuracy: 0.9994 - loss: 0.0022 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 11: early stopping\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step\n",
      "\n",
      "Training fold 6\n",
      "\n",
      "Generating upsampling using SMOTE...\n",
      "200000 samples after upsampling.\n",
      "Class distribution for training after upsampling: Counter({np.int64(0): 6250, np.int64(1): 6250, np.int64(2): 6250, np.int64(3): 6250, np.int64(4): 6250, np.int64(5): 6250, np.int64(6): 6250, np.int64(7): 6250, np.int64(8): 6250, np.int64(9): 6250, np.int64(10): 6250, np.int64(11): 6250, np.int64(12): 6250, np.int64(13): 6250, np.int64(14): 6250, np.int64(15): 6250, np.int64(16): 6250, np.int64(17): 6250, np.int64(18): 6250, np.int64(19): 6250, np.int64(20): 6250, np.int64(21): 6250, np.int64(22): 6250, np.int64(23): 6250, np.int64(24): 6250, np.int64(25): 6250, np.int64(26): 6250, np.int64(27): 6250, np.int64(28): 6250, np.int64(29): 6250, np.int64(30): 6250, np.int64(31): 6250})\n",
      "Finishing upsampling.\n",
      "\n",
      "Epoch 1/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m166s\u001b[0m 27ms/step - accuracy: 0.9974 - loss: 0.0091 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 2/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m159s\u001b[0m 26ms/step - accuracy: 0.9988 - loss: 0.0044 - val_accuracy: 1.0000 - val_loss: 1.1742e-08\n",
      "Epoch 3/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m157s\u001b[0m 25ms/step - accuracy: 0.9990 - loss: 0.0047 - val_accuracy: 1.0000 - val_loss: 1.0610e-08\n",
      "Epoch 4/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m160s\u001b[0m 26ms/step - accuracy: 0.9989 - loss: 0.0045 - val_accuracy: 1.0000 - val_loss: 1.3649e-08\n",
      "Epoch 5/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m161s\u001b[0m 26ms/step - accuracy: 0.9993 - loss: 0.0029 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 6/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m160s\u001b[0m 26ms/step - accuracy: 0.9991 - loss: 0.0032 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 7/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m161s\u001b[0m 26ms/step - accuracy: 0.9992 - loss: 0.0030 - val_accuracy: 1.0000 - val_loss: 4.2675e-06\n",
      "Epoch 8/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m161s\u001b[0m 26ms/step - accuracy: 0.9993 - loss: 0.0023 - val_accuracy: 1.0000 - val_loss: 6.0201e-09\n",
      "Epoch 9/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m162s\u001b[0m 26ms/step - accuracy: 0.9992 - loss: 0.0029 - val_accuracy: 1.0000 - val_loss: 4.7684e-10\n",
      "Epoch 10/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m165s\u001b[0m 27ms/step - accuracy: 0.9994 - loss: 0.0026 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 11/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m163s\u001b[0m 26ms/step - accuracy: 0.9992 - loss: 0.0029 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 11: early stopping\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step\n",
      "\n",
      "Training fold 7\n",
      "\n",
      "Generating upsampling using SMOTE...\n",
      "200000 samples after upsampling.\n",
      "Class distribution for training after upsampling: Counter({np.int64(0): 6250, np.int64(1): 6250, np.int64(2): 6250, np.int64(3): 6250, np.int64(4): 6250, np.int64(5): 6250, np.int64(6): 6250, np.int64(7): 6250, np.int64(8): 6250, np.int64(9): 6250, np.int64(10): 6250, np.int64(11): 6250, np.int64(12): 6250, np.int64(13): 6250, np.int64(14): 6250, np.int64(15): 6250, np.int64(16): 6250, np.int64(17): 6250, np.int64(18): 6250, np.int64(19): 6250, np.int64(20): 6250, np.int64(21): 6250, np.int64(22): 6250, np.int64(23): 6250, np.int64(24): 6250, np.int64(25): 6250, np.int64(26): 6250, np.int64(27): 6250, np.int64(28): 6250, np.int64(29): 6250, np.int64(30): 6250, np.int64(31): 6250})\n",
      "Finishing upsampling.\n",
      "\n",
      "Epoch 1/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m176s\u001b[0m 28ms/step - accuracy: 0.9976 - loss: 0.0081 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 2/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m173s\u001b[0m 28ms/step - accuracy: 0.9988 - loss: 0.0036 - val_accuracy: 1.0000 - val_loss: 5.5313e-08\n",
      "Epoch 3/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m173s\u001b[0m 28ms/step - accuracy: 0.9990 - loss: 0.0031 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 4/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m173s\u001b[0m 28ms/step - accuracy: 0.9991 - loss: 0.0030 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 5/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m173s\u001b[0m 28ms/step - accuracy: 0.9991 - loss: 0.0029 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 6/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m173s\u001b[0m 28ms/step - accuracy: 0.9991 - loss: 0.0036 - val_accuracy: 1.0000 - val_loss: 2.2054e-09\n",
      "Epoch 7/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m175s\u001b[0m 28ms/step - accuracy: 0.9992 - loss: 0.0029 - val_accuracy: 1.0000 - val_loss: 5.1617e-08\n",
      "Epoch 8/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m175s\u001b[0m 28ms/step - accuracy: 0.9993 - loss: 0.0027 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 9/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m177s\u001b[0m 29ms/step - accuracy: 0.9992 - loss: 0.0023 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 10/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m177s\u001b[0m 29ms/step - accuracy: 0.9992 - loss: 0.0027 - val_accuracy: 1.0000 - val_loss: 5.3644e-10\n",
      "Epoch 11/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m177s\u001b[0m 29ms/step - accuracy: 0.9993 - loss: 0.0021 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 11: early stopping\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step\n",
      "\n",
      "Training fold 8\n",
      "\n",
      "Generating upsampling using SMOTE...\n",
      "200000 samples after upsampling.\n",
      "Class distribution for training after upsampling: Counter({np.int64(0): 6250, np.int64(1): 6250, np.int64(2): 6250, np.int64(3): 6250, np.int64(4): 6250, np.int64(5): 6250, np.int64(6): 6250, np.int64(7): 6250, np.int64(8): 6250, np.int64(9): 6250, np.int64(10): 6250, np.int64(11): 6250, np.int64(12): 6250, np.int64(13): 6250, np.int64(14): 6250, np.int64(15): 6250, np.int64(16): 6250, np.int64(17): 6250, np.int64(18): 6250, np.int64(19): 6250, np.int64(20): 6250, np.int64(21): 6250, np.int64(22): 6250, np.int64(23): 6250, np.int64(24): 6250, np.int64(25): 6250, np.int64(26): 6250, np.int64(27): 6250, np.int64(28): 6250, np.int64(29): 6250, np.int64(30): 6250, np.int64(31): 6250})\n",
      "Finishing upsampling.\n",
      "\n",
      "Epoch 1/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m184s\u001b[0m 30ms/step - accuracy: 0.9978 - loss: 0.0072 - val_accuracy: 1.0000 - val_loss: 2.3484e-08\n",
      "Epoch 2/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m180s\u001b[0m 29ms/step - accuracy: 0.9987 - loss: 0.0047 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 3/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m196s\u001b[0m 28ms/step - accuracy: 0.9989 - loss: 0.0040 - val_accuracy: 1.0000 - val_loss: 1.9670e-09\n",
      "Epoch 4/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m181s\u001b[0m 29ms/step - accuracy: 0.9991 - loss: 0.0038 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 5/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m181s\u001b[0m 29ms/step - accuracy: 0.9992 - loss: 0.0032 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 6/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m182s\u001b[0m 29ms/step - accuracy: 0.9991 - loss: 0.0030 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 7/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m183s\u001b[0m 30ms/step - accuracy: 0.9992 - loss: 0.0026 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 8/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m182s\u001b[0m 29ms/step - accuracy: 0.9993 - loss: 0.0025 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 9/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m183s\u001b[0m 30ms/step - accuracy: 0.9994 - loss: 0.0023 - val_accuracy: 1.0000 - val_loss: 2.3842e-09\n",
      "Epoch 10/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m186s\u001b[0m 30ms/step - accuracy: 0.9993 - loss: 0.0026 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 11/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m186s\u001b[0m 30ms/step - accuracy: 0.9994 - loss: 0.0026 - val_accuracy: 1.0000 - val_loss: 4.1723e-10\n",
      "Epoch 12/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m187s\u001b[0m 30ms/step - accuracy: 0.9994 - loss: 0.0021 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 12: early stopping\n",
      "Restoring model weights from the end of the best epoch: 2.\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step\n",
      "\n",
      "Training fold 9\n",
      "\n",
      "Generating upsampling using SMOTE...\n",
      "200000 samples after upsampling.\n",
      "Class distribution for training after upsampling: Counter({np.int64(0): 6250, np.int64(1): 6250, np.int64(2): 6250, np.int64(3): 6250, np.int64(4): 6250, np.int64(5): 6250, np.int64(6): 6250, np.int64(7): 6250, np.int64(8): 6250, np.int64(9): 6250, np.int64(10): 6250, np.int64(11): 6250, np.int64(12): 6250, np.int64(13): 6250, np.int64(14): 6250, np.int64(15): 6250, np.int64(16): 6250, np.int64(17): 6250, np.int64(18): 6250, np.int64(19): 6250, np.int64(20): 6250, np.int64(21): 6250, np.int64(22): 6250, np.int64(23): 6250, np.int64(24): 6250, np.int64(25): 6250, np.int64(26): 6250, np.int64(27): 6250, np.int64(28): 6250, np.int64(29): 6250, np.int64(30): 6250, np.int64(31): 6250})\n",
      "Finishing upsampling.\n",
      "\n",
      "Epoch 1/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m197s\u001b[0m 32ms/step - accuracy: 0.9975 - loss: 0.0085 - val_accuracy: 1.0000 - val_loss: 1.6153e-08\n",
      "Epoch 2/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m193s\u001b[0m 31ms/step - accuracy: 0.9989 - loss: 0.0038 - val_accuracy: 1.0000 - val_loss: 2.2590e-08\n",
      "Epoch 3/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m194s\u001b[0m 31ms/step - accuracy: 0.9990 - loss: 0.0036 - val_accuracy: 1.0000 - val_loss: 3.1173e-08\n",
      "Epoch 4/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m194s\u001b[0m 31ms/step - accuracy: 0.9991 - loss: 0.0033 - val_accuracy: 1.0000 - val_loss: 6.0484e-07\n",
      "Epoch 5/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m195s\u001b[0m 31ms/step - accuracy: 0.9993 - loss: 0.0022 - val_accuracy: 1.0000 - val_loss: 1.1325e-09\n",
      "Epoch 6/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m207s\u001b[0m 34ms/step - accuracy: 0.9994 - loss: 0.0028 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 7/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m200s\u001b[0m 32ms/step - accuracy: 0.9993 - loss: 0.0020 - val_accuracy: 1.0000 - val_loss: 8.3624e-08\n",
      "Epoch 8/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m197s\u001b[0m 32ms/step - accuracy: 0.9993 - loss: 0.0026 - val_accuracy: 1.0000 - val_loss: 6.1511e-08\n",
      "Epoch 9/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m198s\u001b[0m 32ms/step - accuracy: 0.9995 - loss: 0.0019 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 10/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m201s\u001b[0m 33ms/step - accuracy: 0.9993 - loss: 0.0025 - val_accuracy: 1.0000 - val_loss: 1.7285e-09\n",
      "Epoch 11/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m201s\u001b[0m 32ms/step - accuracy: 0.9993 - loss: 0.0023 - val_accuracy: 1.0000 - val_loss: 8.8215e-09\n",
      "Epoch 12/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m201s\u001b[0m 33ms/step - accuracy: 0.9993 - loss: 0.0028 - val_accuracy: 1.0000 - val_loss: 7.1526e-10\n",
      "Epoch 13/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m202s\u001b[0m 33ms/step - accuracy: 0.9995 - loss: 0.0019 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 14/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m204s\u001b[0m 33ms/step - accuracy: 0.9994 - loss: 0.0019 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 15/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m206s\u001b[0m 33ms/step - accuracy: 0.9994 - loss: 0.0021 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 16/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m207s\u001b[0m 33ms/step - accuracy: 0.9995 - loss: 0.0019 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 16: early stopping\n",
      "Restoring model weights from the end of the best epoch: 6.\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step\n",
      "\n",
      "Training fold 10\n",
      "\n",
      "Generating upsampling using SMOTE...\n",
      "200000 samples after upsampling.\n",
      "Class distribution for training after upsampling: Counter({np.int64(0): 6250, np.int64(1): 6250, np.int64(2): 6250, np.int64(3): 6250, np.int64(4): 6250, np.int64(5): 6250, np.int64(6): 6250, np.int64(7): 6250, np.int64(8): 6250, np.int64(9): 6250, np.int64(10): 6250, np.int64(11): 6250, np.int64(12): 6250, np.int64(13): 6250, np.int64(14): 6250, np.int64(15): 6250, np.int64(16): 6250, np.int64(17): 6250, np.int64(18): 6250, np.int64(19): 6250, np.int64(20): 6250, np.int64(21): 6250, np.int64(22): 6250, np.int64(23): 6250, np.int64(24): 6250, np.int64(25): 6250, np.int64(26): 6250, np.int64(27): 6250, np.int64(28): 6250, np.int64(29): 6250, np.int64(30): 6250, np.int64(31): 6250})\n",
      "Finishing upsampling.\n",
      "\n",
      "Epoch 1/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m216s\u001b[0m 35ms/step - accuracy: 0.9976 - loss: 0.0086 - val_accuracy: 1.0000 - val_loss: 3.3379e-09\n",
      "Epoch 2/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m210s\u001b[0m 34ms/step - accuracy: 0.9989 - loss: 0.0049 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 3/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m210s\u001b[0m 34ms/step - accuracy: 0.9993 - loss: 0.0028 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 4/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m212s\u001b[0m 34ms/step - accuracy: 0.9992 - loss: 0.0038 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 5/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m212s\u001b[0m 34ms/step - accuracy: 0.9993 - loss: 0.0031 - val_accuracy: 1.0000 - val_loss: 1.1325e-09\n",
      "Epoch 6/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m211s\u001b[0m 34ms/step - accuracy: 0.9992 - loss: 0.0026 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 7/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m212s\u001b[0m 34ms/step - accuracy: 0.9996 - loss: 0.0017 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 8/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m212s\u001b[0m 34ms/step - accuracy: 0.9993 - loss: 0.0025 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 9/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m213s\u001b[0m 34ms/step - accuracy: 0.9993 - loss: 0.0026 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 10/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m215s\u001b[0m 35ms/step - accuracy: 0.9994 - loss: 0.0024 - val_accuracy: 1.0000 - val_loss: 4.6492e-09\n",
      "Epoch 11/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m215s\u001b[0m 35ms/step - accuracy: 0.9993 - loss: 0.0032 - val_accuracy: 1.0000 - val_loss: 5.2608e-06\n",
      "Epoch 12/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m216s\u001b[0m 35ms/step - accuracy: 0.9993 - loss: 0.0024 - val_accuracy: 1.0000 - val_loss: 1.0133e-09\n",
      "Epoch 12: early stopping\n",
      "Restoring model weights from the end of the best epoch: 2.\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step\n",
      "\n",
      "Time taken for training:  07:16:32\n",
      "\n",
      "\n",
      "Fold 1 - Train Accuracy 0.9995 - Test Accuracy 0.6012\n",
      "Fold 2 - Train Accuracy 0.9995 - Test Accuracy 0.8777\n",
      "Fold 3 - Train Accuracy 0.9996 - Test Accuracy 0.9483\n",
      "Fold 4 - Train Accuracy 0.9997 - Test Accuracy 0.9417\n",
      "Fold 5 - Train Accuracy 0.9994 - Test Accuracy 0.9814\n",
      "Fold 6 - Train Accuracy 0.9995 - Test Accuracy 0.9868\n",
      "Fold 7 - Train Accuracy 0.9997 - Test Accuracy 0.9883\n",
      "Fold 8 - Train Accuracy 0.9997 - Test Accuracy 0.9864\n",
      "Fold 9 - Train Accuracy 0.9997 - Test Accuracy 0.9569\n",
      "Fold 10 - Train Accuracy 0.9998 - Test Accuracy 0.9918\n",
      "\n",
      "Mean Train Accuracy: 0.9996 - Std: 0.0001 \n",
      "Mean Test Accuracy: 0.9261 - Std: 0.1132 \n",
      "\n",
      "Evaluate other metrics:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.95      0.95     13905\n",
      "           1       0.93      0.97      0.95       723\n",
      "           2       0.96      0.97      0.97      2659\n",
      "           3       0.85      0.84      0.84      1123\n",
      "           4       0.88      0.87      0.88       384\n",
      "           5       0.95      0.84      0.89        44\n",
      "           6       0.92      0.90      0.91       663\n",
      "           7       0.94      0.90      0.92        94\n",
      "           8       0.91      0.83      0.87        12\n",
      "           9       0.92      0.88      0.90       786\n",
      "          10       0.62      0.89      0.73         9\n",
      "          11       0.93      0.90      0.91        98\n",
      "          12       0.86      0.75      0.80         8\n",
      "          13       1.00      0.90      0.95        20\n",
      "          14       0.84      0.90      0.87        77\n",
      "          15       0.95      0.92      0.93        62\n",
      "          16       0.81      0.85      0.83       917\n",
      "          17       0.93      0.94      0.94       473\n",
      "          18       0.91      0.91      0.91        22\n",
      "          19       0.89      0.89      0.89       122\n",
      "          20       0.87      0.88      0.88       111\n",
      "          21       0.86      0.90      0.88       201\n",
      "          22       0.86      0.86      0.86         7\n",
      "          23       0.92      0.82      0.87        96\n",
      "          24       0.80      0.86      0.83      1045\n",
      "          25       0.91      0.87      0.89       540\n",
      "          26       0.89      0.86      0.87      1334\n",
      "          27       0.78      0.89      0.83        28\n",
      "          28       0.89      0.94      0.92        35\n",
      "          29       0.92      0.87      0.89        77\n",
      "          30       0.92      0.84      0.88        64\n",
      "          31       0.55      0.86      0.67         7\n",
      "\n",
      "    accuracy                           0.93     25746\n",
      "   macro avg       0.88      0.88      0.88     25746\n",
      "weighted avg       0.93      0.93      0.93     25746\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Build a \"time window\" structure to handle the dataset as a time series.\n",
    "new_df = load_dataset(\"aVR\")\n",
    "X_array, y_array = build_time_window_structure(new_df, \"aVR\")\n",
    "X_array, y_array = remove_classes_with_less_samples(X_array, y_array)\n",
    "\n",
    "# Train the CNN model.\n",
    "v1_model = create_v1()\n",
    "v1_num_epochs = 300\n",
    "v1_batch_size = 32\n",
    "v1_validation_split = 0.01\n",
    "\n",
    "training_history_v1 = train_cnn_model(v1_model, X_array, y_array, v1_num_epochs, v1_batch_size, v1_validation_split, \"v1_model_aVR.keras\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d176951-e933-4b9c-99a2-0b8feb53f203",
   "metadata": {},
   "source": [
    "#### aVL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bf2df936-284f-40ba-bc77-fac990e58f4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Start loading CSV file...\n",
      "Finish loading CSV file.\n",
      "\n",
      "Starting build_time_window_structure function...\n",
      "\n",
      "Shape of features:  (25770, 1250)\n",
      "Quantity os samples (labels):  25770\n",
      "\n",
      "Finishing build_time_window_structure function.\n",
      "\n",
      "Remove classes with less than 6 samples.\n",
      "\n",
      "Show classes identification:\n",
      "Class: 0 - Arrhythmia code: 1\n",
      "Class: 1 - Arrhythmia code: 21\n",
      "Class: 2 - Arrhythmia code: 22\n",
      "Class: 3 - Arrhythmia code: 23\n",
      "Class: 4 - Arrhythmia code: 30\n",
      "Class: 5 - Arrhythmia code: 36\n",
      "Class: 6 - Arrhythmia code: 50\n",
      "Class: 7 - Arrhythmia code: 51\n",
      "Class: 8 - Arrhythmia code: 54\n",
      "Class: 9 - Arrhythmia code: 60\n",
      "Class: 10 - Arrhythmia code: 80\n",
      "Class: 11 - Arrhythmia code: 82\n",
      "Class: 12 - Arrhythmia code: 83\n",
      "Class: 13 - Arrhythmia code: 88\n",
      "Class: 14 - Arrhythmia code: 101\n",
      "Class: 15 - Arrhythmia code: 104\n",
      "Class: 16 - Arrhythmia code: 105\n",
      "Class: 17 - Arrhythmia code: 106\n",
      "Class: 18 - Arrhythmia code: 108\n",
      "Class: 19 - Arrhythmia code: 120\n",
      "Class: 20 - Arrhythmia code: 121\n",
      "Class: 21 - Arrhythmia code: 125\n",
      "Class: 22 - Arrhythmia code: 140\n",
      "Class: 23 - Arrhythmia code: 142\n",
      "Class: 24 - Arrhythmia code: 145\n",
      "Class: 25 - Arrhythmia code: 146\n",
      "Class: 26 - Arrhythmia code: 147\n",
      "Class: 27 - Arrhythmia code: 155\n",
      "Class: 28 - Arrhythmia code: 160\n",
      "Class: 29 - Arrhythmia code: 161\n",
      "Class: 30 - Arrhythmia code: 165\n",
      "Class: 31 - Arrhythmia code: 166\n",
      "\n",
      "Check for dataset balance:\n",
      "Class =   0   Qty =    13905   Percentage = 54.01 %\n",
      "Class =   2   Qty =     2659   Percentage = 10.33 %\n",
      "Class =  26   Qty =     1334   Percentage = 5.18 %\n",
      "Class =   3   Qty =     1123   Percentage = 4.36 %\n",
      "Class =  24   Qty =     1045   Percentage = 4.06 %\n",
      "Class =  16   Qty =      917   Percentage = 3.56 %\n",
      "Class =   9   Qty =      786   Percentage = 3.05 %\n",
      "Class =   1   Qty =      723   Percentage = 2.81 %\n",
      "Class =   6   Qty =      663   Percentage = 2.58 %\n",
      "Class =  25   Qty =      540   Percentage = 2.10 %\n",
      "Class =  17   Qty =      473   Percentage = 1.84 %\n",
      "Class =   4   Qty =      384   Percentage = 1.49 %\n",
      "Class =  21   Qty =      201   Percentage = 0.78 %\n",
      "Class =  19   Qty =      122   Percentage = 0.47 %\n",
      "Class =  20   Qty =      111   Percentage = 0.43 %\n",
      "Class =  11   Qty =       98   Percentage = 0.38 %\n",
      "Class =  23   Qty =       96   Percentage = 0.37 %\n",
      "Class =   7   Qty =       94   Percentage = 0.37 %\n",
      "Class =  14   Qty =       77   Percentage = 0.30 %\n",
      "Class =  29   Qty =       77   Percentage = 0.30 %\n",
      "Class =  30   Qty =       64   Percentage = 0.25 %\n",
      "Class =  15   Qty =       62   Percentage = 0.24 %\n",
      "Class =   5   Qty =       44   Percentage = 0.17 %\n",
      "Class =  28   Qty =       35   Percentage = 0.14 %\n",
      "Class =  27   Qty =       28   Percentage = 0.11 %\n",
      "Class =  18   Qty =       22   Percentage = 0.09 %\n",
      "Class =  13   Qty =       20   Percentage = 0.08 %\n",
      "Class =   8   Qty =       12   Percentage = 0.05 %\n",
      "Class =  10   Qty =        9   Percentage = 0.03 %\n",
      "Class =  12   Qty =        8   Percentage = 0.03 %\n",
      "Class =  22   Qty =        7   Percentage = 0.03 %\n",
      "Class =  31   Qty =        7   Percentage = 0.03 %\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1248</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)        │            <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1248</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)        │            <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1246</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)        │           <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1246</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)        │            <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">623</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">619</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)        │           <span style=\"color: #00af00; text-decoration-color: #00af00\">656</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_2           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">619</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)        │            <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">615</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)        │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,296</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_3           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">615</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)        │            <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">307</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4912</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">628,864</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_4           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,128</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv1d (\u001b[38;5;33mConv1D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1248\u001b[0m, \u001b[38;5;34m8\u001b[0m)        │            \u001b[38;5;34m32\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1248\u001b[0m, \u001b[38;5;34m8\u001b[0m)        │            \u001b[38;5;34m32\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_1 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1246\u001b[0m, \u001b[38;5;34m8\u001b[0m)        │           \u001b[38;5;34m200\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_1           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1246\u001b[0m, \u001b[38;5;34m8\u001b[0m)        │            \u001b[38;5;34m32\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d (\u001b[38;5;33mMaxPooling1D\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m623\u001b[0m, \u001b[38;5;34m8\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_2 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m619\u001b[0m, \u001b[38;5;34m16\u001b[0m)        │           \u001b[38;5;34m656\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_2           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m619\u001b[0m, \u001b[38;5;34m16\u001b[0m)        │            \u001b[38;5;34m64\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_3 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m615\u001b[0m, \u001b[38;5;34m16\u001b[0m)        │         \u001b[38;5;34m1,296\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_3           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m615\u001b[0m, \u001b[38;5;34m16\u001b[0m)        │            \u001b[38;5;34m64\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_1 (\u001b[38;5;33mMaxPooling1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m307\u001b[0m, \u001b[38;5;34m16\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4912\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │       \u001b[38;5;34m628,864\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_4           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │           \u001b[38;5;34m512\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │         \u001b[38;5;34m4,128\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">635,880</span> (2.43 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m635,880\u001b[0m (2.43 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">635,528</span> (2.42 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m635,528\u001b[0m (2.42 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">352</span> (1.38 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m352\u001b[0m (1.38 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting training...\n",
      "\n",
      "Training fold 1\n",
      "\n",
      "Generating upsampling using SMOTE...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\DeveloperTools\\python\\3.11.0\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:737: UserWarning: The least populated class in y has only 7 members, which is less than n_splits=10.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200000 samples after upsampling.\n",
      "Class distribution for training after upsampling: Counter({np.int64(0): 6250, np.int64(1): 6250, np.int64(2): 6250, np.int64(3): 6250, np.int64(4): 6250, np.int64(5): 6250, np.int64(6): 6250, np.int64(7): 6250, np.int64(8): 6250, np.int64(9): 6250, np.int64(10): 6250, np.int64(11): 6250, np.int64(12): 6250, np.int64(13): 6250, np.int64(14): 6250, np.int64(15): 6250, np.int64(16): 6250, np.int64(17): 6250, np.int64(18): 6250, np.int64(19): 6250, np.int64(20): 6250, np.int64(21): 6250, np.int64(22): 6250, np.int64(23): 6250, np.int64(24): 6250, np.int64(25): 6250, np.int64(26): 6250, np.int64(27): 6250, np.int64(28): 6250, np.int64(29): 6250, np.int64(30): 6250, np.int64(31): 6250})\n",
      "Finishing upsampling.\n",
      "\n",
      "Epoch 1/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 17ms/step - accuracy: 0.8459 - loss: 0.6004 - val_accuracy: 1.0000 - val_loss: 4.2854e-05\n",
      "Epoch 2/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 17ms/step - accuracy: 0.9720 - loss: 0.0878 - val_accuracy: 1.0000 - val_loss: 5.5915e-05\n",
      "Epoch 3/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 17ms/step - accuracy: 0.9857 - loss: 0.0449 - val_accuracy: 1.0000 - val_loss: 3.6037e-05\n",
      "Epoch 4/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m111s\u001b[0m 18ms/step - accuracy: 0.9906 - loss: 0.0295 - val_accuracy: 1.0000 - val_loss: 2.6453e-07\n",
      "Epoch 5/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m115s\u001b[0m 19ms/step - accuracy: 0.9925 - loss: 0.0227 - val_accuracy: 1.0000 - val_loss: 8.5948e-06\n",
      "Epoch 6/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m117s\u001b[0m 19ms/step - accuracy: 0.9944 - loss: 0.0176 - val_accuracy: 1.0000 - val_loss: 2.9743e-07\n",
      "Epoch 7/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m126s\u001b[0m 20ms/step - accuracy: 0.9947 - loss: 0.0160 - val_accuracy: 1.0000 - val_loss: 8.7440e-08\n",
      "Epoch 8/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 23ms/step - accuracy: 0.9960 - loss: 0.0132 - val_accuracy: 1.0000 - val_loss: 4.6098e-07\n",
      "Epoch 9/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m126s\u001b[0m 20ms/step - accuracy: 0.9964 - loss: 0.0114 - val_accuracy: 1.0000 - val_loss: 7.7486e-08\n",
      "Epoch 10/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m119s\u001b[0m 19ms/step - accuracy: 0.9968 - loss: 0.0098 - val_accuracy: 1.0000 - val_loss: 3.7336e-07\n",
      "Epoch 11/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m121s\u001b[0m 20ms/step - accuracy: 0.9969 - loss: 0.0102 - val_accuracy: 1.0000 - val_loss: 2.0862e-09\n",
      "Epoch 12/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m110s\u001b[0m 18ms/step - accuracy: 0.9971 - loss: 0.0090 - val_accuracy: 1.0000 - val_loss: 2.2143e-07\n",
      "Epoch 13/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m100s\u001b[0m 16ms/step - accuracy: 0.9974 - loss: 0.0080 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 14/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 17ms/step - accuracy: 0.9975 - loss: 0.0077 - val_accuracy: 1.0000 - val_loss: 1.3709e-09\n",
      "Epoch 15/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m110s\u001b[0m 18ms/step - accuracy: 0.9977 - loss: 0.0070 - val_accuracy: 1.0000 - val_loss: 9.6560e-09\n",
      "Epoch 16/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m112s\u001b[0m 18ms/step - accuracy: 0.9978 - loss: 0.0074 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 17/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m115s\u001b[0m 19ms/step - accuracy: 0.9977 - loss: 0.0069 - val_accuracy: 1.0000 - val_loss: 2.2054e-09\n",
      "Epoch 18/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m115s\u001b[0m 19ms/step - accuracy: 0.9980 - loss: 0.0065 - val_accuracy: 1.0000 - val_loss: 5.9616e-06\n",
      "Epoch 19/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m130s\u001b[0m 17ms/step - accuracy: 0.9978 - loss: 0.0069 - val_accuracy: 1.0000 - val_loss: 1.8060e-08\n",
      "Epoch 20/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m111s\u001b[0m 18ms/step - accuracy: 0.9982 - loss: 0.0062 - val_accuracy: 1.0000 - val_loss: 1.0381e-06\n",
      "Epoch 21/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m114s\u001b[0m 18ms/step - accuracy: 0.9980 - loss: 0.0064 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 22/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m117s\u001b[0m 19ms/step - accuracy: 0.9981 - loss: 0.0059 - val_accuracy: 1.0000 - val_loss: 1.9670e-09\n",
      "Epoch 23/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m121s\u001b[0m 20ms/step - accuracy: 0.9982 - loss: 0.0065 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 23: early stopping\n",
      "Restoring model weights from the end of the best epoch: 13.\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step\n",
      "\n",
      "Training fold 2\n",
      "\n",
      "Generating upsampling using SMOTE...\n",
      "200000 samples after upsampling.\n",
      "Class distribution for training after upsampling: Counter({np.int64(0): 6250, np.int64(1): 6250, np.int64(2): 6250, np.int64(3): 6250, np.int64(4): 6250, np.int64(5): 6250, np.int64(6): 6250, np.int64(7): 6250, np.int64(8): 6250, np.int64(9): 6250, np.int64(10): 6250, np.int64(11): 6250, np.int64(12): 6250, np.int64(13): 6250, np.int64(14): 6250, np.int64(15): 6250, np.int64(16): 6250, np.int64(17): 6250, np.int64(18): 6250, np.int64(19): 6250, np.int64(20): 6250, np.int64(21): 6250, np.int64(22): 6250, np.int64(23): 6250, np.int64(24): 6250, np.int64(25): 6250, np.int64(26): 6250, np.int64(27): 6250, np.int64(28): 6250, np.int64(29): 6250, np.int64(30): 6250, np.int64(31): 6250})\n",
      "Finishing upsampling.\n",
      "\n",
      "Epoch 1/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m110s\u001b[0m 18ms/step - accuracy: 0.9683 - loss: 0.1725 - val_accuracy: 1.0000 - val_loss: 7.6890e-09\n",
      "Epoch 2/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m120s\u001b[0m 19ms/step - accuracy: 0.9948 - loss: 0.0164 - val_accuracy: 1.0000 - val_loss: 1.0014e-08\n",
      "Epoch 3/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m125s\u001b[0m 20ms/step - accuracy: 0.9966 - loss: 0.0103 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 4/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m122s\u001b[0m 20ms/step - accuracy: 0.9972 - loss: 0.0089 - val_accuracy: 1.0000 - val_loss: 2.9051e-07\n",
      "Epoch 5/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m116s\u001b[0m 19ms/step - accuracy: 0.9978 - loss: 0.0071 - val_accuracy: 1.0000 - val_loss: 2.5272e-08\n",
      "Epoch 6/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m114s\u001b[0m 18ms/step - accuracy: 0.9976 - loss: 0.0078 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 7/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m119s\u001b[0m 19ms/step - accuracy: 0.9979 - loss: 0.0069 - val_accuracy: 1.0000 - val_loss: 1.2428e-07\n",
      "Epoch 8/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m123s\u001b[0m 20ms/step - accuracy: 0.9979 - loss: 0.0070 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 9/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m115s\u001b[0m 19ms/step - accuracy: 0.9979 - loss: 0.0069 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 10/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m115s\u001b[0m 19ms/step - accuracy: 0.9981 - loss: 0.0062 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 11/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m120s\u001b[0m 19ms/step - accuracy: 0.9981 - loss: 0.0060 - val_accuracy: 1.0000 - val_loss: 3.2454e-07\n",
      "Epoch 12/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m113s\u001b[0m 18ms/step - accuracy: 0.9982 - loss: 0.0062 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 13/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m114s\u001b[0m 18ms/step - accuracy: 0.9985 - loss: 0.0053 - val_accuracy: 1.0000 - val_loss: 2.2650e-09\n",
      "Epoch 13: early stopping\n",
      "Restoring model weights from the end of the best epoch: 3.\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\n",
      "Training fold 3\n",
      "\n",
      "Generating upsampling using SMOTE...\n",
      "200000 samples after upsampling.\n",
      "Class distribution for training after upsampling: Counter({np.int64(0): 6250, np.int64(1): 6250, np.int64(2): 6250, np.int64(3): 6250, np.int64(4): 6250, np.int64(5): 6250, np.int64(6): 6250, np.int64(7): 6250, np.int64(8): 6250, np.int64(9): 6250, np.int64(10): 6250, np.int64(11): 6250, np.int64(12): 6250, np.int64(13): 6250, np.int64(14): 6250, np.int64(15): 6250, np.int64(16): 6250, np.int64(17): 6250, np.int64(18): 6250, np.int64(19): 6250, np.int64(20): 6250, np.int64(21): 6250, np.int64(22): 6250, np.int64(23): 6250, np.int64(24): 6250, np.int64(25): 6250, np.int64(26): 6250, np.int64(27): 6250, np.int64(28): 6250, np.int64(29): 6250, np.int64(30): 6250, np.int64(31): 6250})\n",
      "Finishing upsampling.\n",
      "\n",
      "Epoch 1/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m120s\u001b[0m 19ms/step - accuracy: 0.9889 - loss: 0.0384 - val_accuracy: 1.0000 - val_loss: 1.4105e-06\n",
      "Epoch 2/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m122s\u001b[0m 20ms/step - accuracy: 0.9966 - loss: 0.0109 - val_accuracy: 1.0000 - val_loss: 2.9206e-09\n",
      "Epoch 3/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m114s\u001b[0m 18ms/step - accuracy: 0.9977 - loss: 0.0086 - val_accuracy: 1.0000 - val_loss: 5.9605e-11\n",
      "Epoch 4/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m115s\u001b[0m 19ms/step - accuracy: 0.9975 - loss: 0.0084 - val_accuracy: 1.0000 - val_loss: 9.0956e-08\n",
      "Epoch 5/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m117s\u001b[0m 19ms/step - accuracy: 0.9977 - loss: 0.0077 - val_accuracy: 1.0000 - val_loss: 2.5511e-08\n",
      "Epoch 6/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m124s\u001b[0m 20ms/step - accuracy: 0.9982 - loss: 0.0069 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 7/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m119s\u001b[0m 19ms/step - accuracy: 0.9982 - loss: 0.0059 - val_accuracy: 1.0000 - val_loss: 1.4961e-08\n",
      "Epoch 8/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m116s\u001b[0m 19ms/step - accuracy: 0.9983 - loss: 0.0059 - val_accuracy: 1.0000 - val_loss: 1.3918e-07\n",
      "Epoch 9/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m120s\u001b[0m 19ms/step - accuracy: 0.9984 - loss: 0.0058 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 10/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m120s\u001b[0m 19ms/step - accuracy: 0.9984 - loss: 0.0069 - val_accuracy: 1.0000 - val_loss: 2.6822e-09\n",
      "Epoch 11/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m114s\u001b[0m 18ms/step - accuracy: 0.9985 - loss: 0.0054 - val_accuracy: 1.0000 - val_loss: 2.3842e-10\n",
      "Epoch 12/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m118s\u001b[0m 19ms/step - accuracy: 0.9985 - loss: 0.0048 - val_accuracy: 1.0000 - val_loss: 8.0526e-08\n",
      "Epoch 13/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m120s\u001b[0m 19ms/step - accuracy: 0.9985 - loss: 0.0050 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 14/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m125s\u001b[0m 20ms/step - accuracy: 0.9985 - loss: 0.0051 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 15/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m115s\u001b[0m 19ms/step - accuracy: 0.9987 - loss: 0.0053 - val_accuracy: 1.0000 - val_loss: 5.3644e-10\n",
      "Epoch 16/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m130s\u001b[0m 21ms/step - accuracy: 0.9988 - loss: 0.0048 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 16: early stopping\n",
      "Restoring model weights from the end of the best epoch: 6.\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "\n",
      "Training fold 4\n",
      "\n",
      "Generating upsampling using SMOTE...\n",
      "200000 samples after upsampling.\n",
      "Class distribution for training after upsampling: Counter({np.int64(0): 6250, np.int64(1): 6250, np.int64(2): 6250, np.int64(3): 6250, np.int64(4): 6250, np.int64(5): 6250, np.int64(6): 6250, np.int64(7): 6250, np.int64(8): 6250, np.int64(9): 6250, np.int64(10): 6250, np.int64(11): 6250, np.int64(12): 6250, np.int64(13): 6250, np.int64(14): 6250, np.int64(15): 6250, np.int64(16): 6250, np.int64(17): 6250, np.int64(18): 6250, np.int64(19): 6250, np.int64(20): 6250, np.int64(21): 6250, np.int64(22): 6250, np.int64(23): 6250, np.int64(24): 6250, np.int64(25): 6250, np.int64(26): 6250, np.int64(27): 6250, np.int64(28): 6250, np.int64(29): 6250, np.int64(30): 6250, np.int64(31): 6250})\n",
      "Finishing upsampling.\n",
      "\n",
      "Epoch 1/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m119s\u001b[0m 19ms/step - accuracy: 0.9909 - loss: 0.0308 - val_accuracy: 1.0000 - val_loss: 1.1206e-08\n",
      "Epoch 2/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m118s\u001b[0m 19ms/step - accuracy: 0.9976 - loss: 0.0078 - val_accuracy: 1.0000 - val_loss: 1.1795e-06\n",
      "Epoch 3/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m120s\u001b[0m 19ms/step - accuracy: 0.9980 - loss: 0.0068 - val_accuracy: 1.0000 - val_loss: 3.5624e-07\n",
      "Epoch 4/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m124s\u001b[0m 20ms/step - accuracy: 0.9982 - loss: 0.0066 - val_accuracy: 1.0000 - val_loss: 1.7822e-08\n",
      "Epoch 5/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m127s\u001b[0m 21ms/step - accuracy: 0.9982 - loss: 0.0059 - val_accuracy: 1.0000 - val_loss: 1.7047e-08\n",
      "Epoch 6/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m119s\u001b[0m 19ms/step - accuracy: 0.9984 - loss: 0.0058 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 7/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m122s\u001b[0m 20ms/step - accuracy: 0.9987 - loss: 0.0048 - val_accuracy: 1.0000 - val_loss: 1.9670e-09\n",
      "Epoch 8/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m124s\u001b[0m 20ms/step - accuracy: 0.9986 - loss: 0.0056 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 9/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m125s\u001b[0m 20ms/step - accuracy: 0.9987 - loss: 0.0044 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 10/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m119s\u001b[0m 19ms/step - accuracy: 0.9986 - loss: 0.0061 - val_accuracy: 1.0000 - val_loss: 4.1723e-10\n",
      "Epoch 11/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m123s\u001b[0m 20ms/step - accuracy: 0.9986 - loss: 0.0047 - val_accuracy: 1.0000 - val_loss: 1.0967e-08\n",
      "Epoch 12/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m127s\u001b[0m 20ms/step - accuracy: 0.9989 - loss: 0.0044 - val_accuracy: 1.0000 - val_loss: 5.3644e-10\n",
      "Epoch 13/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m130s\u001b[0m 21ms/step - accuracy: 0.9988 - loss: 0.0044 - val_accuracy: 1.0000 - val_loss: 6.5565e-10\n",
      "Epoch 14/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m121s\u001b[0m 20ms/step - accuracy: 0.9989 - loss: 0.0036 - val_accuracy: 1.0000 - val_loss: 9.9540e-09\n",
      "Epoch 15/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m125s\u001b[0m 20ms/step - accuracy: 0.9987 - loss: 0.0045 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 16/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m119s\u001b[0m 19ms/step - accuracy: 0.9989 - loss: 0.0045 - val_accuracy: 1.0000 - val_loss: 1.7881e-10\n",
      "Epoch 16: early stopping\n",
      "Restoring model weights from the end of the best epoch: 6.\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\n",
      "Training fold 5\n",
      "\n",
      "Generating upsampling using SMOTE...\n",
      "200000 samples after upsampling.\n",
      "Class distribution for training after upsampling: Counter({np.int64(0): 6250, np.int64(1): 6250, np.int64(2): 6250, np.int64(3): 6250, np.int64(4): 6250, np.int64(5): 6250, np.int64(6): 6250, np.int64(7): 6250, np.int64(8): 6250, np.int64(9): 6250, np.int64(10): 6250, np.int64(11): 6250, np.int64(12): 6250, np.int64(13): 6250, np.int64(14): 6250, np.int64(15): 6250, np.int64(16): 6250, np.int64(17): 6250, np.int64(18): 6250, np.int64(19): 6250, np.int64(20): 6250, np.int64(21): 6250, np.int64(22): 6250, np.int64(23): 6250, np.int64(24): 6250, np.int64(25): 6250, np.int64(26): 6250, np.int64(27): 6250, np.int64(28): 6250, np.int64(29): 6250, np.int64(30): 6250, np.int64(31): 6250})\n",
      "Finishing upsampling.\n",
      "\n",
      "Epoch 1/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m122s\u001b[0m 20ms/step - accuracy: 0.9927 - loss: 0.0265 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 2/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m123s\u001b[0m 20ms/step - accuracy: 0.9979 - loss: 0.0070 - val_accuracy: 1.0000 - val_loss: 2.5630e-09\n",
      "Epoch 3/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m126s\u001b[0m 20ms/step - accuracy: 0.9985 - loss: 0.0058 - val_accuracy: 1.0000 - val_loss: 2.0879e-07\n",
      "Epoch 4/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m130s\u001b[0m 21ms/step - accuracy: 0.9984 - loss: 0.0054 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 5/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m123s\u001b[0m 20ms/step - accuracy: 0.9983 - loss: 0.0054 - val_accuracy: 1.0000 - val_loss: 8.2969e-08\n",
      "Epoch 6/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m126s\u001b[0m 20ms/step - accuracy: 0.9985 - loss: 0.0050 - val_accuracy: 1.0000 - val_loss: 2.8139e-07\n",
      "Epoch 7/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m131s\u001b[0m 21ms/step - accuracy: 0.9986 - loss: 0.0051 - val_accuracy: 1.0000 - val_loss: 3.9339e-09\n",
      "Epoch 8/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m136s\u001b[0m 22ms/step - accuracy: 0.9986 - loss: 0.0047 - val_accuracy: 1.0000 - val_loss: 3.9935e-09\n",
      "Epoch 9/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m129s\u001b[0m 21ms/step - accuracy: 0.9989 - loss: 0.0039 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 10/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m127s\u001b[0m 21ms/step - accuracy: 0.9985 - loss: 0.0045 - val_accuracy: 1.0000 - val_loss: 7.1526e-10\n",
      "Epoch 11/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m133s\u001b[0m 21ms/step - accuracy: 0.9987 - loss: 0.0045 - val_accuracy: 1.0000 - val_loss: 1.7881e-10\n",
      "Epoch 11: early stopping\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step\n",
      "\n",
      "Training fold 6\n",
      "\n",
      "Generating upsampling using SMOTE...\n",
      "200000 samples after upsampling.\n",
      "Class distribution for training after upsampling: Counter({np.int64(0): 6250, np.int64(1): 6250, np.int64(2): 6250, np.int64(3): 6250, np.int64(4): 6250, np.int64(5): 6250, np.int64(6): 6250, np.int64(7): 6250, np.int64(8): 6250, np.int64(9): 6250, np.int64(10): 6250, np.int64(11): 6250, np.int64(12): 6250, np.int64(13): 6250, np.int64(14): 6250, np.int64(15): 6250, np.int64(16): 6250, np.int64(17): 6250, np.int64(18): 6250, np.int64(19): 6250, np.int64(20): 6250, np.int64(21): 6250, np.int64(22): 6250, np.int64(23): 6250, np.int64(24): 6250, np.int64(25): 6250, np.int64(26): 6250, np.int64(27): 6250, np.int64(28): 6250, np.int64(29): 6250, np.int64(30): 6250, np.int64(31): 6250})\n",
      "Finishing upsampling.\n",
      "\n",
      "Epoch 1/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m126s\u001b[0m 20ms/step - accuracy: 0.9960 - loss: 0.0130 - val_accuracy: 1.0000 - val_loss: 1.1206e-07\n",
      "Epoch 2/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m128s\u001b[0m 21ms/step - accuracy: 0.9979 - loss: 0.0069 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 3/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m132s\u001b[0m 21ms/step - accuracy: 0.9984 - loss: 0.0064 - val_accuracy: 1.0000 - val_loss: 1.2517e-09\n",
      "Epoch 4/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m136s\u001b[0m 22ms/step - accuracy: 0.9986 - loss: 0.0050 - val_accuracy: 1.0000 - val_loss: 4.7684e-10\n",
      "Epoch 5/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m126s\u001b[0m 20ms/step - accuracy: 0.9986 - loss: 0.0051 - val_accuracy: 1.0000 - val_loss: 6.5565e-10\n",
      "Epoch 6/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m130s\u001b[0m 21ms/step - accuracy: 0.9988 - loss: 0.0041 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 7/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m134s\u001b[0m 22ms/step - accuracy: 0.9986 - loss: 0.0045 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 8/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m132s\u001b[0m 21ms/step - accuracy: 0.9989 - loss: 0.0042 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 9/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m128s\u001b[0m 21ms/step - accuracy: 0.9989 - loss: 0.0047 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 10/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m131s\u001b[0m 21ms/step - accuracy: 0.9989 - loss: 0.0037 - val_accuracy: 1.0000 - val_loss: 9.5367e-10\n",
      "Epoch 11/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m137s\u001b[0m 22ms/step - accuracy: 0.9988 - loss: 0.0047 - val_accuracy: 1.0000 - val_loss: 2.8610e-09\n",
      "Epoch 12/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m129s\u001b[0m 21ms/step - accuracy: 0.9988 - loss: 0.0041 - val_accuracy: 1.0000 - val_loss: 8.9407e-10\n",
      "Epoch 12: early stopping\n",
      "Restoring model weights from the end of the best epoch: 2.\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step\n",
      "\n",
      "Training fold 7\n",
      "\n",
      "Generating upsampling using SMOTE...\n",
      "200000 samples after upsampling.\n",
      "Class distribution for training after upsampling: Counter({np.int64(0): 6250, np.int64(1): 6250, np.int64(2): 6250, np.int64(3): 6250, np.int64(4): 6250, np.int64(5): 6250, np.int64(6): 6250, np.int64(7): 6250, np.int64(8): 6250, np.int64(9): 6250, np.int64(10): 6250, np.int64(11): 6250, np.int64(12): 6250, np.int64(13): 6250, np.int64(14): 6250, np.int64(15): 6250, np.int64(16): 6250, np.int64(17): 6250, np.int64(18): 6250, np.int64(19): 6250, np.int64(20): 6250, np.int64(21): 6250, np.int64(22): 6250, np.int64(23): 6250, np.int64(24): 6250, np.int64(25): 6250, np.int64(26): 6250, np.int64(27): 6250, np.int64(28): 6250, np.int64(29): 6250, np.int64(30): 6250, np.int64(31): 6250})\n",
      "Finishing upsampling.\n",
      "\n",
      "Epoch 1/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m130s\u001b[0m 21ms/step - accuracy: 0.9958 - loss: 0.0133 - val_accuracy: 1.0000 - val_loss: 8.0466e-09\n",
      "Epoch 2/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m128s\u001b[0m 21ms/step - accuracy: 0.9985 - loss: 0.0052 - val_accuracy: 1.0000 - val_loss: 7.7486e-09\n",
      "Epoch 3/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m146s\u001b[0m 24ms/step - accuracy: 0.9986 - loss: 0.0049 - val_accuracy: 1.0000 - val_loss: 3.9339e-09\n",
      "Epoch 4/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m135s\u001b[0m 22ms/step - accuracy: 0.9989 - loss: 0.0045 - val_accuracy: 1.0000 - val_loss: 2.3842e-10\n",
      "Epoch 5/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m126s\u001b[0m 20ms/step - accuracy: 0.9987 - loss: 0.0048 - val_accuracy: 1.0000 - val_loss: 4.4107e-09\n",
      "Epoch 6/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m132s\u001b[0m 21ms/step - accuracy: 0.9987 - loss: 0.0048 - val_accuracy: 1.0000 - val_loss: 6.6161e-09\n",
      "Epoch 7/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m134s\u001b[0m 22ms/step - accuracy: 0.9989 - loss: 0.0042 - val_accuracy: 1.0000 - val_loss: 4.1723e-10\n",
      "Epoch 8/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m133s\u001b[0m 22ms/step - accuracy: 0.9988 - loss: 0.0044 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 9/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m133s\u001b[0m 22ms/step - accuracy: 0.9991 - loss: 0.0035 - val_accuracy: 1.0000 - val_loss: 3.3975e-09\n",
      "Epoch 10/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m136s\u001b[0m 22ms/step - accuracy: 0.9990 - loss: 0.0038 - val_accuracy: 1.0000 - val_loss: 2.0564e-08\n",
      "Epoch 11/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m126s\u001b[0m 20ms/step - accuracy: 0.9990 - loss: 0.0035 - val_accuracy: 1.0000 - val_loss: 3.5763e-10\n",
      "Epoch 12/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m128s\u001b[0m 21ms/step - accuracy: 0.9990 - loss: 0.0033 - val_accuracy: 1.0000 - val_loss: 2.9802e-10\n",
      "Epoch 13/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m131s\u001b[0m 21ms/step - accuracy: 0.9988 - loss: 0.0040 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 14/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m132s\u001b[0m 21ms/step - accuracy: 0.9991 - loss: 0.0035 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 15/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m127s\u001b[0m 21ms/step - accuracy: 0.9992 - loss: 0.0031 - val_accuracy: 1.0000 - val_loss: 4.1723e-10\n",
      "Epoch 16/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m130s\u001b[0m 21ms/step - accuracy: 0.9990 - loss: 0.0041 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 17/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m135s\u001b[0m 22ms/step - accuracy: 0.9989 - loss: 0.0036 - val_accuracy: 1.0000 - val_loss: 1.3113e-09\n",
      "Epoch 18/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m126s\u001b[0m 20ms/step - accuracy: 0.9991 - loss: 0.0032 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 18: early stopping\n",
      "Restoring model weights from the end of the best epoch: 8.\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\n",
      "Training fold 8\n",
      "\n",
      "Generating upsampling using SMOTE...\n",
      "200000 samples after upsampling.\n",
      "Class distribution for training after upsampling: Counter({np.int64(0): 6250, np.int64(1): 6250, np.int64(2): 6250, np.int64(3): 6250, np.int64(4): 6250, np.int64(5): 6250, np.int64(6): 6250, np.int64(7): 6250, np.int64(8): 6250, np.int64(9): 6250, np.int64(10): 6250, np.int64(11): 6250, np.int64(12): 6250, np.int64(13): 6250, np.int64(14): 6250, np.int64(15): 6250, np.int64(16): 6250, np.int64(17): 6250, np.int64(18): 6250, np.int64(19): 6250, np.int64(20): 6250, np.int64(21): 6250, np.int64(22): 6250, np.int64(23): 6250, np.int64(24): 6250, np.int64(25): 6250, np.int64(26): 6250, np.int64(27): 6250, np.int64(28): 6250, np.int64(29): 6250, np.int64(30): 6250, np.int64(31): 6250})\n",
      "Finishing upsampling.\n",
      "\n",
      "Epoch 1/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m133s\u001b[0m 22ms/step - accuracy: 0.9949 - loss: 0.0178 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 2/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m127s\u001b[0m 20ms/step - accuracy: 0.9983 - loss: 0.0059 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 3/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m131s\u001b[0m 21ms/step - accuracy: 0.9986 - loss: 0.0046 - val_accuracy: 1.0000 - val_loss: 4.1723e-10\n",
      "Epoch 4/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m134s\u001b[0m 22ms/step - accuracy: 0.9987 - loss: 0.0050 - val_accuracy: 1.0000 - val_loss: 9.3817e-08\n",
      "Epoch 5/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m134s\u001b[0m 22ms/step - accuracy: 0.9988 - loss: 0.0045 - val_accuracy: 1.0000 - val_loss: 5.9605e-11\n",
      "Epoch 6/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m129s\u001b[0m 21ms/step - accuracy: 0.9989 - loss: 0.0045 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 7/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m133s\u001b[0m 21ms/step - accuracy: 0.9989 - loss: 0.0035 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 8/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m139s\u001b[0m 22ms/step - accuracy: 0.9991 - loss: 0.0032 - val_accuracy: 1.0000 - val_loss: 2.3842e-10\n",
      "Epoch 9/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m128s\u001b[0m 21ms/step - accuracy: 0.9991 - loss: 0.0028 - val_accuracy: 1.0000 - val_loss: 9.1195e-09\n",
      "Epoch 10/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m131s\u001b[0m 21ms/step - accuracy: 0.9991 - loss: 0.0045 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 11/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m136s\u001b[0m 22ms/step - accuracy: 0.9990 - loss: 0.0035 - val_accuracy: 1.0000 - val_loss: 7.1526e-10\n",
      "Epoch 11: early stopping\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step\n",
      "\n",
      "Training fold 9\n",
      "\n",
      "Generating upsampling using SMOTE...\n",
      "200000 samples after upsampling.\n",
      "Class distribution for training after upsampling: Counter({np.int64(0): 6250, np.int64(1): 6250, np.int64(2): 6250, np.int64(3): 6250, np.int64(4): 6250, np.int64(5): 6250, np.int64(6): 6250, np.int64(7): 6250, np.int64(8): 6250, np.int64(9): 6250, np.int64(10): 6250, np.int64(11): 6250, np.int64(12): 6250, np.int64(13): 6250, np.int64(14): 6250, np.int64(15): 6250, np.int64(16): 6250, np.int64(17): 6250, np.int64(18): 6250, np.int64(19): 6250, np.int64(20): 6250, np.int64(21): 6250, np.int64(22): 6250, np.int64(23): 6250, np.int64(24): 6250, np.int64(25): 6250, np.int64(26): 6250, np.int64(27): 6250, np.int64(28): 6250, np.int64(29): 6250, np.int64(30): 6250, np.int64(31): 6250})\n",
      "Finishing upsampling.\n",
      "\n",
      "Epoch 1/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m137s\u001b[0m 22ms/step - accuracy: 0.9968 - loss: 0.0109 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 2/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m139s\u001b[0m 23ms/step - accuracy: 0.9984 - loss: 0.0060 - val_accuracy: 1.0000 - val_loss: 1.0431e-08\n",
      "Epoch 3/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m135s\u001b[0m 22ms/step - accuracy: 0.9986 - loss: 0.0050 - val_accuracy: 1.0000 - val_loss: 1.6093e-08\n",
      "Epoch 4/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m134s\u001b[0m 22ms/step - accuracy: 0.9989 - loss: 0.0039 - val_accuracy: 1.0000 - val_loss: 7.1526e-10\n",
      "Epoch 5/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m138s\u001b[0m 22ms/step - accuracy: 0.9989 - loss: 0.0045 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 6/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m136s\u001b[0m 22ms/step - accuracy: 0.9989 - loss: 0.0032 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 7/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m133s\u001b[0m 21ms/step - accuracy: 0.9990 - loss: 0.0038 - val_accuracy: 1.0000 - val_loss: 5.7860e-07\n",
      "Epoch 8/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m138s\u001b[0m 22ms/step - accuracy: 0.9990 - loss: 0.0035 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 9/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m133s\u001b[0m 21ms/step - accuracy: 0.9990 - loss: 0.0032 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 10/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m131s\u001b[0m 21ms/step - accuracy: 0.9991 - loss: 0.0031 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 11/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m136s\u001b[0m 22ms/step - accuracy: 0.9991 - loss: 0.0036 - val_accuracy: 1.0000 - val_loss: 9.8126e-06\n",
      "Epoch 11: early stopping\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "\n",
      "Training fold 10\n",
      "\n",
      "Generating upsampling using SMOTE...\n",
      "200000 samples after upsampling.\n",
      "Class distribution for training after upsampling: Counter({np.int64(0): 6250, np.int64(1): 6250, np.int64(2): 6250, np.int64(3): 6250, np.int64(4): 6250, np.int64(5): 6250, np.int64(6): 6250, np.int64(7): 6250, np.int64(8): 6250, np.int64(9): 6250, np.int64(10): 6250, np.int64(11): 6250, np.int64(12): 6250, np.int64(13): 6250, np.int64(14): 6250, np.int64(15): 6250, np.int64(16): 6250, np.int64(17): 6250, np.int64(18): 6250, np.int64(19): 6250, np.int64(20): 6250, np.int64(21): 6250, np.int64(22): 6250, np.int64(23): 6250, np.int64(24): 6250, np.int64(25): 6250, np.int64(26): 6250, np.int64(27): 6250, np.int64(28): 6250, np.int64(29): 6250, np.int64(30): 6250, np.int64(31): 6250})\n",
      "Finishing upsampling.\n",
      "\n",
      "Epoch 1/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m136s\u001b[0m 22ms/step - accuracy: 0.9974 - loss: 0.0096 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 2/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m137s\u001b[0m 22ms/step - accuracy: 0.9986 - loss: 0.0052 - val_accuracy: 1.0000 - val_loss: 2.1458e-09\n",
      "Epoch 3/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m131s\u001b[0m 21ms/step - accuracy: 0.9987 - loss: 0.0042 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 4/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m132s\u001b[0m 21ms/step - accuracy: 0.9990 - loss: 0.0037 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 5/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m137s\u001b[0m 22ms/step - accuracy: 0.9991 - loss: 0.0032 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 6/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m138s\u001b[0m 22ms/step - accuracy: 0.9989 - loss: 0.0040 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 7/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m133s\u001b[0m 22ms/step - accuracy: 0.9990 - loss: 0.0040 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 8/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m139s\u001b[0m 22ms/step - accuracy: 0.9990 - loss: 0.0037 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 9/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 23ms/step - accuracy: 0.9990 - loss: 0.0033 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 10/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 23ms/step - accuracy: 0.9991 - loss: 0.0034 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 11/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 23ms/step - accuracy: 0.9992 - loss: 0.0027 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 11: early stopping\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step\n",
      "\n",
      "Time taken for training:  06:00:47\n",
      "\n",
      "\n",
      "Fold 1 - Train Accuracy 0.9993 - Test Accuracy 0.4874\n",
      "Fold 2 - Train Accuracy 0.9985 - Test Accuracy 0.8311\n",
      "Fold 3 - Train Accuracy 0.9997 - Test Accuracy 0.8683\n",
      "Fold 4 - Train Accuracy 0.9996 - Test Accuracy 0.9060\n",
      "Fold 5 - Train Accuracy 0.9993 - Test Accuracy 0.9682\n",
      "Fold 6 - Train Accuracy 0.9995 - Test Accuracy 0.9658\n",
      "Fold 7 - Train Accuracy 0.9995 - Test Accuracy 0.9138\n",
      "Fold 8 - Train Accuracy 0.9994 - Test Accuracy 0.9810\n",
      "Fold 9 - Train Accuracy 0.9997 - Test Accuracy 0.9841\n",
      "Fold 10 - Train Accuracy 0.9992 - Test Accuracy 0.9860\n",
      "\n",
      "Mean Train Accuracy: 0.9994 - Std: 0.0003 \n",
      "Mean Test Accuracy: 0.8892 - Std: 0.1432 \n",
      "\n",
      "Evaluate other metrics:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.90      0.92     13905\n",
      "           1       0.91      0.95      0.93       723\n",
      "           2       0.94      0.95      0.95      2659\n",
      "           3       0.80      0.80      0.80      1123\n",
      "           4       0.85      0.82      0.83       384\n",
      "           5       0.79      0.86      0.83        44\n",
      "           6       0.83      0.88      0.85       663\n",
      "           7       0.86      0.87      0.87        94\n",
      "           8       0.83      0.83      0.83        12\n",
      "           9       0.88      0.88      0.88       786\n",
      "          10       0.43      0.67      0.52         9\n",
      "          11       0.88      0.86      0.87        98\n",
      "          12       0.67      0.75      0.71         8\n",
      "          13       0.78      0.90      0.84        20\n",
      "          14       0.93      0.90      0.91        77\n",
      "          15       0.93      0.92      0.93        62\n",
      "          16       0.73      0.84      0.78       917\n",
      "          17       0.83      0.93      0.88       473\n",
      "          18       0.90      0.86      0.88        22\n",
      "          19       0.76      0.84      0.80       122\n",
      "          20       0.90      0.84      0.87       111\n",
      "          21       0.83      0.86      0.84       201\n",
      "          22       0.70      1.00      0.82         7\n",
      "          23       0.80      0.78      0.79        96\n",
      "          24       0.76      0.83      0.79      1045\n",
      "          25       0.78      0.84      0.81       540\n",
      "          26       0.78      0.81      0.80      1334\n",
      "          27       0.89      0.86      0.87        28\n",
      "          28       0.89      0.91      0.90        35\n",
      "          29       0.89      0.83      0.86        77\n",
      "          30       0.77      0.89      0.83        64\n",
      "          31       0.86      0.86      0.86         7\n",
      "\n",
      "    accuracy                           0.89     25746\n",
      "   macro avg       0.82      0.86      0.84     25746\n",
      "weighted avg       0.89      0.89      0.89     25746\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Build a \"time window\" structure to handle the dataset as a time series.\n",
    "new_df = load_dataset(\"aVL\")\n",
    "X_array, y_array = build_time_window_structure(new_df, \"aVL\")\n",
    "X_array, y_array = remove_classes_with_less_samples(X_array, y_array)\n",
    "\n",
    "# Train the CNN model.\n",
    "v1_model = create_v1()\n",
    "v1_num_epochs = 300\n",
    "v1_batch_size = 32\n",
    "v1_validation_split = 0.01\n",
    "\n",
    "training_history_v1 = train_cnn_model(v1_model, X_array, y_array, v1_num_epochs, v1_batch_size, v1_validation_split, \"v1_model_aVL.keras\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2208e2-0e65-456a-b89e-12edae1f335e",
   "metadata": {},
   "source": [
    "#### aVF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "edc52dc0-fd6e-4fbb-8e35-49f257922c67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Start loading CSV file...\n",
      "Finish loading CSV file.\n",
      "\n",
      "Starting build_time_window_structure function...\n",
      "\n",
      "Shape of features:  (25770, 1250)\n",
      "Quantity os samples (labels):  25770\n",
      "\n",
      "Finishing build_time_window_structure function.\n",
      "\n",
      "Remove classes with less than 6 samples.\n",
      "\n",
      "Show classes identification:\n",
      "Class: 0 - Arrhythmia code: 1\n",
      "Class: 1 - Arrhythmia code: 21\n",
      "Class: 2 - Arrhythmia code: 22\n",
      "Class: 3 - Arrhythmia code: 23\n",
      "Class: 4 - Arrhythmia code: 30\n",
      "Class: 5 - Arrhythmia code: 36\n",
      "Class: 6 - Arrhythmia code: 50\n",
      "Class: 7 - Arrhythmia code: 51\n",
      "Class: 8 - Arrhythmia code: 54\n",
      "Class: 9 - Arrhythmia code: 60\n",
      "Class: 10 - Arrhythmia code: 80\n",
      "Class: 11 - Arrhythmia code: 82\n",
      "Class: 12 - Arrhythmia code: 83\n",
      "Class: 13 - Arrhythmia code: 88\n",
      "Class: 14 - Arrhythmia code: 101\n",
      "Class: 15 - Arrhythmia code: 104\n",
      "Class: 16 - Arrhythmia code: 105\n",
      "Class: 17 - Arrhythmia code: 106\n",
      "Class: 18 - Arrhythmia code: 108\n",
      "Class: 19 - Arrhythmia code: 120\n",
      "Class: 20 - Arrhythmia code: 121\n",
      "Class: 21 - Arrhythmia code: 125\n",
      "Class: 22 - Arrhythmia code: 140\n",
      "Class: 23 - Arrhythmia code: 142\n",
      "Class: 24 - Arrhythmia code: 145\n",
      "Class: 25 - Arrhythmia code: 146\n",
      "Class: 26 - Arrhythmia code: 147\n",
      "Class: 27 - Arrhythmia code: 155\n",
      "Class: 28 - Arrhythmia code: 160\n",
      "Class: 29 - Arrhythmia code: 161\n",
      "Class: 30 - Arrhythmia code: 165\n",
      "Class: 31 - Arrhythmia code: 166\n",
      "\n",
      "Check for dataset balance:\n",
      "Class =   0   Qty =    13905   Percentage = 54.01 %\n",
      "Class =   2   Qty =     2659   Percentage = 10.33 %\n",
      "Class =  26   Qty =     1334   Percentage = 5.18 %\n",
      "Class =   3   Qty =     1123   Percentage = 4.36 %\n",
      "Class =  24   Qty =     1045   Percentage = 4.06 %\n",
      "Class =  16   Qty =      917   Percentage = 3.56 %\n",
      "Class =   9   Qty =      786   Percentage = 3.05 %\n",
      "Class =   1   Qty =      723   Percentage = 2.81 %\n",
      "Class =   6   Qty =      663   Percentage = 2.58 %\n",
      "Class =  25   Qty =      540   Percentage = 2.10 %\n",
      "Class =  17   Qty =      473   Percentage = 1.84 %\n",
      "Class =   4   Qty =      384   Percentage = 1.49 %\n",
      "Class =  21   Qty =      201   Percentage = 0.78 %\n",
      "Class =  19   Qty =      122   Percentage = 0.47 %\n",
      "Class =  20   Qty =      111   Percentage = 0.43 %\n",
      "Class =  11   Qty =       98   Percentage = 0.38 %\n",
      "Class =  23   Qty =       96   Percentage = 0.37 %\n",
      "Class =   7   Qty =       94   Percentage = 0.37 %\n",
      "Class =  14   Qty =       77   Percentage = 0.30 %\n",
      "Class =  29   Qty =       77   Percentage = 0.30 %\n",
      "Class =  30   Qty =       64   Percentage = 0.25 %\n",
      "Class =  15   Qty =       62   Percentage = 0.24 %\n",
      "Class =   5   Qty =       44   Percentage = 0.17 %\n",
      "Class =  28   Qty =       35   Percentage = 0.14 %\n",
      "Class =  27   Qty =       28   Percentage = 0.11 %\n",
      "Class =  18   Qty =       22   Percentage = 0.09 %\n",
      "Class =  13   Qty =       20   Percentage = 0.08 %\n",
      "Class =   8   Qty =       12   Percentage = 0.05 %\n",
      "Class =  10   Qty =        9   Percentage = 0.03 %\n",
      "Class =  12   Qty =        8   Percentage = 0.03 %\n",
      "Class =  22   Qty =        7   Percentage = 0.03 %\n",
      "Class =  31   Qty =        7   Percentage = 0.03 %\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv1d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1248</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)        │            <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_5           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1248</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)        │            <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1246</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)        │           <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_6           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1246</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)        │            <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">623</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">619</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)        │           <span style=\"color: #00af00; text-decoration-color: #00af00\">656</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_7           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">619</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)        │            <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">615</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)        │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,296</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_8           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">615</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)        │            <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">307</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4912</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">628,864</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_9           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,128</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv1d_4 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1248\u001b[0m, \u001b[38;5;34m8\u001b[0m)        │            \u001b[38;5;34m32\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_5           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1248\u001b[0m, \u001b[38;5;34m8\u001b[0m)        │            \u001b[38;5;34m32\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_5 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1246\u001b[0m, \u001b[38;5;34m8\u001b[0m)        │           \u001b[38;5;34m200\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_6           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1246\u001b[0m, \u001b[38;5;34m8\u001b[0m)        │            \u001b[38;5;34m32\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_2 (\u001b[38;5;33mMaxPooling1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m623\u001b[0m, \u001b[38;5;34m8\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_6 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m619\u001b[0m, \u001b[38;5;34m16\u001b[0m)        │           \u001b[38;5;34m656\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_7           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m619\u001b[0m, \u001b[38;5;34m16\u001b[0m)        │            \u001b[38;5;34m64\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_7 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m615\u001b[0m, \u001b[38;5;34m16\u001b[0m)        │         \u001b[38;5;34m1,296\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_8           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m615\u001b[0m, \u001b[38;5;34m16\u001b[0m)        │            \u001b[38;5;34m64\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_3 (\u001b[38;5;33mMaxPooling1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m307\u001b[0m, \u001b[38;5;34m16\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_1 (\u001b[38;5;33mFlatten\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4912\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │       \u001b[38;5;34m628,864\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_9           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │           \u001b[38;5;34m512\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │         \u001b[38;5;34m4,128\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">635,880</span> (2.43 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m635,880\u001b[0m (2.43 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">635,528</span> (2.42 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m635,528\u001b[0m (2.42 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">352</span> (1.38 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m352\u001b[0m (1.38 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting training...\n",
      "\n",
      "Training fold 1\n",
      "\n",
      "Generating upsampling using SMOTE...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\DeveloperTools\\python\\3.11.0\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:737: UserWarning: The least populated class in y has only 7 members, which is less than n_splits=10.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200000 samples after upsampling.\n",
      "Class distribution for training after upsampling: Counter({np.int64(0): 6250, np.int64(1): 6250, np.int64(2): 6250, np.int64(3): 6250, np.int64(4): 6250, np.int64(5): 6250, np.int64(6): 6250, np.int64(7): 6250, np.int64(8): 6250, np.int64(9): 6250, np.int64(10): 6250, np.int64(11): 6250, np.int64(12): 6250, np.int64(13): 6250, np.int64(14): 6250, np.int64(15): 6250, np.int64(16): 6250, np.int64(17): 6250, np.int64(18): 6250, np.int64(19): 6250, np.int64(20): 6250, np.int64(21): 6250, np.int64(22): 6250, np.int64(23): 6250, np.int64(24): 6250, np.int64(25): 6250, np.int64(26): 6250, np.int64(27): 6250, np.int64(28): 6250, np.int64(29): 6250, np.int64(30): 6250, np.int64(31): 6250})\n",
      "Finishing upsampling.\n",
      "\n",
      "Epoch 1/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m178s\u001b[0m 28ms/step - accuracy: 0.8506 - loss: 0.5835 - val_accuracy: 1.0000 - val_loss: 8.0297e-04\n",
      "Epoch 2/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m175s\u001b[0m 28ms/step - accuracy: 0.9761 - loss: 0.0769 - val_accuracy: 1.0000 - val_loss: 4.2102e-06\n",
      "Epoch 3/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m181s\u001b[0m 29ms/step - accuracy: 0.9882 - loss: 0.0381 - val_accuracy: 1.0000 - val_loss: 1.3718e-05\n",
      "Epoch 4/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m187s\u001b[0m 30ms/step - accuracy: 0.9921 - loss: 0.0245 - val_accuracy: 1.0000 - val_loss: 7.5820e-05\n",
      "Epoch 5/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m175s\u001b[0m 28ms/step - accuracy: 0.9940 - loss: 0.0190 - val_accuracy: 1.0000 - val_loss: 4.7021e-07\n",
      "Epoch 6/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m185s\u001b[0m 30ms/step - accuracy: 0.9948 - loss: 0.0157 - val_accuracy: 1.0000 - val_loss: 3.5052e-06\n",
      "Epoch 7/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m188s\u001b[0m 30ms/step - accuracy: 0.9958 - loss: 0.0133 - val_accuracy: 1.0000 - val_loss: 4.5001e-08\n",
      "Epoch 8/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m176s\u001b[0m 28ms/step - accuracy: 0.9963 - loss: 0.0120 - val_accuracy: 1.0000 - val_loss: 1.6153e-08\n",
      "Epoch 9/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m186s\u001b[0m 30ms/step - accuracy: 0.9966 - loss: 0.0101 - val_accuracy: 1.0000 - val_loss: 5.1260e-09\n",
      "Epoch 10/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m184s\u001b[0m 30ms/step - accuracy: 0.9966 - loss: 0.0107 - val_accuracy: 1.0000 - val_loss: 2.9802e-10\n",
      "Epoch 11/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m180s\u001b[0m 29ms/step - accuracy: 0.9973 - loss: 0.0089 - val_accuracy: 1.0000 - val_loss: 5.1598e-06\n",
      "Epoch 12/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m191s\u001b[0m 31ms/step - accuracy: 0.9972 - loss: 0.0087 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 13/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m177s\u001b[0m 29ms/step - accuracy: 0.9977 - loss: 0.0073 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 14/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m183s\u001b[0m 30ms/step - accuracy: 0.9977 - loss: 0.0073 - val_accuracy: 1.0000 - val_loss: 3.2902e-08\n",
      "Epoch 15/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m197s\u001b[0m 32ms/step - accuracy: 0.9976 - loss: 0.0077 - val_accuracy: 1.0000 - val_loss: 4.0948e-08\n",
      "Epoch 16/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m181s\u001b[0m 29ms/step - accuracy: 0.9978 - loss: 0.0073 - val_accuracy: 1.0000 - val_loss: 2.3842e-09\n",
      "Epoch 17/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m189s\u001b[0m 30ms/step - accuracy: 0.9980 - loss: 0.0058 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 18/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m200s\u001b[0m 32ms/step - accuracy: 0.9981 - loss: 0.0058 - val_accuracy: 1.0000 - val_loss: 8.7689e-07\n",
      "Epoch 19/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m192s\u001b[0m 31ms/step - accuracy: 0.9983 - loss: 0.0055 - val_accuracy: 1.0000 - val_loss: 9.5056e-07\n",
      "Epoch 20/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m193s\u001b[0m 31ms/step - accuracy: 0.9983 - loss: 0.0059 - val_accuracy: 1.0000 - val_loss: 9.3341e-08\n",
      "Epoch 21/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m207s\u001b[0m 33ms/step - accuracy: 0.9983 - loss: 0.0060 - val_accuracy: 1.0000 - val_loss: 1.0610e-08\n",
      "Epoch 22/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m205s\u001b[0m 33ms/step - accuracy: 0.9985 - loss: 0.0049 - val_accuracy: 1.0000 - val_loss: 8.9407e-10\n",
      "Epoch 22: early stopping\n",
      "Restoring model weights from the end of the best epoch: 12.\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step\n",
      "\n",
      "Training fold 2\n",
      "\n",
      "Generating upsampling using SMOTE...\n",
      "200000 samples after upsampling.\n",
      "Class distribution for training after upsampling: Counter({np.int64(0): 6250, np.int64(1): 6250, np.int64(2): 6250, np.int64(3): 6250, np.int64(4): 6250, np.int64(5): 6250, np.int64(6): 6250, np.int64(7): 6250, np.int64(8): 6250, np.int64(9): 6250, np.int64(10): 6250, np.int64(11): 6250, np.int64(12): 6250, np.int64(13): 6250, np.int64(14): 6250, np.int64(15): 6250, np.int64(16): 6250, np.int64(17): 6250, np.int64(18): 6250, np.int64(19): 6250, np.int64(20): 6250, np.int64(21): 6250, np.int64(22): 6250, np.int64(23): 6250, np.int64(24): 6250, np.int64(25): 6250, np.int64(26): 6250, np.int64(27): 6250, np.int64(28): 6250, np.int64(29): 6250, np.int64(30): 6250, np.int64(31): 6250})\n",
      "Finishing upsampling.\n",
      "\n",
      "Epoch 1/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m207s\u001b[0m 33ms/step - accuracy: 0.9713 - loss: 0.1595 - val_accuracy: 1.0000 - val_loss: 1.5861e-07\n",
      "Epoch 2/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m212s\u001b[0m 34ms/step - accuracy: 0.9952 - loss: 0.0154 - val_accuracy: 1.0000 - val_loss: 7.7497e-07\n",
      "Epoch 3/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m197s\u001b[0m 32ms/step - accuracy: 0.9974 - loss: 0.0091 - val_accuracy: 1.0000 - val_loss: 1.2755e-08\n",
      "Epoch 4/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m209s\u001b[0m 34ms/step - accuracy: 0.9978 - loss: 0.0072 - val_accuracy: 1.0000 - val_loss: 5.2540e-07\n",
      "Epoch 5/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m222s\u001b[0m 36ms/step - accuracy: 0.9977 - loss: 0.0075 - val_accuracy: 1.0000 - val_loss: 1.4305e-09\n",
      "Epoch 6/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m209s\u001b[0m 34ms/step - accuracy: 0.9982 - loss: 0.0060 - val_accuracy: 1.0000 - val_loss: 1.3530e-08\n",
      "Epoch 7/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m208s\u001b[0m 34ms/step - accuracy: 0.9979 - loss: 0.0075 - val_accuracy: 1.0000 - val_loss: 4.4227e-08\n",
      "Epoch 8/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m221s\u001b[0m 36ms/step - accuracy: 0.9983 - loss: 0.0060 - val_accuracy: 1.0000 - val_loss: 7.6890e-09\n",
      "Epoch 9/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m206s\u001b[0m 33ms/step - accuracy: 0.9983 - loss: 0.0060 - val_accuracy: 1.0000 - val_loss: 6.1393e-09\n",
      "Epoch 10/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m220s\u001b[0m 36ms/step - accuracy: 0.9985 - loss: 0.0050 - val_accuracy: 1.0000 - val_loss: 1.4102e-07\n",
      "Epoch 11/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m216s\u001b[0m 35ms/step - accuracy: 0.9985 - loss: 0.0051 - val_accuracy: 1.0000 - val_loss: 1.7881e-10\n",
      "Epoch 12/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m212s\u001b[0m 34ms/step - accuracy: 0.9984 - loss: 0.0052 - val_accuracy: 1.0000 - val_loss: 2.8610e-09\n",
      "Epoch 13/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m223s\u001b[0m 36ms/step - accuracy: 0.9988 - loss: 0.0046 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 14/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m223s\u001b[0m 36ms/step - accuracy: 0.9983 - loss: 0.0056 - val_accuracy: 1.0000 - val_loss: 1.5736e-08\n",
      "Epoch 15/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m208s\u001b[0m 34ms/step - accuracy: 0.9986 - loss: 0.0051 - val_accuracy: 1.0000 - val_loss: 1.0729e-09\n",
      "Epoch 16/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m226s\u001b[0m 36ms/step - accuracy: 0.9988 - loss: 0.0046 - val_accuracy: 1.0000 - val_loss: 1.1730e-07\n",
      "Epoch 17/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m210s\u001b[0m 34ms/step - accuracy: 0.9989 - loss: 0.0040 - val_accuracy: 1.0000 - val_loss: 6.3777e-09\n",
      "Epoch 18/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m221s\u001b[0m 36ms/step - accuracy: 0.9987 - loss: 0.0044 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 19/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m216s\u001b[0m 35ms/step - accuracy: 0.9985 - loss: 0.0054 - val_accuracy: 1.0000 - val_loss: 1.5175e-07\n",
      "Epoch 20/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m216s\u001b[0m 35ms/step - accuracy: 0.9988 - loss: 0.0038 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 21/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m223s\u001b[0m 36ms/step - accuracy: 0.9990 - loss: 0.0035 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 22/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m214s\u001b[0m 35ms/step - accuracy: 0.9990 - loss: 0.0038 - val_accuracy: 1.0000 - val_loss: 3.6955e-09\n",
      "Epoch 23/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m272s\u001b[0m 44ms/step - accuracy: 0.9990 - loss: 0.0037 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 23: early stopping\n",
      "Restoring model weights from the end of the best epoch: 13.\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step\n",
      "\n",
      "Training fold 3\n",
      "\n",
      "Generating upsampling using SMOTE...\n",
      "200000 samples after upsampling.\n",
      "Class distribution for training after upsampling: Counter({np.int64(0): 6250, np.int64(1): 6250, np.int64(2): 6250, np.int64(3): 6250, np.int64(4): 6250, np.int64(5): 6250, np.int64(6): 6250, np.int64(7): 6250, np.int64(8): 6250, np.int64(9): 6250, np.int64(10): 6250, np.int64(11): 6250, np.int64(12): 6250, np.int64(13): 6250, np.int64(14): 6250, np.int64(15): 6250, np.int64(16): 6250, np.int64(17): 6250, np.int64(18): 6250, np.int64(19): 6250, np.int64(20): 6250, np.int64(21): 6250, np.int64(22): 6250, np.int64(23): 6250, np.int64(24): 6250, np.int64(25): 6250, np.int64(26): 6250, np.int64(27): 6250, np.int64(28): 6250, np.int64(29): 6250, np.int64(30): 6250, np.int64(31): 6250})\n",
      "Finishing upsampling.\n",
      "\n",
      "Epoch 1/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m245s\u001b[0m 40ms/step - accuracy: 0.9864 - loss: 0.0624 - val_accuracy: 1.0000 - val_loss: 5.4834e-07\n",
      "Epoch 2/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m221s\u001b[0m 36ms/step - accuracy: 0.9972 - loss: 0.0097 - val_accuracy: 1.0000 - val_loss: 1.9670e-09\n",
      "Epoch 3/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m191s\u001b[0m 31ms/step - accuracy: 0.9977 - loss: 0.0097 - val_accuracy: 1.0000 - val_loss: 2.9802e-08\n",
      "Epoch 4/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m200s\u001b[0m 32ms/step - accuracy: 0.9984 - loss: 0.0055 - val_accuracy: 1.0000 - val_loss: 1.8221e-07\n",
      "Epoch 5/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m213s\u001b[0m 34ms/step - accuracy: 0.9986 - loss: 0.0056 - val_accuracy: 1.0000 - val_loss: 4.0173e-08\n",
      "Epoch 6/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m224s\u001b[0m 36ms/step - accuracy: 0.9984 - loss: 0.0053 - val_accuracy: 1.0000 - val_loss: 1.3649e-08\n",
      "Epoch 7/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m220s\u001b[0m 36ms/step - accuracy: 0.9986 - loss: 0.0048 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 8/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m217s\u001b[0m 35ms/step - accuracy: 0.9984 - loss: 0.0057 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 9/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m232s\u001b[0m 38ms/step - accuracy: 0.9987 - loss: 0.0051 - val_accuracy: 1.0000 - val_loss: 1.2290e-07\n",
      "Epoch 10/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m218s\u001b[0m 35ms/step - accuracy: 0.9986 - loss: 0.0052 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 11/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m218s\u001b[0m 35ms/step - accuracy: 0.9989 - loss: 0.0037 - val_accuracy: 1.0000 - val_loss: 1.4305e-09\n",
      "Epoch 12/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m229s\u001b[0m 37ms/step - accuracy: 0.9987 - loss: 0.0041 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 13/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m213s\u001b[0m 34ms/step - accuracy: 0.9989 - loss: 0.0039 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 14/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m227s\u001b[0m 37ms/step - accuracy: 0.9990 - loss: 0.0040 - val_accuracy: 1.0000 - val_loss: 9.2487e-06\n",
      "Epoch 15/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m217s\u001b[0m 35ms/step - accuracy: 0.9990 - loss: 0.0035 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 16/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m225s\u001b[0m 36ms/step - accuracy: 0.9991 - loss: 0.0036 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 17/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m218s\u001b[0m 35ms/step - accuracy: 0.9991 - loss: 0.0032 - val_accuracy: 1.0000 - val_loss: 7.0930e-09\n",
      "Epoch 17: early stopping\n",
      "Restoring model weights from the end of the best epoch: 7.\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step\n",
      "\n",
      "Training fold 4\n",
      "\n",
      "Generating upsampling using SMOTE...\n",
      "200000 samples after upsampling.\n",
      "Class distribution for training after upsampling: Counter({np.int64(0): 6250, np.int64(1): 6250, np.int64(2): 6250, np.int64(3): 6250, np.int64(4): 6250, np.int64(5): 6250, np.int64(6): 6250, np.int64(7): 6250, np.int64(8): 6250, np.int64(9): 6250, np.int64(10): 6250, np.int64(11): 6250, np.int64(12): 6250, np.int64(13): 6250, np.int64(14): 6250, np.int64(15): 6250, np.int64(16): 6250, np.int64(17): 6250, np.int64(18): 6250, np.int64(19): 6250, np.int64(20): 6250, np.int64(21): 6250, np.int64(22): 6250, np.int64(23): 6250, np.int64(24): 6250, np.int64(25): 6250, np.int64(26): 6250, np.int64(27): 6250, np.int64(28): 6250, np.int64(29): 6250, np.int64(30): 6250, np.int64(31): 6250})\n",
      "Finishing upsampling.\n",
      "\n",
      "Epoch 1/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m243s\u001b[0m 39ms/step - accuracy: 0.9927 - loss: 0.0266 - val_accuracy: 1.0000 - val_loss: 2.1425e-06\n",
      "Epoch 2/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m229s\u001b[0m 37ms/step - accuracy: 0.9979 - loss: 0.0069 - val_accuracy: 1.0000 - val_loss: 9.7156e-09\n",
      "Epoch 3/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m245s\u001b[0m 40ms/step - accuracy: 0.9985 - loss: 0.0053 - val_accuracy: 1.0000 - val_loss: 1.4156e-07\n",
      "Epoch 4/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m222s\u001b[0m 36ms/step - accuracy: 0.9988 - loss: 0.0044 - val_accuracy: 1.0000 - val_loss: 1.6689e-09\n",
      "Epoch 5/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m260s\u001b[0m 42ms/step - accuracy: 0.9988 - loss: 0.0041 - val_accuracy: 1.0000 - val_loss: 8.8452e-08\n",
      "Epoch 6/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m263s\u001b[0m 42ms/step - accuracy: 0.9990 - loss: 0.0039 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 7/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m254s\u001b[0m 41ms/step - accuracy: 0.9990 - loss: 0.0036 - val_accuracy: 1.0000 - val_loss: 7.4565e-08\n",
      "Epoch 8/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m249s\u001b[0m 40ms/step - accuracy: 0.9989 - loss: 0.0040 - val_accuracy: 1.0000 - val_loss: 1.2791e-07\n",
      "Epoch 9/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m261s\u001b[0m 42ms/step - accuracy: 0.9989 - loss: 0.0034 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 10/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m245s\u001b[0m 40ms/step - accuracy: 0.9991 - loss: 0.0036 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 11/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m259s\u001b[0m 42ms/step - accuracy: 0.9989 - loss: 0.0039 - val_accuracy: 1.0000 - val_loss: 1.2219e-08\n",
      "Epoch 12/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m247s\u001b[0m 40ms/step - accuracy: 0.9991 - loss: 0.0039 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 13/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m263s\u001b[0m 42ms/step - accuracy: 0.9992 - loss: 0.0032 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 14/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m251s\u001b[0m 41ms/step - accuracy: 0.9993 - loss: 0.0025 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 15/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m248s\u001b[0m 40ms/step - accuracy: 0.9990 - loss: 0.0041 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 16/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m255s\u001b[0m 41ms/step - accuracy: 0.9990 - loss: 0.0036 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 16: early stopping\n",
      "Restoring model weights from the end of the best epoch: 6.\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step\n",
      "\n",
      "Training fold 5\n",
      "\n",
      "Generating upsampling using SMOTE...\n",
      "200000 samples after upsampling.\n",
      "Class distribution for training after upsampling: Counter({np.int64(0): 6250, np.int64(1): 6250, np.int64(2): 6250, np.int64(3): 6250, np.int64(4): 6250, np.int64(5): 6250, np.int64(6): 6250, np.int64(7): 6250, np.int64(8): 6250, np.int64(9): 6250, np.int64(10): 6250, np.int64(11): 6250, np.int64(12): 6250, np.int64(13): 6250, np.int64(14): 6250, np.int64(15): 6250, np.int64(16): 6250, np.int64(17): 6250, np.int64(18): 6250, np.int64(19): 6250, np.int64(20): 6250, np.int64(21): 6250, np.int64(22): 6250, np.int64(23): 6250, np.int64(24): 6250, np.int64(25): 6250, np.int64(26): 6250, np.int64(27): 6250, np.int64(28): 6250, np.int64(29): 6250, np.int64(30): 6250, np.int64(31): 6250})\n",
      "Finishing upsampling.\n",
      "\n",
      "Epoch 1/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m257s\u001b[0m 41ms/step - accuracy: 0.9952 - loss: 0.0185 - val_accuracy: 1.0000 - val_loss: 2.5988e-08\n",
      "Epoch 2/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m276s\u001b[0m 45ms/step - accuracy: 0.9983 - loss: 0.0054 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 3/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m259s\u001b[0m 42ms/step - accuracy: 0.9987 - loss: 0.0037 - val_accuracy: 1.0000 - val_loss: 1.0490e-08\n",
      "Epoch 4/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m266s\u001b[0m 43ms/step - accuracy: 0.9985 - loss: 0.0058 - val_accuracy: 1.0000 - val_loss: 3.1590e-09\n",
      "Epoch 5/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m268s\u001b[0m 43ms/step - accuracy: 0.9989 - loss: 0.0044 - val_accuracy: 1.0000 - val_loss: 5.3644e-10\n",
      "Epoch 6/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m274s\u001b[0m 44ms/step - accuracy: 0.9992 - loss: 0.0034 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 7/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m279s\u001b[0m 45ms/step - accuracy: 0.9989 - loss: 0.0039 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 8/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m269s\u001b[0m 43ms/step - accuracy: 0.9991 - loss: 0.0034 - val_accuracy: 1.0000 - val_loss: 1.1146e-08\n",
      "Epoch 9/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m288s\u001b[0m 47ms/step - accuracy: 0.9991 - loss: 0.0038 - val_accuracy: 1.0000 - val_loss: 5.4478e-08\n",
      "Epoch 10/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m289s\u001b[0m 47ms/step - accuracy: 0.9991 - loss: 0.0030 - val_accuracy: 1.0000 - val_loss: 1.5497e-09\n",
      "Epoch 11/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m280s\u001b[0m 45ms/step - accuracy: 0.9991 - loss: 0.0037 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 12/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m288s\u001b[0m 46ms/step - accuracy: 0.9991 - loss: 0.0030 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 12: early stopping\n",
      "Restoring model weights from the end of the best epoch: 2.\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step\n",
      "\n",
      "Training fold 6\n",
      "\n",
      "Generating upsampling using SMOTE...\n",
      "200000 samples after upsampling.\n",
      "Class distribution for training after upsampling: Counter({np.int64(0): 6250, np.int64(1): 6250, np.int64(2): 6250, np.int64(3): 6250, np.int64(4): 6250, np.int64(5): 6250, np.int64(6): 6250, np.int64(7): 6250, np.int64(8): 6250, np.int64(9): 6250, np.int64(10): 6250, np.int64(11): 6250, np.int64(12): 6250, np.int64(13): 6250, np.int64(14): 6250, np.int64(15): 6250, np.int64(16): 6250, np.int64(17): 6250, np.int64(18): 6250, np.int64(19): 6250, np.int64(20): 6250, np.int64(21): 6250, np.int64(22): 6250, np.int64(23): 6250, np.int64(24): 6250, np.int64(25): 6250, np.int64(26): 6250, np.int64(27): 6250, np.int64(28): 6250, np.int64(29): 6250, np.int64(30): 6250, np.int64(31): 6250})\n",
      "Finishing upsampling.\n",
      "\n",
      "Epoch 1/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m310s\u001b[0m 50ms/step - accuracy: 0.9969 - loss: 0.0110 - val_accuracy: 1.0000 - val_loss: 9.9716e-06\n",
      "Epoch 2/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m286s\u001b[0m 46ms/step - accuracy: 0.9983 - loss: 0.0056 - val_accuracy: 1.0000 - val_loss: 5.4657e-08\n",
      "Epoch 3/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m305s\u001b[0m 49ms/step - accuracy: 0.9987 - loss: 0.0042 - val_accuracy: 1.0000 - val_loss: 1.8477e-09\n",
      "Epoch 4/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m308s\u001b[0m 50ms/step - accuracy: 0.9989 - loss: 0.0045 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 5/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m284s\u001b[0m 46ms/step - accuracy: 0.9990 - loss: 0.0039 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 6/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m304s\u001b[0m 49ms/step - accuracy: 0.9991 - loss: 0.0034 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 7/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m314s\u001b[0m 51ms/step - accuracy: 0.9994 - loss: 0.0029 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 8/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m305s\u001b[0m 49ms/step - accuracy: 0.9991 - loss: 0.0039 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 9/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m317s\u001b[0m 51ms/step - accuracy: 0.9991 - loss: 0.0036 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 10/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m337s\u001b[0m 54ms/step - accuracy: 0.9991 - loss: 0.0034 - val_accuracy: 1.0000 - val_loss: 4.5300e-09\n",
      "Epoch 11/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m315s\u001b[0m 51ms/step - accuracy: 0.9991 - loss: 0.0031 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 12/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m315s\u001b[0m 51ms/step - accuracy: 0.9991 - loss: 0.0033 - val_accuracy: 1.0000 - val_loss: 1.1921e-10\n",
      "Epoch 13/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m324s\u001b[0m 52ms/step - accuracy: 0.9991 - loss: 0.0036 - val_accuracy: 1.0000 - val_loss: 1.7881e-09\n",
      "Epoch 14/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m321s\u001b[0m 52ms/step - accuracy: 0.9994 - loss: 0.0024 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 14: early stopping\n",
      "Restoring model weights from the end of the best epoch: 4.\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step\n",
      "\n",
      "Training fold 7\n",
      "\n",
      "Generating upsampling using SMOTE...\n",
      "200000 samples after upsampling.\n",
      "Class distribution for training after upsampling: Counter({np.int64(0): 6250, np.int64(1): 6250, np.int64(2): 6250, np.int64(3): 6250, np.int64(4): 6250, np.int64(5): 6250, np.int64(6): 6250, np.int64(7): 6250, np.int64(8): 6250, np.int64(9): 6250, np.int64(10): 6250, np.int64(11): 6250, np.int64(12): 6250, np.int64(13): 6250, np.int64(14): 6250, np.int64(15): 6250, np.int64(16): 6250, np.int64(17): 6250, np.int64(18): 6250, np.int64(19): 6250, np.int64(20): 6250, np.int64(21): 6250, np.int64(22): 6250, np.int64(23): 6250, np.int64(24): 6250, np.int64(25): 6250, np.int64(26): 6250, np.int64(27): 6250, np.int64(28): 6250, np.int64(29): 6250, np.int64(30): 6250, np.int64(31): 6250})\n",
      "Finishing upsampling.\n",
      "\n",
      "Epoch 1/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m353s\u001b[0m 57ms/step - accuracy: 0.9970 - loss: 0.0114 - val_accuracy: 1.0000 - val_loss: 3.8623e-08\n",
      "Epoch 2/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m349s\u001b[0m 56ms/step - accuracy: 0.9986 - loss: 0.0051 - val_accuracy: 1.0000 - val_loss: 2.7763e-07\n",
      "Epoch 3/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m360s\u001b[0m 58ms/step - accuracy: 0.9988 - loss: 0.0041 - val_accuracy: 1.0000 - val_loss: 1.4579e-07\n",
      "Epoch 4/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m354s\u001b[0m 57ms/step - accuracy: 0.9992 - loss: 0.0028 - val_accuracy: 1.0000 - val_loss: 5.3644e-10\n",
      "Epoch 5/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m348s\u001b[0m 56ms/step - accuracy: 0.9991 - loss: 0.0032 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 6/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m337s\u001b[0m 55ms/step - accuracy: 0.9991 - loss: 0.0036 - val_accuracy: 1.0000 - val_loss: 5.9605e-11\n",
      "Epoch 7/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m349s\u001b[0m 56ms/step - accuracy: 0.9992 - loss: 0.0028 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 8/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m352s\u001b[0m 57ms/step - accuracy: 0.9993 - loss: 0.0029 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 9/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m364s\u001b[0m 59ms/step - accuracy: 0.9992 - loss: 0.0029 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 10/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m352s\u001b[0m 57ms/step - accuracy: 0.9992 - loss: 0.0028 - val_accuracy: 1.0000 - val_loss: 9.2803e-08\n",
      "Epoch 11/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m354s\u001b[0m 57ms/step - accuracy: 0.9992 - loss: 0.0030 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 12/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m347s\u001b[0m 56ms/step - accuracy: 0.9994 - loss: 0.0023 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 13/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m369s\u001b[0m 60ms/step - accuracy: 0.9991 - loss: 0.0033 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 14/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m348s\u001b[0m 56ms/step - accuracy: 0.9994 - loss: 0.0032 - val_accuracy: 1.0000 - val_loss: 1.9224e-05\n",
      "Epoch 15/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m359s\u001b[0m 58ms/step - accuracy: 0.9993 - loss: 0.0024 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 15: early stopping\n",
      "Restoring model weights from the end of the best epoch: 5.\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step\n",
      "\n",
      "Training fold 8\n",
      "\n",
      "Generating upsampling using SMOTE...\n",
      "200000 samples after upsampling.\n",
      "Class distribution for training after upsampling: Counter({np.int64(0): 6250, np.int64(1): 6250, np.int64(2): 6250, np.int64(3): 6250, np.int64(4): 6250, np.int64(5): 6250, np.int64(6): 6250, np.int64(7): 6250, np.int64(8): 6250, np.int64(9): 6250, np.int64(10): 6250, np.int64(11): 6250, np.int64(12): 6250, np.int64(13): 6250, np.int64(14): 6250, np.int64(15): 6250, np.int64(16): 6250, np.int64(17): 6250, np.int64(18): 6250, np.int64(19): 6250, np.int64(20): 6250, np.int64(21): 6250, np.int64(22): 6250, np.int64(23): 6250, np.int64(24): 6250, np.int64(25): 6250, np.int64(26): 6250, np.int64(27): 6250, np.int64(28): 6250, np.int64(29): 6250, np.int64(30): 6250, np.int64(31): 6250})\n",
      "Finishing upsampling.\n",
      "\n",
      "Epoch 1/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m374s\u001b[0m 60ms/step - accuracy: 0.9976 - loss: 0.0083 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 2/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m372s\u001b[0m 60ms/step - accuracy: 0.9988 - loss: 0.0038 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 3/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m367s\u001b[0m 59ms/step - accuracy: 0.9990 - loss: 0.0046 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 4/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m398s\u001b[0m 64ms/step - accuracy: 0.9989 - loss: 0.0042 - val_accuracy: 1.0000 - val_loss: 2.9802e-09\n",
      "Epoch 5/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m388s\u001b[0m 63ms/step - accuracy: 0.9990 - loss: 0.0034 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 6/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m387s\u001b[0m 63ms/step - accuracy: 0.9993 - loss: 0.0024 - val_accuracy: 1.0000 - val_loss: 4.1723e-10\n",
      "Epoch 7/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m383s\u001b[0m 62ms/step - accuracy: 0.9992 - loss: 0.0046 - val_accuracy: 1.0000 - val_loss: 2.9802e-10\n",
      "Epoch 8/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m388s\u001b[0m 63ms/step - accuracy: 0.9993 - loss: 0.0030 - val_accuracy: 1.0000 - val_loss: 5.3644e-09\n",
      "Epoch 9/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m382s\u001b[0m 62ms/step - accuracy: 0.9991 - loss: 0.0038 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 10/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m385s\u001b[0m 62ms/step - accuracy: 0.9993 - loss: 0.0034 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 11/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m400s\u001b[0m 65ms/step - accuracy: 0.9992 - loss: 0.0026 - val_accuracy: 1.0000 - val_loss: 6.2166e-08\n",
      "Epoch 11: early stopping\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step\n",
      "\n",
      "Training fold 9\n",
      "\n",
      "Generating upsampling using SMOTE...\n",
      "200000 samples after upsampling.\n",
      "Class distribution for training after upsampling: Counter({np.int64(0): 6250, np.int64(1): 6250, np.int64(2): 6250, np.int64(3): 6250, np.int64(4): 6250, np.int64(5): 6250, np.int64(6): 6250, np.int64(7): 6250, np.int64(8): 6250, np.int64(9): 6250, np.int64(10): 6250, np.int64(11): 6250, np.int64(12): 6250, np.int64(13): 6250, np.int64(14): 6250, np.int64(15): 6250, np.int64(16): 6250, np.int64(17): 6250, np.int64(18): 6250, np.int64(19): 6250, np.int64(20): 6250, np.int64(21): 6250, np.int64(22): 6250, np.int64(23): 6250, np.int64(24): 6250, np.int64(25): 6250, np.int64(26): 6250, np.int64(27): 6250, np.int64(28): 6250, np.int64(29): 6250, np.int64(30): 6250, np.int64(31): 6250})\n",
      "Finishing upsampling.\n",
      "\n",
      "Epoch 1/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m409s\u001b[0m 66ms/step - accuracy: 0.9977 - loss: 0.0076 - val_accuracy: 1.0000 - val_loss: 5.3644e-10\n",
      "Epoch 2/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m393s\u001b[0m 64ms/step - accuracy: 0.9987 - loss: 0.0048 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 3/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m404s\u001b[0m 65ms/step - accuracy: 0.9989 - loss: 0.0042 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 4/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m405s\u001b[0m 65ms/step - accuracy: 0.9992 - loss: 0.0036 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 5/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m405s\u001b[0m 65ms/step - accuracy: 0.9991 - loss: 0.0028 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 6/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m398s\u001b[0m 64ms/step - accuracy: 0.9993 - loss: 0.0026 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 7/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m404s\u001b[0m 65ms/step - accuracy: 0.9993 - loss: 0.0029 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 8/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m392s\u001b[0m 63ms/step - accuracy: 0.9992 - loss: 0.0029 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 9/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m391s\u001b[0m 63ms/step - accuracy: 0.9991 - loss: 0.0031 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 10/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m391s\u001b[0m 63ms/step - accuracy: 0.9993 - loss: 0.0026 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 11/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m411s\u001b[0m 66ms/step - accuracy: 0.9993 - loss: 0.0035 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 12/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m401s\u001b[0m 65ms/step - accuracy: 0.9993 - loss: 0.0022 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 12: early stopping\n",
      "Restoring model weights from the end of the best epoch: 2.\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 20ms/step\n",
      "\n",
      "Training fold 10\n",
      "\n",
      "Generating upsampling using SMOTE...\n",
      "200000 samples after upsampling.\n",
      "Class distribution for training after upsampling: Counter({np.int64(0): 6250, np.int64(1): 6250, np.int64(2): 6250, np.int64(3): 6250, np.int64(4): 6250, np.int64(5): 6250, np.int64(6): 6250, np.int64(7): 6250, np.int64(8): 6250, np.int64(9): 6250, np.int64(10): 6250, np.int64(11): 6250, np.int64(12): 6250, np.int64(13): 6250, np.int64(14): 6250, np.int64(15): 6250, np.int64(16): 6250, np.int64(17): 6250, np.int64(18): 6250, np.int64(19): 6250, np.int64(20): 6250, np.int64(21): 6250, np.int64(22): 6250, np.int64(23): 6250, np.int64(24): 6250, np.int64(25): 6250, np.int64(26): 6250, np.int64(27): 6250, np.int64(28): 6250, np.int64(29): 6250, np.int64(30): 6250, np.int64(31): 6250})\n",
      "Finishing upsampling.\n",
      "\n",
      "Epoch 1/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m402s\u001b[0m 65ms/step - accuracy: 0.9978 - loss: 0.0078 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 2/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m420s\u001b[0m 68ms/step - accuracy: 0.9990 - loss: 0.0046 - val_accuracy: 1.0000 - val_loss: 5.2452e-09\n",
      "Epoch 3/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m442s\u001b[0m 71ms/step - accuracy: 0.9991 - loss: 0.0036 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 4/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m416s\u001b[0m 67ms/step - accuracy: 0.9992 - loss: 0.0036 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 5/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m428s\u001b[0m 69ms/step - accuracy: 0.9994 - loss: 0.0022 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 6/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m417s\u001b[0m 67ms/step - accuracy: 0.9992 - loss: 0.0035 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 7/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m426s\u001b[0m 69ms/step - accuracy: 0.9993 - loss: 0.0025 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 8/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m431s\u001b[0m 70ms/step - accuracy: 0.9993 - loss: 0.0028 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 9/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m436s\u001b[0m 70ms/step - accuracy: 0.9992 - loss: 0.0035 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 10/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m433s\u001b[0m 70ms/step - accuracy: 0.9992 - loss: 0.0025 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 11/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m437s\u001b[0m 71ms/step - accuracy: 0.9993 - loss: 0.0027 - val_accuracy: 1.0000 - val_loss: 8.0466e-09\n",
      "Epoch 11: early stopping\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 20ms/step\n",
      "\n",
      "Time taken for training:  13:28:29\n",
      "\n",
      "\n",
      "Fold 1 - Train Accuracy 0.9994 - Test Accuracy 0.5534\n",
      "Fold 2 - Train Accuracy 0.9994 - Test Accuracy 0.7371\n",
      "Fold 3 - Train Accuracy 0.9996 - Test Accuracy 0.8998\n",
      "Fold 4 - Train Accuracy 0.9994 - Test Accuracy 0.9278\n",
      "Fold 5 - Train Accuracy 0.9997 - Test Accuracy 0.9724\n",
      "Fold 6 - Train Accuracy 0.9996 - Test Accuracy 0.9740\n",
      "Fold 7 - Train Accuracy 0.9998 - Test Accuracy 0.9767\n",
      "Fold 8 - Train Accuracy 0.9994 - Test Accuracy 0.9883\n",
      "Fold 9 - Train Accuracy 0.9998 - Test Accuracy 0.9860\n",
      "Fold 10 - Train Accuracy 0.9997 - Test Accuracy 0.9938\n",
      "\n",
      "Mean Train Accuracy: 0.9996 - Std: 0.0002 \n",
      "Mean Test Accuracy: 0.9009 - Std: 0.1371 \n",
      "\n",
      "Evaluate other metrics:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.93      0.93     13905\n",
      "           1       0.94      0.94      0.94       723\n",
      "           2       0.93      0.97      0.95      2659\n",
      "           3       0.84      0.81      0.82      1123\n",
      "           4       0.85      0.81      0.83       384\n",
      "           5       0.82      0.82      0.82        44\n",
      "           6       0.89      0.86      0.87       663\n",
      "           7       0.89      0.86      0.88        94\n",
      "           8       0.69      0.75      0.72        12\n",
      "           9       0.85      0.90      0.88       786\n",
      "          10       0.64      0.78      0.70         9\n",
      "          11       0.85      0.79      0.81        98\n",
      "          12       0.75      0.75      0.75         8\n",
      "          13       0.93      0.70      0.80        20\n",
      "          14       0.83      0.90      0.86        77\n",
      "          15       0.93      0.89      0.91        62\n",
      "          16       0.79      0.82      0.80       917\n",
      "          17       0.86      0.84      0.85       473\n",
      "          18       0.89      0.77      0.83        22\n",
      "          19       0.90      0.83      0.86       122\n",
      "          20       0.82      0.84      0.83       111\n",
      "          21       0.85      0.85      0.85       201\n",
      "          22       0.88      1.00      0.93         7\n",
      "          23       0.88      0.78      0.83        96\n",
      "          24       0.84      0.82      0.83      1045\n",
      "          25       0.82      0.84      0.83       540\n",
      "          26       0.80      0.83      0.81      1334\n",
      "          27       0.81      0.79      0.80        28\n",
      "          28       0.97      0.89      0.93        35\n",
      "          29       0.76      0.84      0.80        77\n",
      "          30       0.93      0.83      0.88        64\n",
      "          31       0.45      0.71      0.56         7\n",
      "\n",
      "    accuracy                           0.90     25746\n",
      "   macro avg       0.84      0.83      0.83     25746\n",
      "weighted avg       0.90      0.90      0.90     25746\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Build a \"time window\" structure to handle the dataset as a time series.\n",
    "new_df = load_dataset(\"aVF\")\n",
    "X_array, y_array = build_time_window_structure(new_df, \"aVF\")\n",
    "X_array, y_array = remove_classes_with_less_samples(X_array, y_array)\n",
    "\n",
    "# Train the CNN model.\n",
    "v1_model = create_v1()\n",
    "v1_num_epochs = 300\n",
    "v1_batch_size = 32\n",
    "v1_validation_split = 0.01\n",
    "\n",
    "training_history_v1 = train_cnn_model(v1_model, X_array, y_array, v1_num_epochs, v1_batch_size, v1_validation_split, \"v1_model_aVF.keras\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb71dcc-692a-42df-a689-fb840b30eb16",
   "metadata": {},
   "source": [
    "#### V1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b835848-0532-4dc4-a4bf-f8512aceeea9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Start loading CSV file...\n",
      "Finish loading CSV file.\n",
      "\n",
      "Starting build_time_window_structure function...\n",
      "\n",
      "Shape of features:  (25770, 1250)\n",
      "Quantity os samples (labels):  25770\n",
      "\n",
      "Finishing build_time_window_structure function.\n",
      "\n",
      "Remove classes with less than 6 samples.\n",
      "\n",
      "Show classes identification:\n",
      "Class: 0 - Arrhythmia code: 1\n",
      "Class: 1 - Arrhythmia code: 21\n",
      "Class: 2 - Arrhythmia code: 22\n",
      "Class: 3 - Arrhythmia code: 23\n",
      "Class: 4 - Arrhythmia code: 30\n",
      "Class: 5 - Arrhythmia code: 36\n",
      "Class: 6 - Arrhythmia code: 50\n",
      "Class: 7 - Arrhythmia code: 51\n",
      "Class: 8 - Arrhythmia code: 54\n",
      "Class: 9 - Arrhythmia code: 60\n",
      "Class: 10 - Arrhythmia code: 80\n",
      "Class: 11 - Arrhythmia code: 82\n",
      "Class: 12 - Arrhythmia code: 83\n",
      "Class: 13 - Arrhythmia code: 88\n",
      "Class: 14 - Arrhythmia code: 101\n",
      "Class: 15 - Arrhythmia code: 104\n",
      "Class: 16 - Arrhythmia code: 105\n",
      "Class: 17 - Arrhythmia code: 106\n",
      "Class: 18 - Arrhythmia code: 108\n",
      "Class: 19 - Arrhythmia code: 120\n",
      "Class: 20 - Arrhythmia code: 121\n",
      "Class: 21 - Arrhythmia code: 125\n",
      "Class: 22 - Arrhythmia code: 140\n",
      "Class: 23 - Arrhythmia code: 142\n",
      "Class: 24 - Arrhythmia code: 145\n",
      "Class: 25 - Arrhythmia code: 146\n",
      "Class: 26 - Arrhythmia code: 147\n",
      "Class: 27 - Arrhythmia code: 155\n",
      "Class: 28 - Arrhythmia code: 160\n",
      "Class: 29 - Arrhythmia code: 161\n",
      "Class: 30 - Arrhythmia code: 165\n",
      "Class: 31 - Arrhythmia code: 166\n",
      "\n",
      "Check for dataset balance:\n",
      "Class =   0   Qty =    13905   Percentage = 54.01 %\n",
      "Class =   2   Qty =     2659   Percentage = 10.33 %\n",
      "Class =  26   Qty =     1334   Percentage = 5.18 %\n",
      "Class =   3   Qty =     1123   Percentage = 4.36 %\n",
      "Class =  24   Qty =     1045   Percentage = 4.06 %\n",
      "Class =  16   Qty =      917   Percentage = 3.56 %\n",
      "Class =   9   Qty =      786   Percentage = 3.05 %\n",
      "Class =   1   Qty =      723   Percentage = 2.81 %\n",
      "Class =   6   Qty =      663   Percentage = 2.58 %\n",
      "Class =  25   Qty =      540   Percentage = 2.10 %\n",
      "Class =  17   Qty =      473   Percentage = 1.84 %\n",
      "Class =   4   Qty =      384   Percentage = 1.49 %\n",
      "Class =  21   Qty =      201   Percentage = 0.78 %\n",
      "Class =  19   Qty =      122   Percentage = 0.47 %\n",
      "Class =  20   Qty =      111   Percentage = 0.43 %\n",
      "Class =  11   Qty =       98   Percentage = 0.38 %\n",
      "Class =  23   Qty =       96   Percentage = 0.37 %\n",
      "Class =   7   Qty =       94   Percentage = 0.37 %\n",
      "Class =  14   Qty =       77   Percentage = 0.30 %\n",
      "Class =  29   Qty =       77   Percentage = 0.30 %\n",
      "Class =  30   Qty =       64   Percentage = 0.25 %\n",
      "Class =  15   Qty =       62   Percentage = 0.24 %\n",
      "Class =   5   Qty =       44   Percentage = 0.17 %\n",
      "Class =  28   Qty =       35   Percentage = 0.14 %\n",
      "Class =  27   Qty =       28   Percentage = 0.11 %\n",
      "Class =  18   Qty =       22   Percentage = 0.09 %\n",
      "Class =  13   Qty =       20   Percentage = 0.08 %\n",
      "Class =   8   Qty =       12   Percentage = 0.05 %\n",
      "Class =  10   Qty =        9   Percentage = 0.03 %\n",
      "Class =  12   Qty =        8   Percentage = 0.03 %\n",
      "Class =  22   Qty =        7   Percentage = 0.03 %\n",
      "Class =  31   Qty =        7   Percentage = 0.03 %\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1248</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)        │            <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1248</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)        │            <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1246</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)        │           <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1246</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)        │            <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">623</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">619</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)        │           <span style=\"color: #00af00; text-decoration-color: #00af00\">656</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_2           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">619</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)        │            <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">615</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)        │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,296</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_3           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">615</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)        │            <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">307</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4912</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">628,864</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_4           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,128</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv1d (\u001b[38;5;33mConv1D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1248\u001b[0m, \u001b[38;5;34m8\u001b[0m)        │            \u001b[38;5;34m32\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1248\u001b[0m, \u001b[38;5;34m8\u001b[0m)        │            \u001b[38;5;34m32\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_1 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1246\u001b[0m, \u001b[38;5;34m8\u001b[0m)        │           \u001b[38;5;34m200\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_1           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1246\u001b[0m, \u001b[38;5;34m8\u001b[0m)        │            \u001b[38;5;34m32\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d (\u001b[38;5;33mMaxPooling1D\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m623\u001b[0m, \u001b[38;5;34m8\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_2 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m619\u001b[0m, \u001b[38;5;34m16\u001b[0m)        │           \u001b[38;5;34m656\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_2           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m619\u001b[0m, \u001b[38;5;34m16\u001b[0m)        │            \u001b[38;5;34m64\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_3 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m615\u001b[0m, \u001b[38;5;34m16\u001b[0m)        │         \u001b[38;5;34m1,296\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_3           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m615\u001b[0m, \u001b[38;5;34m16\u001b[0m)        │            \u001b[38;5;34m64\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_1 (\u001b[38;5;33mMaxPooling1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m307\u001b[0m, \u001b[38;5;34m16\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4912\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │       \u001b[38;5;34m628,864\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_4           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │           \u001b[38;5;34m512\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │         \u001b[38;5;34m4,128\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">635,880</span> (2.43 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m635,880\u001b[0m (2.43 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">635,528</span> (2.42 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m635,528\u001b[0m (2.42 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">352</span> (1.38 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m352\u001b[0m (1.38 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting training...\n",
      "\n",
      "Training fold 1\n",
      "\n",
      "Generating upsampling using SMOTE...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\DeveloperTools\\python\\3.11.0\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:737: UserWarning: The least populated class in y has only 7 members, which is less than n_splits=10.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200000 samples after upsampling.\n",
      "Class distribution for training after upsampling: Counter({np.int64(0): 6250, np.int64(1): 6250, np.int64(2): 6250, np.int64(3): 6250, np.int64(4): 6250, np.int64(5): 6250, np.int64(6): 6250, np.int64(7): 6250, np.int64(8): 6250, np.int64(9): 6250, np.int64(10): 6250, np.int64(11): 6250, np.int64(12): 6250, np.int64(13): 6250, np.int64(14): 6250, np.int64(15): 6250, np.int64(16): 6250, np.int64(17): 6250, np.int64(18): 6250, np.int64(19): 6250, np.int64(20): 6250, np.int64(21): 6250, np.int64(22): 6250, np.int64(23): 6250, np.int64(24): 6250, np.int64(25): 6250, np.int64(26): 6250, np.int64(27): 6250, np.int64(28): 6250, np.int64(29): 6250, np.int64(30): 6250, np.int64(31): 6250})\n",
      "Finishing upsampling.\n",
      "\n",
      "Epoch 1/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 17ms/step - accuracy: 0.8505 - loss: 0.5826 - val_accuracy: 1.0000 - val_loss: 0.0010\n",
      "Epoch 2/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m101s\u001b[0m 16ms/step - accuracy: 0.9714 - loss: 0.0915 - val_accuracy: 1.0000 - val_loss: 3.2618e-04\n",
      "Epoch 3/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m103s\u001b[0m 17ms/step - accuracy: 0.9860 - loss: 0.0451 - val_accuracy: 1.0000 - val_loss: 8.0121e-04\n",
      "Epoch 4/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 17ms/step - accuracy: 0.9904 - loss: 0.0301 - val_accuracy: 1.0000 - val_loss: 9.3341e-08\n",
      "Epoch 5/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 18ms/step - accuracy: 0.9928 - loss: 0.0224 - val_accuracy: 1.0000 - val_loss: 1.1620e-06\n",
      "Epoch 6/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m111s\u001b[0m 18ms/step - accuracy: 0.9944 - loss: 0.0171 - val_accuracy: 1.0000 - val_loss: 1.0324e-04\n",
      "Epoch 7/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m116s\u001b[0m 19ms/step - accuracy: 0.9949 - loss: 0.0165 - val_accuracy: 1.0000 - val_loss: 2.1935e-08\n",
      "Epoch 8/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 17ms/step - accuracy: 0.9960 - loss: 0.0129 - val_accuracy: 1.0000 - val_loss: 5.5373e-08\n",
      "Epoch 9/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 18ms/step - accuracy: 0.9959 - loss: 0.0121 - val_accuracy: 1.0000 - val_loss: 1.0228e-07\n",
      "Epoch 10/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m112s\u001b[0m 18ms/step - accuracy: 0.9968 - loss: 0.0098 - val_accuracy: 1.0000 - val_loss: 1.2309e-06\n",
      "Epoch 11/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m115s\u001b[0m 19ms/step - accuracy: 0.9971 - loss: 0.0094 - val_accuracy: 1.0000 - val_loss: 2.4199e-08\n",
      "Epoch 12/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 17ms/step - accuracy: 0.9969 - loss: 0.0101 - val_accuracy: 1.0000 - val_loss: 4.1702e-05\n",
      "Epoch 13/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 18ms/step - accuracy: 0.9971 - loss: 0.0090 - val_accuracy: 1.0000 - val_loss: 8.8811e-09\n",
      "Epoch 14/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m113s\u001b[0m 18ms/step - accuracy: 0.9977 - loss: 0.0080 - val_accuracy: 1.0000 - val_loss: 2.8014e-09\n",
      "Epoch 15/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m119s\u001b[0m 19ms/step - accuracy: 0.9977 - loss: 0.0079 - val_accuracy: 1.0000 - val_loss: 1.6749e-08\n",
      "Epoch 16/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m114s\u001b[0m 18ms/step - accuracy: 0.9979 - loss: 0.0068 - val_accuracy: 1.0000 - val_loss: 4.7088e-09\n",
      "Epoch 17/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m111s\u001b[0m 18ms/step - accuracy: 0.9980 - loss: 0.0075 - val_accuracy: 1.0000 - val_loss: 6.8247e-08\n",
      "Epoch 18/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m115s\u001b[0m 19ms/step - accuracy: 0.9981 - loss: 0.0064 - val_accuracy: 1.0000 - val_loss: 9.6857e-08\n",
      "Epoch 19/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m121s\u001b[0m 19ms/step - accuracy: 0.9982 - loss: 0.0059 - val_accuracy: 1.0000 - val_loss: 4.3004e-04\n",
      "Epoch 20/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m120s\u001b[0m 19ms/step - accuracy: 0.9982 - loss: 0.0060 - val_accuracy: 1.0000 - val_loss: 7.6428e-06\n",
      "Epoch 21/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m112s\u001b[0m 18ms/step - accuracy: 0.9984 - loss: 0.0057 - val_accuracy: 1.0000 - val_loss: 1.1921e-10\n",
      "Epoch 22/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m117s\u001b[0m 19ms/step - accuracy: 0.9984 - loss: 0.0059 - val_accuracy: 1.0000 - val_loss: 5.4836e-09\n",
      "Epoch 23/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m122s\u001b[0m 20ms/step - accuracy: 0.9984 - loss: 0.0053 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 24/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m112s\u001b[0m 18ms/step - accuracy: 0.9984 - loss: 0.0055 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 25/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m114s\u001b[0m 18ms/step - accuracy: 0.9985 - loss: 0.0051 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 26/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m118s\u001b[0m 19ms/step - accuracy: 0.9984 - loss: 0.0051 - val_accuracy: 1.0000 - val_loss: 1.8895e-08\n",
      "Epoch 27/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m122s\u001b[0m 20ms/step - accuracy: 0.9986 - loss: 0.0043 - val_accuracy: 1.0000 - val_loss: 5.9605e-11\n",
      "Epoch 28/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m111s\u001b[0m 18ms/step - accuracy: 0.9985 - loss: 0.0046 - val_accuracy: 1.0000 - val_loss: 5.4598e-08\n",
      "Epoch 29/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m115s\u001b[0m 19ms/step - accuracy: 0.9986 - loss: 0.0043 - val_accuracy: 1.0000 - val_loss: 5.4359e-08\n",
      "Epoch 30/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m118s\u001b[0m 19ms/step - accuracy: 0.9988 - loss: 0.0038 - val_accuracy: 1.0000 - val_loss: 1.7881e-09\n",
      "Epoch 31/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m126s\u001b[0m 20ms/step - accuracy: 0.9988 - loss: 0.0038 - val_accuracy: 1.0000 - val_loss: 1.4305e-09\n",
      "Epoch 32/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m117s\u001b[0m 19ms/step - accuracy: 0.9987 - loss: 0.0045 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 33/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m117s\u001b[0m 19ms/step - accuracy: 0.9986 - loss: 0.0049 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 33: early stopping\n",
      "Restoring model weights from the end of the best epoch: 23.\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step\n",
      "\n",
      "Training fold 2\n",
      "\n",
      "Generating upsampling using SMOTE...\n",
      "200000 samples after upsampling.\n",
      "Class distribution for training after upsampling: Counter({np.int64(0): 6250, np.int64(1): 6250, np.int64(2): 6250, np.int64(3): 6250, np.int64(4): 6250, np.int64(5): 6250, np.int64(6): 6250, np.int64(7): 6250, np.int64(8): 6250, np.int64(9): 6250, np.int64(10): 6250, np.int64(11): 6250, np.int64(12): 6250, np.int64(13): 6250, np.int64(14): 6250, np.int64(15): 6250, np.int64(16): 6250, np.int64(17): 6250, np.int64(18): 6250, np.int64(19): 6250, np.int64(20): 6250, np.int64(21): 6250, np.int64(22): 6250, np.int64(23): 6250, np.int64(24): 6250, np.int64(25): 6250, np.int64(26): 6250, np.int64(27): 6250, np.int64(28): 6250, np.int64(29): 6250, np.int64(30): 6250, np.int64(31): 6250})\n",
      "Finishing upsampling.\n",
      "\n",
      "Epoch 1/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m123s\u001b[0m 20ms/step - accuracy: 0.9699 - loss: 0.1845 - val_accuracy: 1.0000 - val_loss: 6.4552e-08\n",
      "Epoch 2/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m126s\u001b[0m 20ms/step - accuracy: 0.9944 - loss: 0.0178 - val_accuracy: 1.0000 - val_loss: 5.9605e-10\n",
      "Epoch 3/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m123s\u001b[0m 20ms/step - accuracy: 0.9971 - loss: 0.0097 - val_accuracy: 1.0000 - val_loss: 4.5341e-07\n",
      "Epoch 4/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m118s\u001b[0m 19ms/step - accuracy: 0.9980 - loss: 0.0069 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 5/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m123s\u001b[0m 20ms/step - accuracy: 0.9982 - loss: 0.0062 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 6/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m127s\u001b[0m 21ms/step - accuracy: 0.9984 - loss: 0.0061 - val_accuracy: 1.0000 - val_loss: 5.3644e-10\n",
      "Epoch 7/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m124s\u001b[0m 20ms/step - accuracy: 0.9985 - loss: 0.0051 - val_accuracy: 1.0000 - val_loss: 6.6161e-09\n",
      "Epoch 8/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m120s\u001b[0m 19ms/step - accuracy: 0.9986 - loss: 0.0048 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 9/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m124s\u001b[0m 20ms/step - accuracy: 0.9981 - loss: 0.0059 - val_accuracy: 1.0000 - val_loss: 1.2875e-08\n",
      "Epoch 10/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m133s\u001b[0m 22ms/step - accuracy: 0.9986 - loss: 0.0059 - val_accuracy: 1.0000 - val_loss: 1.7524e-08\n",
      "Epoch 11/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m124s\u001b[0m 20ms/step - accuracy: 0.9987 - loss: 0.0046 - val_accuracy: 1.0000 - val_loss: 5.2571e-08\n",
      "Epoch 12/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m122s\u001b[0m 20ms/step - accuracy: 0.9989 - loss: 0.0040 - val_accuracy: 1.0000 - val_loss: 2.3842e-10\n",
      "Epoch 13/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m130s\u001b[0m 21ms/step - accuracy: 0.9987 - loss: 0.0047 - val_accuracy: 1.0000 - val_loss: 1.8547e-06\n",
      "Epoch 14/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m130s\u001b[0m 21ms/step - accuracy: 0.9988 - loss: 0.0048 - val_accuracy: 1.0000 - val_loss: 5.7220e-09\n",
      "Epoch 14: early stopping\n",
      "Restoring model weights from the end of the best epoch: 4.\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\n",
      "Training fold 3\n",
      "\n",
      "Generating upsampling using SMOTE...\n",
      "200000 samples after upsampling.\n",
      "Class distribution for training after upsampling: Counter({np.int64(0): 6250, np.int64(1): 6250, np.int64(2): 6250, np.int64(3): 6250, np.int64(4): 6250, np.int64(5): 6250, np.int64(6): 6250, np.int64(7): 6250, np.int64(8): 6250, np.int64(9): 6250, np.int64(10): 6250, np.int64(11): 6250, np.int64(12): 6250, np.int64(13): 6250, np.int64(14): 6250, np.int64(15): 6250, np.int64(16): 6250, np.int64(17): 6250, np.int64(18): 6250, np.int64(19): 6250, np.int64(20): 6250, np.int64(21): 6250, np.int64(22): 6250, np.int64(23): 6250, np.int64(24): 6250, np.int64(25): 6250, np.int64(26): 6250, np.int64(27): 6250, np.int64(28): 6250, np.int64(29): 6250, np.int64(30): 6250, np.int64(31): 6250})\n",
      "Finishing upsampling.\n",
      "\n",
      "Epoch 1/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m121s\u001b[0m 20ms/step - accuracy: 0.9902 - loss: 0.0360 - val_accuracy: 1.0000 - val_loss: 1.1623e-08\n",
      "Epoch 2/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m123s\u001b[0m 20ms/step - accuracy: 0.9973 - loss: 0.0098 - val_accuracy: 1.0000 - val_loss: 1.9073e-09\n",
      "Epoch 3/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m127s\u001b[0m 21ms/step - accuracy: 0.9979 - loss: 0.0079 - val_accuracy: 1.0000 - val_loss: 8.2254e-09\n",
      "Epoch 4/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m132s\u001b[0m 21ms/step - accuracy: 0.9983 - loss: 0.0055 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 5/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m134s\u001b[0m 22ms/step - accuracy: 0.9986 - loss: 0.0062 - val_accuracy: 1.0000 - val_loss: 1.0729e-09\n",
      "Epoch 6/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m123s\u001b[0m 20ms/step - accuracy: 0.9985 - loss: 0.0050 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 7/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m130s\u001b[0m 21ms/step - accuracy: 0.9988 - loss: 0.0046 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 8/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m137s\u001b[0m 22ms/step - accuracy: 0.9990 - loss: 0.0043 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 9/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m123s\u001b[0m 20ms/step - accuracy: 0.9988 - loss: 0.0041 - val_accuracy: 1.0000 - val_loss: 2.2590e-08\n",
      "Epoch 10/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m127s\u001b[0m 21ms/step - accuracy: 0.9986 - loss: 0.0050 - val_accuracy: 1.0000 - val_loss: 5.9605e-11\n",
      "Epoch 11/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m134s\u001b[0m 22ms/step - accuracy: 0.9988 - loss: 0.0046 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 12/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m127s\u001b[0m 21ms/step - accuracy: 0.9989 - loss: 0.0041 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 13/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m125s\u001b[0m 20ms/step - accuracy: 0.9989 - loss: 0.0045 - val_accuracy: 1.0000 - val_loss: 2.3842e-10\n",
      "Epoch 14/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m132s\u001b[0m 21ms/step - accuracy: 0.9986 - loss: 0.0051 - val_accuracy: 1.0000 - val_loss: 7.1526e-10\n",
      "Epoch 14: early stopping\n",
      "Restoring model weights from the end of the best epoch: 4.\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step\n",
      "\n",
      "Training fold 4\n",
      "\n",
      "Generating upsampling using SMOTE...\n",
      "200000 samples after upsampling.\n",
      "Class distribution for training after upsampling: Counter({np.int64(0): 6250, np.int64(1): 6250, np.int64(2): 6250, np.int64(3): 6250, np.int64(4): 6250, np.int64(5): 6250, np.int64(6): 6250, np.int64(7): 6250, np.int64(8): 6250, np.int64(9): 6250, np.int64(10): 6250, np.int64(11): 6250, np.int64(12): 6250, np.int64(13): 6250, np.int64(14): 6250, np.int64(15): 6250, np.int64(16): 6250, np.int64(17): 6250, np.int64(18): 6250, np.int64(19): 6250, np.int64(20): 6250, np.int64(21): 6250, np.int64(22): 6250, np.int64(23): 6250, np.int64(24): 6250, np.int64(25): 6250, np.int64(26): 6250, np.int64(27): 6250, np.int64(28): 6250, np.int64(29): 6250, np.int64(30): 6250, np.int64(31): 6250})\n",
      "Finishing upsampling.\n",
      "\n",
      "Epoch 1/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m137s\u001b[0m 22ms/step - accuracy: 0.9942 - loss: 0.0208 - val_accuracy: 1.0000 - val_loss: 1.1086e-08\n",
      "Epoch 2/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m129s\u001b[0m 21ms/step - accuracy: 0.9980 - loss: 0.0075 - val_accuracy: 1.0000 - val_loss: 1.8239e-08\n",
      "Epoch 3/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m125s\u001b[0m 20ms/step - accuracy: 0.9985 - loss: 0.0052 - val_accuracy: 1.0000 - val_loss: 9.5367e-10\n",
      "Epoch 4/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m131s\u001b[0m 21ms/step - accuracy: 0.9987 - loss: 0.0050 - val_accuracy: 1.0000 - val_loss: 4.1723e-10\n",
      "Epoch 5/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 23ms/step - accuracy: 0.9988 - loss: 0.0049 - val_accuracy: 1.0000 - val_loss: 1.1921e-10\n",
      "Epoch 6/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m133s\u001b[0m 22ms/step - accuracy: 0.9986 - loss: 0.0044 - val_accuracy: 1.0000 - val_loss: 8.9407e-10\n",
      "Epoch 7/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m130s\u001b[0m 21ms/step - accuracy: 0.9988 - loss: 0.0039 - val_accuracy: 1.0000 - val_loss: 2.0206e-08\n",
      "Epoch 8/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m138s\u001b[0m 22ms/step - accuracy: 0.9988 - loss: 0.0039 - val_accuracy: 1.0000 - val_loss: 2.9802e-10\n",
      "Epoch 9/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m136s\u001b[0m 22ms/step - accuracy: 0.9989 - loss: 0.0039 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 10/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m129s\u001b[0m 21ms/step - accuracy: 0.9989 - loss: 0.0045 - val_accuracy: 1.0000 - val_loss: 1.1159e-06\n",
      "Epoch 11/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m136s\u001b[0m 22ms/step - accuracy: 0.9991 - loss: 0.0031 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 12/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m145s\u001b[0m 23ms/step - accuracy: 0.9992 - loss: 0.0033 - val_accuracy: 1.0000 - val_loss: 1.9670e-09\n",
      "Epoch 13/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m135s\u001b[0m 22ms/step - accuracy: 0.9990 - loss: 0.0037 - val_accuracy: 1.0000 - val_loss: 2.9802e-10\n",
      "Epoch 14/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m135s\u001b[0m 22ms/step - accuracy: 0.9991 - loss: 0.0037 - val_accuracy: 1.0000 - val_loss: 5.8472e-08\n",
      "Epoch 15/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 23ms/step - accuracy: 0.9990 - loss: 0.0039 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 16/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 23ms/step - accuracy: 0.9992 - loss: 0.0033 - val_accuracy: 1.0000 - val_loss: 5.9605e-11\n",
      "Epoch 17/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m132s\u001b[0m 21ms/step - accuracy: 0.9992 - loss: 0.0028 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 18/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 23ms/step - accuracy: 0.9989 - loss: 0.0037 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 19/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m155s\u001b[0m 25ms/step - accuracy: 0.9992 - loss: 0.0031 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 19: early stopping\n",
      "Restoring model weights from the end of the best epoch: 9.\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "\n",
      "Training fold 5\n",
      "\n",
      "Generating upsampling using SMOTE...\n",
      "200000 samples after upsampling.\n",
      "Class distribution for training after upsampling: Counter({np.int64(0): 6250, np.int64(1): 6250, np.int64(2): 6250, np.int64(3): 6250, np.int64(4): 6250, np.int64(5): 6250, np.int64(6): 6250, np.int64(7): 6250, np.int64(8): 6250, np.int64(9): 6250, np.int64(10): 6250, np.int64(11): 6250, np.int64(12): 6250, np.int64(13): 6250, np.int64(14): 6250, np.int64(15): 6250, np.int64(16): 6250, np.int64(17): 6250, np.int64(18): 6250, np.int64(19): 6250, np.int64(20): 6250, np.int64(21): 6250, np.int64(22): 6250, np.int64(23): 6250, np.int64(24): 6250, np.int64(25): 6250, np.int64(26): 6250, np.int64(27): 6250, np.int64(28): 6250, np.int64(29): 6250, np.int64(30): 6250, np.int64(31): 6250})\n",
      "Finishing upsampling.\n",
      "\n",
      "Epoch 1/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m145s\u001b[0m 23ms/step - accuracy: 0.9937 - loss: 0.0233 - val_accuracy: 1.0000 - val_loss: 5.9605e-10\n",
      "Epoch 2/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m133s\u001b[0m 21ms/step - accuracy: 0.9980 - loss: 0.0069 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 3/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m133s\u001b[0m 22ms/step - accuracy: 0.9982 - loss: 0.0056 - val_accuracy: 1.0000 - val_loss: 1.2755e-08\n",
      "Epoch 4/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 23ms/step - accuracy: 0.9986 - loss: 0.0054 - val_accuracy: 1.0000 - val_loss: 3.8741e-05\n",
      "Epoch 5/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m149s\u001b[0m 24ms/step - accuracy: 0.9988 - loss: 0.0042 - val_accuracy: 1.0000 - val_loss: 2.3842e-10\n",
      "Epoch 6/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m146s\u001b[0m 24ms/step - accuracy: 0.9989 - loss: 0.0047 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 7/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m138s\u001b[0m 22ms/step - accuracy: 0.9988 - loss: 0.0037 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 8/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m147s\u001b[0m 24ms/step - accuracy: 0.9992 - loss: 0.0032 - val_accuracy: 1.0000 - val_loss: 7.2002e-08\n",
      "Epoch 9/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m149s\u001b[0m 24ms/step - accuracy: 0.9992 - loss: 0.0041 - val_accuracy: 1.0000 - val_loss: 3.3379e-09\n",
      "Epoch 10/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 23ms/step - accuracy: 0.9990 - loss: 0.0035 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 11/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m148s\u001b[0m 24ms/step - accuracy: 0.9989 - loss: 0.0039 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 12/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m152s\u001b[0m 25ms/step - accuracy: 0.9992 - loss: 0.0030 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 12: early stopping\n",
      "Restoring model weights from the end of the best epoch: 2.\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step\n",
      "\n",
      "Training fold 6\n",
      "\n",
      "Generating upsampling using SMOTE...\n",
      "200000 samples after upsampling.\n",
      "Class distribution for training after upsampling: Counter({np.int64(0): 6250, np.int64(1): 6250, np.int64(2): 6250, np.int64(3): 6250, np.int64(4): 6250, np.int64(5): 6250, np.int64(6): 6250, np.int64(7): 6250, np.int64(8): 6250, np.int64(9): 6250, np.int64(10): 6250, np.int64(11): 6250, np.int64(12): 6250, np.int64(13): 6250, np.int64(14): 6250, np.int64(15): 6250, np.int64(16): 6250, np.int64(17): 6250, np.int64(18): 6250, np.int64(19): 6250, np.int64(20): 6250, np.int64(21): 6250, np.int64(22): 6250, np.int64(23): 6250, np.int64(24): 6250, np.int64(25): 6250, np.int64(26): 6250, np.int64(27): 6250, np.int64(28): 6250, np.int64(29): 6250, np.int64(30): 6250, np.int64(31): 6250})\n",
      "Finishing upsampling.\n",
      "\n",
      "Epoch 1/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m139s\u001b[0m 22ms/step - accuracy: 0.9961 - loss: 0.0132 - val_accuracy: 1.0000 - val_loss: 6.9618e-08\n",
      "Epoch 2/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 23ms/step - accuracy: 0.9985 - loss: 0.0051 - val_accuracy: 1.0000 - val_loss: 5.5193e-08\n",
      "Epoch 3/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m152s\u001b[0m 25ms/step - accuracy: 0.9989 - loss: 0.0043 - val_accuracy: 1.0000 - val_loss: 2.8014e-09\n",
      "Epoch 4/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m164s\u001b[0m 27ms/step - accuracy: 0.9990 - loss: 0.0039 - val_accuracy: 1.0000 - val_loss: 3.1352e-08\n",
      "Epoch 5/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 23ms/step - accuracy: 0.9986 - loss: 0.0046 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 6/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m153s\u001b[0m 25ms/step - accuracy: 0.9990 - loss: 0.0040 - val_accuracy: 1.0000 - val_loss: 2.2173e-08\n",
      "Epoch 7/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 23ms/step - accuracy: 0.9988 - loss: 0.0046 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 8/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m191s\u001b[0m 21ms/step - accuracy: 0.9990 - loss: 0.0036 - val_accuracy: 1.0000 - val_loss: 1.1292e-05\n",
      "Epoch 9/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m134s\u001b[0m 22ms/step - accuracy: 0.9989 - loss: 0.0036 - val_accuracy: 1.0000 - val_loss: 2.1994e-08\n",
      "Epoch 10/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m139s\u001b[0m 22ms/step - accuracy: 0.9992 - loss: 0.0030 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 11/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m145s\u001b[0m 24ms/step - accuracy: 0.9990 - loss: 0.0038 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 12/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m158s\u001b[0m 25ms/step - accuracy: 0.9991 - loss: 0.0035 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 13/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m157s\u001b[0m 25ms/step - accuracy: 0.9991 - loss: 0.0031 - val_accuracy: 1.0000 - val_loss: 1.2129e-07\n",
      "Epoch 14/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m151s\u001b[0m 24ms/step - accuracy: 0.9992 - loss: 0.0030 - val_accuracy: 1.0000 - val_loss: 2.2292e-08\n",
      "Epoch 15/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m165s\u001b[0m 27ms/step - accuracy: 0.9992 - loss: 0.0027 - val_accuracy: 1.0000 - val_loss: 1.1921e-10\n",
      "Epoch 15: early stopping\n",
      "Restoring model weights from the end of the best epoch: 5.\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step\n",
      "\n",
      "Training fold 7\n",
      "\n",
      "Generating upsampling using SMOTE...\n",
      "200000 samples after upsampling.\n",
      "Class distribution for training after upsampling: Counter({np.int64(0): 6250, np.int64(1): 6250, np.int64(2): 6250, np.int64(3): 6250, np.int64(4): 6250, np.int64(5): 6250, np.int64(6): 6250, np.int64(7): 6250, np.int64(8): 6250, np.int64(9): 6250, np.int64(10): 6250, np.int64(11): 6250, np.int64(12): 6250, np.int64(13): 6250, np.int64(14): 6250, np.int64(15): 6250, np.int64(16): 6250, np.int64(17): 6250, np.int64(18): 6250, np.int64(19): 6250, np.int64(20): 6250, np.int64(21): 6250, np.int64(22): 6250, np.int64(23): 6250, np.int64(24): 6250, np.int64(25): 6250, np.int64(26): 6250, np.int64(27): 6250, np.int64(28): 6250, np.int64(29): 6250, np.int64(30): 6250, np.int64(31): 6250})\n",
      "Finishing upsampling.\n",
      "\n",
      "Epoch 1/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m161s\u001b[0m 26ms/step - accuracy: 0.9965 - loss: 0.0123 - val_accuracy: 1.0000 - val_loss: 5.9605e-10\n",
      "Epoch 2/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m168s\u001b[0m 27ms/step - accuracy: 0.9984 - loss: 0.0051 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 3/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m184s\u001b[0m 30ms/step - accuracy: 0.9989 - loss: 0.0039 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 4/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m194s\u001b[0m 31ms/step - accuracy: 0.9989 - loss: 0.0035 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 5/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m174s\u001b[0m 28ms/step - accuracy: 0.9991 - loss: 0.0029 - val_accuracy: 1.0000 - val_loss: 2.1398e-07\n",
      "Epoch 6/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m192s\u001b[0m 31ms/step - accuracy: 0.9990 - loss: 0.0041 - val_accuracy: 1.0000 - val_loss: 1.0133e-09\n",
      "Epoch 7/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m175s\u001b[0m 28ms/step - accuracy: 0.9990 - loss: 0.0034 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 8/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m194s\u001b[0m 31ms/step - accuracy: 0.9991 - loss: 0.0035 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 9/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m201s\u001b[0m 32ms/step - accuracy: 0.9992 - loss: 0.0032 - val_accuracy: 1.0000 - val_loss: 1.6689e-09\n",
      "Epoch 10/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m188s\u001b[0m 30ms/step - accuracy: 0.9990 - loss: 0.0036 - val_accuracy: 1.0000 - val_loss: 1.1921e-09\n",
      "Epoch 11/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m161s\u001b[0m 26ms/step - accuracy: 0.9992 - loss: 0.0028 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 12/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m172s\u001b[0m 28ms/step - accuracy: 0.9991 - loss: 0.0038 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 12: early stopping\n",
      "Restoring model weights from the end of the best epoch: 2.\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step\n",
      "\n",
      "Training fold 8\n",
      "\n",
      "Generating upsampling using SMOTE...\n",
      "200000 samples after upsampling.\n",
      "Class distribution for training after upsampling: Counter({np.int64(0): 6250, np.int64(1): 6250, np.int64(2): 6250, np.int64(3): 6250, np.int64(4): 6250, np.int64(5): 6250, np.int64(6): 6250, np.int64(7): 6250, np.int64(8): 6250, np.int64(9): 6250, np.int64(10): 6250, np.int64(11): 6250, np.int64(12): 6250, np.int64(13): 6250, np.int64(14): 6250, np.int64(15): 6250, np.int64(16): 6250, np.int64(17): 6250, np.int64(18): 6250, np.int64(19): 6250, np.int64(20): 6250, np.int64(21): 6250, np.int64(22): 6250, np.int64(23): 6250, np.int64(24): 6250, np.int64(25): 6250, np.int64(26): 6250, np.int64(27): 6250, np.int64(28): 6250, np.int64(29): 6250, np.int64(30): 6250, np.int64(31): 6250})\n",
      "Finishing upsampling.\n",
      "\n",
      "Epoch 1/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m201s\u001b[0m 33ms/step - accuracy: 0.9969 - loss: 0.0111 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 2/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m189s\u001b[0m 31ms/step - accuracy: 0.9987 - loss: 0.0042 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 3/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m201s\u001b[0m 32ms/step - accuracy: 0.9989 - loss: 0.0048 - val_accuracy: 1.0000 - val_loss: 5.9605e-09\n",
      "Epoch 4/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m188s\u001b[0m 30ms/step - accuracy: 0.9989 - loss: 0.0039 - val_accuracy: 1.0000 - val_loss: 7.1526e-10\n",
      "Epoch 5/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m202s\u001b[0m 33ms/step - accuracy: 0.9990 - loss: 0.0045 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 6/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m188s\u001b[0m 30ms/step - accuracy: 0.9991 - loss: 0.0032 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 7/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m197s\u001b[0m 32ms/step - accuracy: 0.9991 - loss: 0.0036 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 8/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m197s\u001b[0m 32ms/step - accuracy: 0.9993 - loss: 0.0026 - val_accuracy: 1.0000 - val_loss: 1.5914e-08\n",
      "Epoch 9/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m198s\u001b[0m 32ms/step - accuracy: 0.9991 - loss: 0.0036 - val_accuracy: 1.0000 - val_loss: 6.5565e-10\n",
      "Epoch 10/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m203s\u001b[0m 33ms/step - accuracy: 0.9992 - loss: 0.0030 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 11/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m201s\u001b[0m 32ms/step - accuracy: 0.9991 - loss: 0.0038 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 11: early stopping\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step\n",
      "\n",
      "Training fold 9\n",
      "\n",
      "Generating upsampling using SMOTE...\n",
      "200000 samples after upsampling.\n",
      "Class distribution for training after upsampling: Counter({np.int64(0): 6250, np.int64(1): 6250, np.int64(2): 6250, np.int64(3): 6250, np.int64(4): 6250, np.int64(5): 6250, np.int64(6): 6250, np.int64(7): 6250, np.int64(8): 6250, np.int64(9): 6250, np.int64(10): 6250, np.int64(11): 6250, np.int64(12): 6250, np.int64(13): 6250, np.int64(14): 6250, np.int64(15): 6250, np.int64(16): 6250, np.int64(17): 6250, np.int64(18): 6250, np.int64(19): 6250, np.int64(20): 6250, np.int64(21): 6250, np.int64(22): 6250, np.int64(23): 6250, np.int64(24): 6250, np.int64(25): 6250, np.int64(26): 6250, np.int64(27): 6250, np.int64(28): 6250, np.int64(29): 6250, np.int64(30): 6250, np.int64(31): 6250})\n",
      "Finishing upsampling.\n",
      "\n",
      "Epoch 1/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m205s\u001b[0m 33ms/step - accuracy: 0.9978 - loss: 0.0079 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 2/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m216s\u001b[0m 35ms/step - accuracy: 0.9988 - loss: 0.0045 - val_accuracy: 1.0000 - val_loss: 3.6955e-09\n",
      "Epoch 3/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m198s\u001b[0m 32ms/step - accuracy: 0.9989 - loss: 0.0038 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 4/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m220s\u001b[0m 36ms/step - accuracy: 0.9991 - loss: 0.0035 - val_accuracy: 1.0000 - val_loss: 2.3478e-07\n",
      "Epoch 5/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m197s\u001b[0m 32ms/step - accuracy: 0.9991 - loss: 0.0037 - val_accuracy: 1.0000 - val_loss: 1.7881e-10\n",
      "Epoch 6/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m215s\u001b[0m 35ms/step - accuracy: 0.9992 - loss: 0.0027 - val_accuracy: 1.0000 - val_loss: 8.3446e-10\n",
      "Epoch 7/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m208s\u001b[0m 34ms/step - accuracy: 0.9990 - loss: 0.0040 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 8/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m207s\u001b[0m 33ms/step - accuracy: 0.9992 - loss: 0.0026 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 9/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m218s\u001b[0m 35ms/step - accuracy: 0.9990 - loss: 0.0029 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 10/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m201s\u001b[0m 32ms/step - accuracy: 0.9992 - loss: 0.0035 - val_accuracy: 1.0000 - val_loss: 8.9407e-10\n",
      "Epoch 11/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m220s\u001b[0m 36ms/step - accuracy: 0.9991 - loss: 0.0041 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 11: early stopping\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step\n",
      "\n",
      "Training fold 10\n",
      "\n",
      "Generating upsampling using SMOTE...\n",
      "200000 samples after upsampling.\n",
      "Class distribution for training after upsampling: Counter({np.int64(0): 6250, np.int64(1): 6250, np.int64(2): 6250, np.int64(3): 6250, np.int64(4): 6250, np.int64(5): 6250, np.int64(6): 6250, np.int64(7): 6250, np.int64(8): 6250, np.int64(9): 6250, np.int64(10): 6250, np.int64(11): 6250, np.int64(12): 6250, np.int64(13): 6250, np.int64(14): 6250, np.int64(15): 6250, np.int64(16): 6250, np.int64(17): 6250, np.int64(18): 6250, np.int64(19): 6250, np.int64(20): 6250, np.int64(21): 6250, np.int64(22): 6250, np.int64(23): 6250, np.int64(24): 6250, np.int64(25): 6250, np.int64(26): 6250, np.int64(27): 6250, np.int64(28): 6250, np.int64(29): 6250, np.int64(30): 6250, np.int64(31): 6250})\n",
      "Finishing upsampling.\n",
      "\n",
      "Epoch 1/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m196s\u001b[0m 32ms/step - accuracy: 0.9976 - loss: 0.0095 - val_accuracy: 1.0000 - val_loss: 1.7881e-10\n",
      "Epoch 2/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m203s\u001b[0m 33ms/step - accuracy: 0.9989 - loss: 0.0043 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 3/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m223s\u001b[0m 36ms/step - accuracy: 0.9989 - loss: 0.0039 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 4/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m202s\u001b[0m 33ms/step - accuracy: 0.9990 - loss: 0.0034 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 5/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m216s\u001b[0m 35ms/step - accuracy: 0.9991 - loss: 0.0026 - val_accuracy: 1.0000 - val_loss: 5.1975e-08\n",
      "Epoch 6/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m213s\u001b[0m 34ms/step - accuracy: 0.9991 - loss: 0.0030 - val_accuracy: 1.0000 - val_loss: 7.2896e-08\n",
      "Epoch 7/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m209s\u001b[0m 34ms/step - accuracy: 0.9991 - loss: 0.0031 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 8/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m217s\u001b[0m 35ms/step - accuracy: 0.9992 - loss: 0.0028 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 9/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m212s\u001b[0m 34ms/step - accuracy: 0.9992 - loss: 0.0029 - val_accuracy: 1.0000 - val_loss: 3.6299e-08\n",
      "Epoch 10/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m226s\u001b[0m 37ms/step - accuracy: 0.9992 - loss: 0.0028 - val_accuracy: 1.0000 - val_loss: 6.0797e-09\n",
      "Epoch 11/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m205s\u001b[0m 33ms/step - accuracy: 0.9991 - loss: 0.0032 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 12/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m224s\u001b[0m 36ms/step - accuracy: 0.9992 - loss: 0.0024 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 12: early stopping\n",
      "Restoring model weights from the end of the best epoch: 2.\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step\n",
      "\n",
      "Time taken for training:  07:19:26\n",
      "\n",
      "\n",
      "Fold 1 - Train Accuracy 0.9995 - Test Accuracy 0.5344\n",
      "Fold 2 - Train Accuracy 0.9995 - Test Accuracy 0.8800\n",
      "Fold 3 - Train Accuracy 0.9996 - Test Accuracy 0.9317\n",
      "Fold 4 - Train Accuracy 0.9998 - Test Accuracy 0.9138\n",
      "Fold 5 - Train Accuracy 0.9994 - Test Accuracy 0.9662\n",
      "Fold 6 - Train Accuracy 0.9997 - Test Accuracy 0.9674\n",
      "Fold 7 - Train Accuracy 0.9996 - Test Accuracy 0.9841\n",
      "Fold 8 - Train Accuracy 0.9997 - Test Accuracy 0.9922\n",
      "Fold 9 - Train Accuracy 0.9995 - Test Accuracy 0.9864\n",
      "Fold 10 - Train Accuracy 0.9997 - Test Accuracy 0.9848\n",
      "\n",
      "Mean Train Accuracy: 0.9996 - Std: 0.0001 \n",
      "Mean Test Accuracy: 0.9141 - Std: 0.1313 \n",
      "\n",
      "Evaluate other metrics:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.93      0.94     13905\n",
      "           1       0.94      0.97      0.95       723\n",
      "           2       0.94      0.98      0.96      2659\n",
      "           3       0.84      0.83      0.83      1123\n",
      "           4       0.86      0.88      0.87       384\n",
      "           5       0.88      0.84      0.86        44\n",
      "           6       0.92      0.90      0.91       663\n",
      "           7       0.91      0.90      0.91        94\n",
      "           8       0.83      0.83      0.83        12\n",
      "           9       0.87      0.87      0.87       786\n",
      "          10       0.67      0.89      0.76         9\n",
      "          11       0.88      0.86      0.87        98\n",
      "          12       0.50      0.88      0.64         8\n",
      "          13       0.90      0.90      0.90        20\n",
      "          14       0.93      0.86      0.89        77\n",
      "          15       0.92      0.97      0.94        62\n",
      "          16       0.83      0.87      0.85       917\n",
      "          17       0.92      0.97      0.95       473\n",
      "          18       0.95      0.86      0.90        22\n",
      "          19       0.94      0.85      0.89       122\n",
      "          20       0.90      0.86      0.88       111\n",
      "          21       0.87      0.87      0.87       201\n",
      "          22       0.50      1.00      0.67         7\n",
      "          23       0.86      0.86      0.86        96\n",
      "          24       0.82      0.85      0.84      1045\n",
      "          25       0.85      0.86      0.85       540\n",
      "          26       0.82      0.83      0.83      1334\n",
      "          27       0.96      0.86      0.91        28\n",
      "          28       0.97      0.86      0.91        35\n",
      "          29       0.92      0.87      0.89        77\n",
      "          30       0.94      0.91      0.92        64\n",
      "          31       0.71      0.71      0.71         7\n",
      "\n",
      "    accuracy                           0.91     25746\n",
      "   macro avg       0.86      0.88      0.86     25746\n",
      "weighted avg       0.91      0.91      0.91     25746\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Build a \"time window\" structure to handle the dataset as a time series.\n",
    "new_df = load_dataset(\"V1\")\n",
    "X_array, y_array = build_time_window_structure(new_df, \"V1\")\n",
    "X_array, y_array = remove_classes_with_less_samples(X_array, y_array)\n",
    "\n",
    "# Train the CNN model.\n",
    "v1_model = create_v1()\n",
    "v1_num_epochs = 300\n",
    "v1_batch_size = 32\n",
    "v1_validation_split = 0.01\n",
    "\n",
    "training_history_v1 = train_cnn_model(v1_model, X_array, y_array, v1_num_epochs, v1_batch_size, v1_validation_split, \"v1_model_V1.keras\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb5ce9b-c8da-4148-9c31-b214e86f06c9",
   "metadata": {},
   "source": [
    "#### V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02823a7a-2fe6-46ff-a389-9cda41b46cfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Start loading CSV file...\n",
      "Finish loading CSV file.\n",
      "\n",
      "Starting build_time_window_structure function...\n",
      "\n",
      "Shape of features:  (25770, 1250)\n",
      "Quantity os samples (labels):  25770\n",
      "\n",
      "Finishing build_time_window_structure function.\n",
      "\n",
      "Remove classes with less than 6 samples.\n",
      "\n",
      "Show classes identification:\n",
      "Class: 0 - Arrhythmia code: 1\n",
      "Class: 1 - Arrhythmia code: 21\n",
      "Class: 2 - Arrhythmia code: 22\n",
      "Class: 3 - Arrhythmia code: 23\n",
      "Class: 4 - Arrhythmia code: 30\n",
      "Class: 5 - Arrhythmia code: 36\n",
      "Class: 6 - Arrhythmia code: 50\n",
      "Class: 7 - Arrhythmia code: 51\n",
      "Class: 8 - Arrhythmia code: 54\n",
      "Class: 9 - Arrhythmia code: 60\n",
      "Class: 10 - Arrhythmia code: 80\n",
      "Class: 11 - Arrhythmia code: 82\n",
      "Class: 12 - Arrhythmia code: 83\n",
      "Class: 13 - Arrhythmia code: 88\n",
      "Class: 14 - Arrhythmia code: 101\n",
      "Class: 15 - Arrhythmia code: 104\n",
      "Class: 16 - Arrhythmia code: 105\n",
      "Class: 17 - Arrhythmia code: 106\n",
      "Class: 18 - Arrhythmia code: 108\n",
      "Class: 19 - Arrhythmia code: 120\n",
      "Class: 20 - Arrhythmia code: 121\n",
      "Class: 21 - Arrhythmia code: 125\n",
      "Class: 22 - Arrhythmia code: 140\n",
      "Class: 23 - Arrhythmia code: 142\n",
      "Class: 24 - Arrhythmia code: 145\n",
      "Class: 25 - Arrhythmia code: 146\n",
      "Class: 26 - Arrhythmia code: 147\n",
      "Class: 27 - Arrhythmia code: 155\n",
      "Class: 28 - Arrhythmia code: 160\n",
      "Class: 29 - Arrhythmia code: 161\n",
      "Class: 30 - Arrhythmia code: 165\n",
      "Class: 31 - Arrhythmia code: 166\n",
      "\n",
      "Check for dataset balance:\n",
      "Class =   0   Qty =    13905   Percentage = 54.01 %\n",
      "Class =   2   Qty =     2659   Percentage = 10.33 %\n",
      "Class =  26   Qty =     1334   Percentage = 5.18 %\n",
      "Class =   3   Qty =     1123   Percentage = 4.36 %\n",
      "Class =  24   Qty =     1045   Percentage = 4.06 %\n",
      "Class =  16   Qty =      917   Percentage = 3.56 %\n",
      "Class =   9   Qty =      786   Percentage = 3.05 %\n",
      "Class =   1   Qty =      723   Percentage = 2.81 %\n",
      "Class =   6   Qty =      663   Percentage = 2.58 %\n",
      "Class =  25   Qty =      540   Percentage = 2.10 %\n",
      "Class =  17   Qty =      473   Percentage = 1.84 %\n",
      "Class =   4   Qty =      384   Percentage = 1.49 %\n",
      "Class =  21   Qty =      201   Percentage = 0.78 %\n",
      "Class =  19   Qty =      122   Percentage = 0.47 %\n",
      "Class =  20   Qty =      111   Percentage = 0.43 %\n",
      "Class =  11   Qty =       98   Percentage = 0.38 %\n",
      "Class =  23   Qty =       96   Percentage = 0.37 %\n",
      "Class =   7   Qty =       94   Percentage = 0.37 %\n",
      "Class =  14   Qty =       77   Percentage = 0.30 %\n",
      "Class =  29   Qty =       77   Percentage = 0.30 %\n",
      "Class =  30   Qty =       64   Percentage = 0.25 %\n",
      "Class =  15   Qty =       62   Percentage = 0.24 %\n",
      "Class =   5   Qty =       44   Percentage = 0.17 %\n",
      "Class =  28   Qty =       35   Percentage = 0.14 %\n",
      "Class =  27   Qty =       28   Percentage = 0.11 %\n",
      "Class =  18   Qty =       22   Percentage = 0.09 %\n",
      "Class =  13   Qty =       20   Percentage = 0.08 %\n",
      "Class =   8   Qty =       12   Percentage = 0.05 %\n",
      "Class =  10   Qty =        9   Percentage = 0.03 %\n",
      "Class =  12   Qty =        8   Percentage = 0.03 %\n",
      "Class =  22   Qty =        7   Percentage = 0.03 %\n",
      "Class =  31   Qty =        7   Percentage = 0.03 %\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1248</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)        │            <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1248</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)        │            <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1246</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)        │           <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1246</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)        │            <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">623</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">619</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)        │           <span style=\"color: #00af00; text-decoration-color: #00af00\">656</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_2           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">619</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)        │            <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">615</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)        │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,296</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_3           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">615</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)        │            <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">307</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4912</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">628,864</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_4           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,128</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv1d (\u001b[38;5;33mConv1D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1248\u001b[0m, \u001b[38;5;34m8\u001b[0m)        │            \u001b[38;5;34m32\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1248\u001b[0m, \u001b[38;5;34m8\u001b[0m)        │            \u001b[38;5;34m32\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_1 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1246\u001b[0m, \u001b[38;5;34m8\u001b[0m)        │           \u001b[38;5;34m200\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_1           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1246\u001b[0m, \u001b[38;5;34m8\u001b[0m)        │            \u001b[38;5;34m32\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d (\u001b[38;5;33mMaxPooling1D\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m623\u001b[0m, \u001b[38;5;34m8\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_2 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m619\u001b[0m, \u001b[38;5;34m16\u001b[0m)        │           \u001b[38;5;34m656\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_2           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m619\u001b[0m, \u001b[38;5;34m16\u001b[0m)        │            \u001b[38;5;34m64\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_3 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m615\u001b[0m, \u001b[38;5;34m16\u001b[0m)        │         \u001b[38;5;34m1,296\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_3           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m615\u001b[0m, \u001b[38;5;34m16\u001b[0m)        │            \u001b[38;5;34m64\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_1 (\u001b[38;5;33mMaxPooling1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m307\u001b[0m, \u001b[38;5;34m16\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4912\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │       \u001b[38;5;34m628,864\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_4           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │           \u001b[38;5;34m512\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │         \u001b[38;5;34m4,128\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">635,880</span> (2.43 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m635,880\u001b[0m (2.43 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">635,528</span> (2.42 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m635,528\u001b[0m (2.42 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">352</span> (1.38 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m352\u001b[0m (1.38 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting training...\n",
      "\n",
      "Training fold 1\n",
      "\n",
      "Generating upsampling using SMOTE...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\DeveloperTools\\python\\3.11.0\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:737: UserWarning: The least populated class in y has only 7 members, which is less than n_splits=10.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200000 samples after upsampling.\n",
      "Class distribution for training after upsampling: Counter({np.int64(0): 6250, np.int64(1): 6250, np.int64(2): 6250, np.int64(3): 6250, np.int64(4): 6250, np.int64(5): 6250, np.int64(6): 6250, np.int64(7): 6250, np.int64(8): 6250, np.int64(9): 6250, np.int64(10): 6250, np.int64(11): 6250, np.int64(12): 6250, np.int64(13): 6250, np.int64(14): 6250, np.int64(15): 6250, np.int64(16): 6250, np.int64(17): 6250, np.int64(18): 6250, np.int64(19): 6250, np.int64(20): 6250, np.int64(21): 6250, np.int64(22): 6250, np.int64(23): 6250, np.int64(24): 6250, np.int64(25): 6250, np.int64(26): 6250, np.int64(27): 6250, np.int64(28): 6250, np.int64(29): 6250, np.int64(30): 6250, np.int64(31): 6250})\n",
      "Finishing upsampling.\n",
      "\n",
      "Epoch 1/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 17ms/step - accuracy: 0.8528 - loss: 0.5718 - val_accuracy: 1.0000 - val_loss: 2.7154e-04\n",
      "Epoch 2/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m102s\u001b[0m 17ms/step - accuracy: 0.9724 - loss: 0.0879 - val_accuracy: 1.0000 - val_loss: 2.3150e-05\n",
      "Epoch 3/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m101s\u001b[0m 16ms/step - accuracy: 0.9861 - loss: 0.0445 - val_accuracy: 1.0000 - val_loss: 4.3290e-05\n",
      "Epoch 4/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m101s\u001b[0m 16ms/step - accuracy: 0.9909 - loss: 0.0286 - val_accuracy: 1.0000 - val_loss: 2.1005e-07\n",
      "Epoch 5/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m100s\u001b[0m 16ms/step - accuracy: 0.9935 - loss: 0.0206 - val_accuracy: 1.0000 - val_loss: 3.4325e-06\n",
      "Epoch 6/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m103s\u001b[0m 17ms/step - accuracy: 0.9940 - loss: 0.0179 - val_accuracy: 1.0000 - val_loss: 2.5167e-06\n",
      "Epoch 7/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m102s\u001b[0m 17ms/step - accuracy: 0.9948 - loss: 0.0158 - val_accuracy: 1.0000 - val_loss: 3.0184e-07\n",
      "Epoch 8/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m101s\u001b[0m 16ms/step - accuracy: 0.9958 - loss: 0.0131 - val_accuracy: 1.0000 - val_loss: 1.0162e-05\n",
      "Epoch 9/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m103s\u001b[0m 17ms/step - accuracy: 0.9961 - loss: 0.0123 - val_accuracy: 1.0000 - val_loss: 2.1898e-07\n",
      "Epoch 10/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m101s\u001b[0m 16ms/step - accuracy: 0.9966 - loss: 0.0103 - val_accuracy: 1.0000 - val_loss: 1.1921e-10\n",
      "Epoch 11/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m101s\u001b[0m 16ms/step - accuracy: 0.9967 - loss: 0.0104 - val_accuracy: 1.0000 - val_loss: 4.1562e-06\n",
      "Epoch 12/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m102s\u001b[0m 16ms/step - accuracy: 0.9968 - loss: 0.0099 - val_accuracy: 1.0000 - val_loss: 9.3460e-08\n",
      "Epoch 13/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m102s\u001b[0m 17ms/step - accuracy: 0.9974 - loss: 0.0083 - val_accuracy: 1.0000 - val_loss: 4.6616e-07\n",
      "Epoch 14/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m102s\u001b[0m 17ms/step - accuracy: 0.9972 - loss: 0.0081 - val_accuracy: 1.0000 - val_loss: 5.3644e-10\n",
      "Epoch 15/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m102s\u001b[0m 17ms/step - accuracy: 0.9974 - loss: 0.0078 - val_accuracy: 1.0000 - val_loss: 8.8930e-08\n",
      "Epoch 16/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 17ms/step - accuracy: 0.9977 - loss: 0.0073 - val_accuracy: 1.0000 - val_loss: 1.5497e-09\n",
      "Epoch 17/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m103s\u001b[0m 17ms/step - accuracy: 0.9980 - loss: 0.0063 - val_accuracy: 1.0000 - val_loss: 7.8618e-08\n",
      "Epoch 18/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 17ms/step - accuracy: 0.9979 - loss: 0.0071 - val_accuracy: 1.0000 - val_loss: 4.5896e-08\n",
      "Epoch 19/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 17ms/step - accuracy: 0.9978 - loss: 0.0070 - val_accuracy: 1.0000 - val_loss: 7.0035e-08\n",
      "Epoch 20/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 17ms/step - accuracy: 0.9981 - loss: 0.0061 - val_accuracy: 1.0000 - val_loss: 8.2910e-08\n",
      "Epoch 20: early stopping\n",
      "Restoring model weights from the end of the best epoch: 10.\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step\n",
      "\n",
      "Training fold 2\n",
      "\n",
      "Generating upsampling using SMOTE...\n",
      "200000 samples after upsampling.\n",
      "Class distribution for training after upsampling: Counter({np.int64(0): 6250, np.int64(1): 6250, np.int64(2): 6250, np.int64(3): 6250, np.int64(4): 6250, np.int64(5): 6250, np.int64(6): 6250, np.int64(7): 6250, np.int64(8): 6250, np.int64(9): 6250, np.int64(10): 6250, np.int64(11): 6250, np.int64(12): 6250, np.int64(13): 6250, np.int64(14): 6250, np.int64(15): 6250, np.int64(16): 6250, np.int64(17): 6250, np.int64(18): 6250, np.int64(19): 6250, np.int64(20): 6250, np.int64(21): 6250, np.int64(22): 6250, np.int64(23): 6250, np.int64(24): 6250, np.int64(25): 6250, np.int64(26): 6250, np.int64(27): 6250, np.int64(28): 6250, np.int64(29): 6250, np.int64(30): 6250, np.int64(31): 6250})\n",
      "Finishing upsampling.\n",
      "\n",
      "Epoch 1/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 17ms/step - accuracy: 0.9686 - loss: 0.1517 - val_accuracy: 1.0000 - val_loss: 2.8938e-07\n",
      "Epoch 2/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 17ms/step - accuracy: 0.9937 - loss: 0.0198 - val_accuracy: 1.0000 - val_loss: 7.2042e-07\n",
      "Epoch 3/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 17ms/step - accuracy: 0.9966 - loss: 0.0118 - val_accuracy: 1.0000 - val_loss: 9.0307e-06\n",
      "Epoch 4/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 17ms/step - accuracy: 0.9969 - loss: 0.0098 - val_accuracy: 1.0000 - val_loss: 9.8017e-07\n",
      "Epoch 5/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 17ms/step - accuracy: 0.9971 - loss: 0.0093 - val_accuracy: 1.0000 - val_loss: 1.1921e-10\n",
      "Epoch 6/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 17ms/step - accuracy: 0.9974 - loss: 0.0085 - val_accuracy: 1.0000 - val_loss: 1.3292e-08\n",
      "Epoch 7/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 17ms/step - accuracy: 0.9973 - loss: 0.0088 - val_accuracy: 1.0000 - val_loss: 4.5955e-08\n",
      "Epoch 8/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 17ms/step - accuracy: 0.9977 - loss: 0.0076 - val_accuracy: 1.0000 - val_loss: 9.6797e-08\n",
      "Epoch 9/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 17ms/step - accuracy: 0.9978 - loss: 0.0078 - val_accuracy: 1.0000 - val_loss: 1.4067e-08\n",
      "Epoch 10/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 17ms/step - accuracy: 0.9979 - loss: 0.0070 - val_accuracy: 1.0000 - val_loss: 2.9802e-10\n",
      "Epoch 11/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 17ms/step - accuracy: 0.9980 - loss: 0.0070 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 12/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 17ms/step - accuracy: 0.9983 - loss: 0.0056 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 13/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 17ms/step - accuracy: 0.9983 - loss: 0.0056 - val_accuracy: 1.0000 - val_loss: 1.9968e-08\n",
      "Epoch 14/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 17ms/step - accuracy: 0.9980 - loss: 0.0067 - val_accuracy: 1.0000 - val_loss: 1.1861e-08\n",
      "Epoch 15/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 17ms/step - accuracy: 0.9980 - loss: 0.0065 - val_accuracy: 1.0000 - val_loss: 5.4549e-07\n",
      "Epoch 16/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 17ms/step - accuracy: 0.9986 - loss: 0.0048 - val_accuracy: 1.0000 - val_loss: 2.4438e-09\n",
      "Epoch 17/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 17ms/step - accuracy: 0.9983 - loss: 0.0056 - val_accuracy: 1.0000 - val_loss: 3.9339e-09\n",
      "Epoch 18/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 17ms/step - accuracy: 0.9986 - loss: 0.0048 - val_accuracy: 1.0000 - val_loss: 5.3644e-10\n",
      "Epoch 19/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 17ms/step - accuracy: 0.9987 - loss: 0.0043 - val_accuracy: 1.0000 - val_loss: 1.6093e-09\n",
      "Epoch 20/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 17ms/step - accuracy: 0.9986 - loss: 0.0049 - val_accuracy: 1.0000 - val_loss: 2.9802e-10\n",
      "Epoch 21/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 17ms/step - accuracy: 0.9989 - loss: 0.0041 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 21: early stopping\n",
      "Restoring model weights from the end of the best epoch: 11.\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\n",
      "Training fold 3\n",
      "\n",
      "Generating upsampling using SMOTE...\n",
      "200000 samples after upsampling.\n",
      "Class distribution for training after upsampling: Counter({np.int64(0): 6250, np.int64(1): 6250, np.int64(2): 6250, np.int64(3): 6250, np.int64(4): 6250, np.int64(5): 6250, np.int64(6): 6250, np.int64(7): 6250, np.int64(8): 6250, np.int64(9): 6250, np.int64(10): 6250, np.int64(11): 6250, np.int64(12): 6250, np.int64(13): 6250, np.int64(14): 6250, np.int64(15): 6250, np.int64(16): 6250, np.int64(17): 6250, np.int64(18): 6250, np.int64(19): 6250, np.int64(20): 6250, np.int64(21): 6250, np.int64(22): 6250, np.int64(23): 6250, np.int64(24): 6250, np.int64(25): 6250, np.int64(26): 6250, np.int64(27): 6250, np.int64(28): 6250, np.int64(29): 6250, np.int64(30): 6250, np.int64(31): 6250})\n",
      "Finishing upsampling.\n",
      "\n",
      "Epoch 1/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m110s\u001b[0m 18ms/step - accuracy: 0.9843 - loss: 0.0691 - val_accuracy: 1.0000 - val_loss: 3.1590e-09\n",
      "Epoch 2/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 17ms/step - accuracy: 0.9966 - loss: 0.0115 - val_accuracy: 1.0000 - val_loss: 6.8744e-07\n",
      "Epoch 3/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 17ms/step - accuracy: 0.9977 - loss: 0.0083 - val_accuracy: 1.0000 - val_loss: 9.6916e-08\n",
      "Epoch 4/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 17ms/step - accuracy: 0.9975 - loss: 0.0082 - val_accuracy: 1.0000 - val_loss: 8.8215e-08\n",
      "Epoch 5/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 17ms/step - accuracy: 0.9978 - loss: 0.0072 - val_accuracy: 1.0000 - val_loss: 6.3598e-08\n",
      "Epoch 6/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 17ms/step - accuracy: 0.9982 - loss: 0.0065 - val_accuracy: 1.0000 - val_loss: 1.1921e-09\n",
      "Epoch 7/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 17ms/step - accuracy: 0.9984 - loss: 0.0058 - val_accuracy: 1.0000 - val_loss: 2.4074e-07\n",
      "Epoch 8/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 17ms/step - accuracy: 0.9984 - loss: 0.0063 - val_accuracy: 1.0000 - val_loss: 3.3379e-09\n",
      "Epoch 9/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 17ms/step - accuracy: 0.9984 - loss: 0.0050 - val_accuracy: 1.0000 - val_loss: 5.0664e-09\n",
      "Epoch 10/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 17ms/step - accuracy: 0.9986 - loss: 0.0049 - val_accuracy: 1.0000 - val_loss: 5.9605e-11\n",
      "Epoch 11/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 17ms/step - accuracy: 0.9984 - loss: 0.0064 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 12/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 17ms/step - accuracy: 0.9987 - loss: 0.0045 - val_accuracy: 1.0000 - val_loss: 2.6226e-09\n",
      "Epoch 13/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 17ms/step - accuracy: 0.9989 - loss: 0.0043 - val_accuracy: 1.0000 - val_loss: 5.9605e-11\n",
      "Epoch 14/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 17ms/step - accuracy: 0.9988 - loss: 0.0044 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 15/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 17ms/step - accuracy: 0.9988 - loss: 0.0046 - val_accuracy: 1.0000 - val_loss: 1.4424e-08\n",
      "Epoch 16/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 17ms/step - accuracy: 0.9991 - loss: 0.0035 - val_accuracy: 1.0000 - val_loss: 3.0398e-09\n",
      "Epoch 17/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 17ms/step - accuracy: 0.9988 - loss: 0.0047 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 18/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 17ms/step - accuracy: 0.9988 - loss: 0.0042 - val_accuracy: 1.0000 - val_loss: 3.8147e-09\n",
      "Epoch 19/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 17ms/step - accuracy: 0.9987 - loss: 0.0047 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 20/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 17ms/step - accuracy: 0.9990 - loss: 0.0039 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 21/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 17ms/step - accuracy: 0.9987 - loss: 0.0052 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 21: early stopping\n",
      "Restoring model weights from the end of the best epoch: 11.\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\n",
      "Training fold 4\n",
      "\n",
      "Generating upsampling using SMOTE...\n",
      "200000 samples after upsampling.\n",
      "Class distribution for training after upsampling: Counter({np.int64(0): 6250, np.int64(1): 6250, np.int64(2): 6250, np.int64(3): 6250, np.int64(4): 6250, np.int64(5): 6250, np.int64(6): 6250, np.int64(7): 6250, np.int64(8): 6250, np.int64(9): 6250, np.int64(10): 6250, np.int64(11): 6250, np.int64(12): 6250, np.int64(13): 6250, np.int64(14): 6250, np.int64(15): 6250, np.int64(16): 6250, np.int64(17): 6250, np.int64(18): 6250, np.int64(19): 6250, np.int64(20): 6250, np.int64(21): 6250, np.int64(22): 6250, np.int64(23): 6250, np.int64(24): 6250, np.int64(25): 6250, np.int64(26): 6250, np.int64(27): 6250, np.int64(28): 6250, np.int64(29): 6250, np.int64(30): 6250, np.int64(31): 6250})\n",
      "Finishing upsampling.\n",
      "\n",
      "Epoch 1/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m112s\u001b[0m 18ms/step - accuracy: 0.9889 - loss: 0.0461 - val_accuracy: 1.0000 - val_loss: 5.9605e-10\n",
      "Epoch 2/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 18ms/step - accuracy: 0.9968 - loss: 0.0104 - val_accuracy: 1.0000 - val_loss: 3.9756e-08\n",
      "Epoch 3/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 18ms/step - accuracy: 0.9978 - loss: 0.0076 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 4/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m110s\u001b[0m 18ms/step - accuracy: 0.9984 - loss: 0.0055 - val_accuracy: 1.0000 - val_loss: 6.6161e-09\n",
      "Epoch 5/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 18ms/step - accuracy: 0.9984 - loss: 0.0053 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 6/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m110s\u001b[0m 18ms/step - accuracy: 0.9984 - loss: 0.0056 - val_accuracy: 1.0000 - val_loss: 9.5367e-10\n",
      "Epoch 7/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 18ms/step - accuracy: 0.9986 - loss: 0.0048 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 8/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 18ms/step - accuracy: 0.9988 - loss: 0.0043 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 9/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 18ms/step - accuracy: 0.9988 - loss: 0.0040 - val_accuracy: 1.0000 - val_loss: 1.0133e-09\n",
      "Epoch 10/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m110s\u001b[0m 18ms/step - accuracy: 0.9988 - loss: 0.0038 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 11/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m110s\u001b[0m 18ms/step - accuracy: 0.9988 - loss: 0.0048 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 12/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m110s\u001b[0m 18ms/step - accuracy: 0.9992 - loss: 0.0033 - val_accuracy: 1.0000 - val_loss: 1.9419e-07\n",
      "Epoch 13/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m111s\u001b[0m 18ms/step - accuracy: 0.9991 - loss: 0.0035 - val_accuracy: 1.0000 - val_loss: 1.0133e-09\n",
      "Epoch 13: early stopping\n",
      "Restoring model weights from the end of the best epoch: 3.\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\n",
      "Training fold 5\n",
      "\n",
      "Generating upsampling using SMOTE...\n",
      "200000 samples after upsampling.\n",
      "Class distribution for training after upsampling: Counter({np.int64(0): 6250, np.int64(1): 6250, np.int64(2): 6250, np.int64(3): 6250, np.int64(4): 6250, np.int64(5): 6250, np.int64(6): 6250, np.int64(7): 6250, np.int64(8): 6250, np.int64(9): 6250, np.int64(10): 6250, np.int64(11): 6250, np.int64(12): 6250, np.int64(13): 6250, np.int64(14): 6250, np.int64(15): 6250, np.int64(16): 6250, np.int64(17): 6250, np.int64(18): 6250, np.int64(19): 6250, np.int64(20): 6250, np.int64(21): 6250, np.int64(22): 6250, np.int64(23): 6250, np.int64(24): 6250, np.int64(25): 6250, np.int64(26): 6250, np.int64(27): 6250, np.int64(28): 6250, np.int64(29): 6250, np.int64(30): 6250, np.int64(31): 6250})\n",
      "Finishing upsampling.\n",
      "\n",
      "Epoch 1/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m114s\u001b[0m 18ms/step - accuracy: 0.9935 - loss: 0.0241 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 2/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m113s\u001b[0m 18ms/step - accuracy: 0.9977 - loss: 0.0076 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 3/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m113s\u001b[0m 18ms/step - accuracy: 0.9984 - loss: 0.0060 - val_accuracy: 1.0000 - val_loss: 4.5657e-08\n",
      "Epoch 4/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m115s\u001b[0m 19ms/step - accuracy: 0.9984 - loss: 0.0064 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 5/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m114s\u001b[0m 18ms/step - accuracy: 0.9988 - loss: 0.0049 - val_accuracy: 1.0000 - val_loss: 7.1526e-10\n",
      "Epoch 6/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m113s\u001b[0m 18ms/step - accuracy: 0.9986 - loss: 0.0054 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 7/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m112s\u001b[0m 18ms/step - accuracy: 0.9989 - loss: 0.0038 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 8/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m113s\u001b[0m 18ms/step - accuracy: 0.9989 - loss: 0.0040 - val_accuracy: 1.0000 - val_loss: 3.0398e-09\n",
      "Epoch 9/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m113s\u001b[0m 18ms/step - accuracy: 0.9986 - loss: 0.0051 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 10/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m113s\u001b[0m 18ms/step - accuracy: 0.9985 - loss: 0.0055 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 11/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m114s\u001b[0m 18ms/step - accuracy: 0.9989 - loss: 0.0046 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 11: early stopping\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\n",
      "Training fold 6\n",
      "\n",
      "Generating upsampling using SMOTE...\n",
      "200000 samples after upsampling.\n",
      "Class distribution for training after upsampling: Counter({np.int64(0): 6250, np.int64(1): 6250, np.int64(2): 6250, np.int64(3): 6250, np.int64(4): 6250, np.int64(5): 6250, np.int64(6): 6250, np.int64(7): 6250, np.int64(8): 6250, np.int64(9): 6250, np.int64(10): 6250, np.int64(11): 6250, np.int64(12): 6250, np.int64(13): 6250, np.int64(14): 6250, np.int64(15): 6250, np.int64(16): 6250, np.int64(17): 6250, np.int64(18): 6250, np.int64(19): 6250, np.int64(20): 6250, np.int64(21): 6250, np.int64(22): 6250, np.int64(23): 6250, np.int64(24): 6250, np.int64(25): 6250, np.int64(26): 6250, np.int64(27): 6250, np.int64(28): 6250, np.int64(29): 6250, np.int64(30): 6250, np.int64(31): 6250})\n",
      "Finishing upsampling.\n",
      "\n",
      "Epoch 1/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m120s\u001b[0m 19ms/step - accuracy: 0.9951 - loss: 0.0162 - val_accuracy: 1.0000 - val_loss: 1.4841e-08\n",
      "Epoch 2/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m117s\u001b[0m 19ms/step - accuracy: 0.9982 - loss: 0.0062 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 3/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m115s\u001b[0m 19ms/step - accuracy: 0.9984 - loss: 0.0067 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 4/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m117s\u001b[0m 19ms/step - accuracy: 0.9986 - loss: 0.0055 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 5/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m116s\u001b[0m 19ms/step - accuracy: 0.9987 - loss: 0.0047 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 6/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m116s\u001b[0m 19ms/step - accuracy: 0.9989 - loss: 0.0039 - val_accuracy: 1.0000 - val_loss: 1.9670e-08\n",
      "Epoch 7/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m116s\u001b[0m 19ms/step - accuracy: 0.9989 - loss: 0.0046 - val_accuracy: 1.0000 - val_loss: 1.3351e-08\n",
      "Epoch 8/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m117s\u001b[0m 19ms/step - accuracy: 0.9989 - loss: 0.0042 - val_accuracy: 1.0000 - val_loss: 1.8656e-08\n",
      "Epoch 9/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m117s\u001b[0m 19ms/step - accuracy: 0.9990 - loss: 0.0035 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 10/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m117s\u001b[0m 19ms/step - accuracy: 0.9989 - loss: 0.0036 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 11/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m117s\u001b[0m 19ms/step - accuracy: 0.9989 - loss: 0.0042 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 12/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m117s\u001b[0m 19ms/step - accuracy: 0.9989 - loss: 0.0038 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 12: early stopping\n",
      "Restoring model weights from the end of the best epoch: 2.\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\n",
      "Training fold 7\n",
      "\n",
      "Generating upsampling using SMOTE...\n",
      "200000 samples after upsampling.\n",
      "Class distribution for training after upsampling: Counter({np.int64(0): 6250, np.int64(1): 6250, np.int64(2): 6250, np.int64(3): 6250, np.int64(4): 6250, np.int64(5): 6250, np.int64(6): 6250, np.int64(7): 6250, np.int64(8): 6250, np.int64(9): 6250, np.int64(10): 6250, np.int64(11): 6250, np.int64(12): 6250, np.int64(13): 6250, np.int64(14): 6250, np.int64(15): 6250, np.int64(16): 6250, np.int64(17): 6250, np.int64(18): 6250, np.int64(19): 6250, np.int64(20): 6250, np.int64(21): 6250, np.int64(22): 6250, np.int64(23): 6250, np.int64(24): 6250, np.int64(25): 6250, np.int64(26): 6250, np.int64(27): 6250, np.int64(28): 6250, np.int64(29): 6250, np.int64(30): 6250, np.int64(31): 6250})\n",
      "Finishing upsampling.\n",
      "\n",
      "Epoch 1/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m122s\u001b[0m 20ms/step - accuracy: 0.9956 - loss: 0.0143 - val_accuracy: 1.0000 - val_loss: 2.8610e-09\n",
      "Epoch 2/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m120s\u001b[0m 19ms/step - accuracy: 0.9982 - loss: 0.0065 - val_accuracy: 1.0000 - val_loss: 4.4703e-09\n",
      "Epoch 3/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m120s\u001b[0m 19ms/step - accuracy: 0.9985 - loss: 0.0049 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 4/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m120s\u001b[0m 19ms/step - accuracy: 0.9983 - loss: 0.0051 - val_accuracy: 1.0000 - val_loss: 2.0862e-09\n",
      "Epoch 5/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m121s\u001b[0m 20ms/step - accuracy: 0.9987 - loss: 0.0040 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 6/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m122s\u001b[0m 20ms/step - accuracy: 0.9987 - loss: 0.0043 - val_accuracy: 1.0000 - val_loss: 1.6886e-06\n",
      "Epoch 7/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m122s\u001b[0m 20ms/step - accuracy: 0.9989 - loss: 0.0043 - val_accuracy: 1.0000 - val_loss: 1.9908e-08\n",
      "Epoch 8/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m121s\u001b[0m 20ms/step - accuracy: 0.9990 - loss: 0.0032 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 9/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m122s\u001b[0m 20ms/step - accuracy: 0.9988 - loss: 0.0039 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 10/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m122s\u001b[0m 20ms/step - accuracy: 0.9988 - loss: 0.0045 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 11/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m123s\u001b[0m 20ms/step - accuracy: 0.9992 - loss: 0.0030 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 12/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m123s\u001b[0m 20ms/step - accuracy: 0.9992 - loss: 0.0024 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 13/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m124s\u001b[0m 20ms/step - accuracy: 0.9991 - loss: 0.0029 - val_accuracy: 1.0000 - val_loss: 1.1921e-10\n",
      "Epoch 13: early stopping\n",
      "Restoring model weights from the end of the best epoch: 3.\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\n",
      "Training fold 8\n",
      "\n",
      "Generating upsampling using SMOTE...\n",
      "200000 samples after upsampling.\n",
      "Class distribution for training after upsampling: Counter({np.int64(0): 6250, np.int64(1): 6250, np.int64(2): 6250, np.int64(3): 6250, np.int64(4): 6250, np.int64(5): 6250, np.int64(6): 6250, np.int64(7): 6250, np.int64(8): 6250, np.int64(9): 6250, np.int64(10): 6250, np.int64(11): 6250, np.int64(12): 6250, np.int64(13): 6250, np.int64(14): 6250, np.int64(15): 6250, np.int64(16): 6250, np.int64(17): 6250, np.int64(18): 6250, np.int64(19): 6250, np.int64(20): 6250, np.int64(21): 6250, np.int64(22): 6250, np.int64(23): 6250, np.int64(24): 6250, np.int64(25): 6250, np.int64(26): 6250, np.int64(27): 6250, np.int64(28): 6250, np.int64(29): 6250, np.int64(30): 6250, np.int64(31): 6250})\n",
      "Finishing upsampling.\n",
      "\n",
      "Epoch 1/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m128s\u001b[0m 21ms/step - accuracy: 0.9963 - loss: 0.0126 - val_accuracy: 1.0000 - val_loss: 1.9371e-08\n",
      "Epoch 2/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m125s\u001b[0m 20ms/step - accuracy: 0.9984 - loss: 0.0051 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 3/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m124s\u001b[0m 20ms/step - accuracy: 0.9986 - loss: 0.0049 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 4/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m123s\u001b[0m 20ms/step - accuracy: 0.9990 - loss: 0.0040 - val_accuracy: 1.0000 - val_loss: 5.3644e-10\n",
      "Epoch 5/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m124s\u001b[0m 20ms/step - accuracy: 0.9987 - loss: 0.0044 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 6/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m124s\u001b[0m 20ms/step - accuracy: 0.9991 - loss: 0.0037 - val_accuracy: 1.0000 - val_loss: 2.3842e-09\n",
      "Epoch 7/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m124s\u001b[0m 20ms/step - accuracy: 0.9990 - loss: 0.0040 - val_accuracy: 1.0000 - val_loss: 5.9605e-11\n",
      "Epoch 8/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m126s\u001b[0m 20ms/step - accuracy: 0.9991 - loss: 0.0033 - val_accuracy: 1.0000 - val_loss: 1.4901e-09\n",
      "Epoch 9/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m124s\u001b[0m 20ms/step - accuracy: 0.9992 - loss: 0.0027 - val_accuracy: 1.0000 - val_loss: 3.7551e-09\n",
      "Epoch 10/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m124s\u001b[0m 20ms/step - accuracy: 0.9989 - loss: 0.0035 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 11/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m125s\u001b[0m 20ms/step - accuracy: 0.9991 - loss: 0.0032 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 12/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m125s\u001b[0m 20ms/step - accuracy: 0.9992 - loss: 0.0039 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 12: early stopping\n",
      "Restoring model weights from the end of the best epoch: 2.\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "\n",
      "Training fold 9\n",
      "\n",
      "Generating upsampling using SMOTE...\n",
      "200000 samples after upsampling.\n",
      "Class distribution for training after upsampling: Counter({np.int64(0): 6250, np.int64(1): 6250, np.int64(2): 6250, np.int64(3): 6250, np.int64(4): 6250, np.int64(5): 6250, np.int64(6): 6250, np.int64(7): 6250, np.int64(8): 6250, np.int64(9): 6250, np.int64(10): 6250, np.int64(11): 6250, np.int64(12): 6250, np.int64(13): 6250, np.int64(14): 6250, np.int64(15): 6250, np.int64(16): 6250, np.int64(17): 6250, np.int64(18): 6250, np.int64(19): 6250, np.int64(20): 6250, np.int64(21): 6250, np.int64(22): 6250, np.int64(23): 6250, np.int64(24): 6250, np.int64(25): 6250, np.int64(26): 6250, np.int64(27): 6250, np.int64(28): 6250, np.int64(29): 6250, np.int64(30): 6250, np.int64(31): 6250})\n",
      "Finishing upsampling.\n",
      "\n",
      "Epoch 1/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m130s\u001b[0m 21ms/step - accuracy: 0.9973 - loss: 0.0089 - val_accuracy: 1.0000 - val_loss: 1.1074e-07\n",
      "Epoch 2/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m127s\u001b[0m 20ms/step - accuracy: 0.9987 - loss: 0.0043 - val_accuracy: 1.0000 - val_loss: 1.8477e-09\n",
      "Epoch 3/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m126s\u001b[0m 20ms/step - accuracy: 0.9985 - loss: 0.0050 - val_accuracy: 1.0000 - val_loss: 2.2923e-07\n",
      "Epoch 4/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m126s\u001b[0m 20ms/step - accuracy: 0.9989 - loss: 0.0037 - val_accuracy: 1.0000 - val_loss: 3.4571e-09\n",
      "Epoch 5/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m127s\u001b[0m 21ms/step - accuracy: 0.9990 - loss: 0.0039 - val_accuracy: 1.0000 - val_loss: 1.6093e-09\n",
      "Epoch 6/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m127s\u001b[0m 21ms/step - accuracy: 0.9989 - loss: 0.0035 - val_accuracy: 1.0000 - val_loss: 1.5497e-09\n",
      "Epoch 7/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m127s\u001b[0m 20ms/step - accuracy: 0.9990 - loss: 0.0037 - val_accuracy: 1.0000 - val_loss: 3.8981e-08\n",
      "Epoch 8/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m127s\u001b[0m 21ms/step - accuracy: 0.9993 - loss: 0.0026 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 9/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m127s\u001b[0m 21ms/step - accuracy: 0.9992 - loss: 0.0034 - val_accuracy: 1.0000 - val_loss: 2.8865e-07\n",
      "Epoch 10/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m127s\u001b[0m 21ms/step - accuracy: 0.9990 - loss: 0.0032 - val_accuracy: 1.0000 - val_loss: 1.1921e-10\n",
      "Epoch 11/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m128s\u001b[0m 21ms/step - accuracy: 0.9992 - loss: 0.0035 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 12/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m129s\u001b[0m 21ms/step - accuracy: 0.9992 - loss: 0.0038 - val_accuracy: 1.0000 - val_loss: 1.0848e-08\n",
      "Epoch 13/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m129s\u001b[0m 21ms/step - accuracy: 0.9992 - loss: 0.0027 - val_accuracy: 1.0000 - val_loss: 2.8253e-08\n",
      "Epoch 14/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m129s\u001b[0m 21ms/step - accuracy: 0.9994 - loss: 0.0023 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 15/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m129s\u001b[0m 21ms/step - accuracy: 0.9991 - loss: 0.0027 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 16/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m125s\u001b[0m 20ms/step - accuracy: 0.9992 - loss: 0.0025 - val_accuracy: 1.0000 - val_loss: 4.6730e-08\n",
      "Epoch 17/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 17ms/step - accuracy: 0.9993 - loss: 0.0026 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 18/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 17ms/step - accuracy: 0.9994 - loss: 0.0022 - val_accuracy: 1.0000 - val_loss: 3.0637e-08\n",
      "Epoch 18: early stopping\n",
      "Restoring model weights from the end of the best epoch: 8.\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\n",
      "Training fold 10\n",
      "\n",
      "Generating upsampling using SMOTE...\n",
      "200000 samples after upsampling.\n",
      "Class distribution for training after upsampling: Counter({np.int64(0): 6250, np.int64(1): 6250, np.int64(2): 6250, np.int64(3): 6250, np.int64(4): 6250, np.int64(5): 6250, np.int64(6): 6250, np.int64(7): 6250, np.int64(8): 6250, np.int64(9): 6250, np.int64(10): 6250, np.int64(11): 6250, np.int64(12): 6250, np.int64(13): 6250, np.int64(14): 6250, np.int64(15): 6250, np.int64(16): 6250, np.int64(17): 6250, np.int64(18): 6250, np.int64(19): 6250, np.int64(20): 6250, np.int64(21): 6250, np.int64(22): 6250, np.int64(23): 6250, np.int64(24): 6250, np.int64(25): 6250, np.int64(26): 6250, np.int64(27): 6250, np.int64(28): 6250, np.int64(29): 6250, np.int64(30): 6250, np.int64(31): 6250})\n",
      "Finishing upsampling.\n",
      "\n",
      "Epoch 1/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m121s\u001b[0m 20ms/step - accuracy: 0.9969 - loss: 0.0122 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 2/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m121s\u001b[0m 20ms/step - accuracy: 0.9986 - loss: 0.0050 - val_accuracy: 1.0000 - val_loss: 6.0797e-09\n",
      "Epoch 3/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m120s\u001b[0m 19ms/step - accuracy: 0.9989 - loss: 0.0036 - val_accuracy: 1.0000 - val_loss: 4.1723e-10\n",
      "Epoch 4/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m121s\u001b[0m 20ms/step - accuracy: 0.9990 - loss: 0.0033 - val_accuracy: 1.0000 - val_loss: 3.9934e-07\n",
      "Epoch 5/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m122s\u001b[0m 20ms/step - accuracy: 0.9990 - loss: 0.0036 - val_accuracy: 1.0000 - val_loss: 1.7881e-10\n",
      "Epoch 6/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m123s\u001b[0m 20ms/step - accuracy: 0.9990 - loss: 0.0040 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 7/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m129s\u001b[0m 21ms/step - accuracy: 0.9991 - loss: 0.0028 - val_accuracy: 1.0000 - val_loss: 1.2159e-08\n",
      "Epoch 8/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m125s\u001b[0m 20ms/step - accuracy: 0.9990 - loss: 0.0031 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 9/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m127s\u001b[0m 21ms/step - accuracy: 0.9990 - loss: 0.0039 - val_accuracy: 1.0000 - val_loss: 2.9802e-10\n",
      "Epoch 10/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m128s\u001b[0m 21ms/step - accuracy: 0.9991 - loss: 0.0034 - val_accuracy: 1.0000 - val_loss: 6.5565e-10\n",
      "Epoch 11/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m128s\u001b[0m 21ms/step - accuracy: 0.9992 - loss: 0.0033 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 11: early stopping\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step\n",
      "\n",
      "Time taken for training:  05:41:36\n",
      "\n",
      "\n",
      "Fold 1 - Train Accuracy 0.9984 - Test Accuracy 0.5293\n",
      "Fold 2 - Train Accuracy 0.9994 - Test Accuracy 0.7289\n",
      "Fold 3 - Train Accuracy 0.9994 - Test Accuracy 0.8171\n",
      "Fold 4 - Train Accuracy 0.9994 - Test Accuracy 0.9254\n",
      "Fold 5 - Train Accuracy 0.9990 - Test Accuracy 0.9670\n",
      "Fold 6 - Train Accuracy 0.9995 - Test Accuracy 0.9701\n",
      "Fold 7 - Train Accuracy 0.9995 - Test Accuracy 0.9639\n",
      "Fold 8 - Train Accuracy 0.9995 - Test Accuracy 0.9802\n",
      "Fold 9 - Train Accuracy 0.9997 - Test Accuracy 0.9611\n",
      "Fold 10 - Train Accuracy 0.9995 - Test Accuracy 0.9891\n",
      "\n",
      "Mean Train Accuracy: 0.9993 - Std: 0.0003 \n",
      "Mean Test Accuracy: 0.8832 - Std: 0.1425 \n",
      "\n",
      "Evaluate other metrics:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.92      0.92     13905\n",
      "           1       0.94      0.93      0.94       723\n",
      "           2       0.92      0.97      0.95      2659\n",
      "           3       0.78      0.77      0.78      1123\n",
      "           4       0.84      0.79      0.81       384\n",
      "           5       0.94      0.73      0.82        44\n",
      "           6       0.82      0.84      0.83       663\n",
      "           7       0.86      0.84      0.85        94\n",
      "           8       0.92      0.92      0.92        12\n",
      "           9       0.85      0.81      0.83       786\n",
      "          10       0.78      0.78      0.78         9\n",
      "          11       0.83      0.81      0.82        98\n",
      "          12       0.75      0.75      0.75         8\n",
      "          13       0.88      0.75      0.81        20\n",
      "          14       0.94      0.83      0.88        77\n",
      "          15       0.92      0.94      0.93        62\n",
      "          16       0.78      0.79      0.79       917\n",
      "          17       0.88      0.87      0.87       473\n",
      "          18       0.81      0.77      0.79        22\n",
      "          19       0.85      0.80      0.83       122\n",
      "          20       0.85      0.77      0.81       111\n",
      "          21       0.90      0.78      0.83       201\n",
      "          22       0.50      0.86      0.63         7\n",
      "          23       0.80      0.75      0.77        96\n",
      "          24       0.75      0.77      0.76      1045\n",
      "          25       0.80      0.80      0.80       540\n",
      "          26       0.78      0.77      0.78      1334\n",
      "          27       0.79      0.79      0.79        28\n",
      "          28       0.94      0.91      0.93        35\n",
      "          29       0.87      0.81      0.84        77\n",
      "          30       0.83      0.84      0.84        64\n",
      "          31       0.67      0.86      0.75         7\n",
      "\n",
      "    accuracy                           0.88     25746\n",
      "   macro avg       0.83      0.82      0.83     25746\n",
      "weighted avg       0.88      0.88      0.88     25746\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Build a \"time window\" structure to handle the dataset as a time series.\n",
    "new_df = load_dataset(\"V2\")\n",
    "X_array, y_array = build_time_window_structure(new_df, \"V2\")\n",
    "X_array, y_array = remove_classes_with_less_samples(X_array, y_array)\n",
    "\n",
    "# Train the CNN model.\n",
    "v1_model = create_v1()\n",
    "v1_num_epochs = 300\n",
    "v1_batch_size = 32\n",
    "v1_validation_split = 0.01\n",
    "\n",
    "training_history_v1 = train_cnn_model(v1_model, X_array, y_array, v1_num_epochs, v1_batch_size, v1_validation_split, \"v1_model_V2.keras\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94d85e3-d80b-4382-a810-5f7459e324e3",
   "metadata": {},
   "source": [
    "#### V3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "acf30b68-13b9-4d3e-bb39-b83a0f5c003f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Start loading CSV file...\n",
      "Finish loading CSV file.\n",
      "\n",
      "Starting build_time_window_structure function...\n",
      "\n",
      "Shape of features:  (25770, 1250)\n",
      "Quantity os samples (labels):  25770\n",
      "\n",
      "Finishing build_time_window_structure function.\n",
      "\n",
      "Remove classes with less than 6 samples.\n",
      "\n",
      "Show classes identification:\n",
      "Class: 0 - Arrhythmia code: 1\n",
      "Class: 1 - Arrhythmia code: 21\n",
      "Class: 2 - Arrhythmia code: 22\n",
      "Class: 3 - Arrhythmia code: 23\n",
      "Class: 4 - Arrhythmia code: 30\n",
      "Class: 5 - Arrhythmia code: 36\n",
      "Class: 6 - Arrhythmia code: 50\n",
      "Class: 7 - Arrhythmia code: 51\n",
      "Class: 8 - Arrhythmia code: 54\n",
      "Class: 9 - Arrhythmia code: 60\n",
      "Class: 10 - Arrhythmia code: 80\n",
      "Class: 11 - Arrhythmia code: 82\n",
      "Class: 12 - Arrhythmia code: 83\n",
      "Class: 13 - Arrhythmia code: 88\n",
      "Class: 14 - Arrhythmia code: 101\n",
      "Class: 15 - Arrhythmia code: 104\n",
      "Class: 16 - Arrhythmia code: 105\n",
      "Class: 17 - Arrhythmia code: 106\n",
      "Class: 18 - Arrhythmia code: 108\n",
      "Class: 19 - Arrhythmia code: 120\n",
      "Class: 20 - Arrhythmia code: 121\n",
      "Class: 21 - Arrhythmia code: 125\n",
      "Class: 22 - Arrhythmia code: 140\n",
      "Class: 23 - Arrhythmia code: 142\n",
      "Class: 24 - Arrhythmia code: 145\n",
      "Class: 25 - Arrhythmia code: 146\n",
      "Class: 26 - Arrhythmia code: 147\n",
      "Class: 27 - Arrhythmia code: 155\n",
      "Class: 28 - Arrhythmia code: 160\n",
      "Class: 29 - Arrhythmia code: 161\n",
      "Class: 30 - Arrhythmia code: 165\n",
      "Class: 31 - Arrhythmia code: 166\n",
      "\n",
      "Check for dataset balance:\n",
      "Class =   0   Qty =    13905   Percentage = 54.01 %\n",
      "Class =   2   Qty =     2659   Percentage = 10.33 %\n",
      "Class =  26   Qty =     1334   Percentage = 5.18 %\n",
      "Class =   3   Qty =     1123   Percentage = 4.36 %\n",
      "Class =  24   Qty =     1045   Percentage = 4.06 %\n",
      "Class =  16   Qty =      917   Percentage = 3.56 %\n",
      "Class =   9   Qty =      786   Percentage = 3.05 %\n",
      "Class =   1   Qty =      723   Percentage = 2.81 %\n",
      "Class =   6   Qty =      663   Percentage = 2.58 %\n",
      "Class =  25   Qty =      540   Percentage = 2.10 %\n",
      "Class =  17   Qty =      473   Percentage = 1.84 %\n",
      "Class =   4   Qty =      384   Percentage = 1.49 %\n",
      "Class =  21   Qty =      201   Percentage = 0.78 %\n",
      "Class =  19   Qty =      122   Percentage = 0.47 %\n",
      "Class =  20   Qty =      111   Percentage = 0.43 %\n",
      "Class =  11   Qty =       98   Percentage = 0.38 %\n",
      "Class =  23   Qty =       96   Percentage = 0.37 %\n",
      "Class =   7   Qty =       94   Percentage = 0.37 %\n",
      "Class =  14   Qty =       77   Percentage = 0.30 %\n",
      "Class =  29   Qty =       77   Percentage = 0.30 %\n",
      "Class =  30   Qty =       64   Percentage = 0.25 %\n",
      "Class =  15   Qty =       62   Percentage = 0.24 %\n",
      "Class =   5   Qty =       44   Percentage = 0.17 %\n",
      "Class =  28   Qty =       35   Percentage = 0.14 %\n",
      "Class =  27   Qty =       28   Percentage = 0.11 %\n",
      "Class =  18   Qty =       22   Percentage = 0.09 %\n",
      "Class =  13   Qty =       20   Percentage = 0.08 %\n",
      "Class =   8   Qty =       12   Percentage = 0.05 %\n",
      "Class =  10   Qty =        9   Percentage = 0.03 %\n",
      "Class =  12   Qty =        8   Percentage = 0.03 %\n",
      "Class =  22   Qty =        7   Percentage = 0.03 %\n",
      "Class =  31   Qty =        7   Percentage = 0.03 %\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1248</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)        │            <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1248</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)        │            <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1246</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)        │           <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1246</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)        │            <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">623</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">619</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)        │           <span style=\"color: #00af00; text-decoration-color: #00af00\">656</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_2           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">619</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)        │            <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">615</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)        │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,296</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_3           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">615</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)        │            <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">307</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4912</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">628,864</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_4           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,128</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv1d (\u001b[38;5;33mConv1D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1248\u001b[0m, \u001b[38;5;34m8\u001b[0m)        │            \u001b[38;5;34m32\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1248\u001b[0m, \u001b[38;5;34m8\u001b[0m)        │            \u001b[38;5;34m32\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_1 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1246\u001b[0m, \u001b[38;5;34m8\u001b[0m)        │           \u001b[38;5;34m200\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_1           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1246\u001b[0m, \u001b[38;5;34m8\u001b[0m)        │            \u001b[38;5;34m32\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d (\u001b[38;5;33mMaxPooling1D\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m623\u001b[0m, \u001b[38;5;34m8\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_2 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m619\u001b[0m, \u001b[38;5;34m16\u001b[0m)        │           \u001b[38;5;34m656\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_2           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m619\u001b[0m, \u001b[38;5;34m16\u001b[0m)        │            \u001b[38;5;34m64\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_3 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m615\u001b[0m, \u001b[38;5;34m16\u001b[0m)        │         \u001b[38;5;34m1,296\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_3           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m615\u001b[0m, \u001b[38;5;34m16\u001b[0m)        │            \u001b[38;5;34m64\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_1 (\u001b[38;5;33mMaxPooling1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m307\u001b[0m, \u001b[38;5;34m16\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4912\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │       \u001b[38;5;34m628,864\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_4           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │           \u001b[38;5;34m512\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │         \u001b[38;5;34m4,128\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">635,880</span> (2.43 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m635,880\u001b[0m (2.43 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">635,528</span> (2.42 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m635,528\u001b[0m (2.42 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">352</span> (1.38 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m352\u001b[0m (1.38 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting training...\n",
      "\n",
      "Training fold 1\n",
      "\n",
      "Generating upsampling using SMOTE...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\DeveloperTools\\python\\3.11.0\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:737: UserWarning: The least populated class in y has only 7 members, which is less than n_splits=10.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200000 samples after upsampling.\n",
      "Class distribution for training after upsampling: Counter({np.int64(0): 6250, np.int64(1): 6250, np.int64(2): 6250, np.int64(3): 6250, np.int64(4): 6250, np.int64(5): 6250, np.int64(6): 6250, np.int64(7): 6250, np.int64(8): 6250, np.int64(9): 6250, np.int64(10): 6250, np.int64(11): 6250, np.int64(12): 6250, np.int64(13): 6250, np.int64(14): 6250, np.int64(15): 6250, np.int64(16): 6250, np.int64(17): 6250, np.int64(18): 6250, np.int64(19): 6250, np.int64(20): 6250, np.int64(21): 6250, np.int64(22): 6250, np.int64(23): 6250, np.int64(24): 6250, np.int64(25): 6250, np.int64(26): 6250, np.int64(27): 6250, np.int64(28): 6250, np.int64(29): 6250, np.int64(30): 6250, np.int64(31): 6250})\n",
      "Finishing upsampling.\n",
      "\n",
      "Epoch 1/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m115s\u001b[0m 18ms/step - accuracy: 0.8614 - loss: 0.5435 - val_accuracy: 1.0000 - val_loss: 4.5996e-05\n",
      "Epoch 2/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m113s\u001b[0m 18ms/step - accuracy: 0.9767 - loss: 0.0764 - val_accuracy: 1.0000 - val_loss: 4.5024e-05\n",
      "Epoch 3/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m115s\u001b[0m 19ms/step - accuracy: 0.9876 - loss: 0.0392 - val_accuracy: 1.0000 - val_loss: 1.8717e-06\n",
      "Epoch 4/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m120s\u001b[0m 19ms/step - accuracy: 0.9924 - loss: 0.0240 - val_accuracy: 1.0000 - val_loss: 5.2270e-06\n",
      "Epoch 5/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m127s\u001b[0m 21ms/step - accuracy: 0.9943 - loss: 0.0186 - val_accuracy: 1.0000 - val_loss: 8.4410e-07\n",
      "Epoch 6/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m120s\u001b[0m 19ms/step - accuracy: 0.9953 - loss: 0.0149 - val_accuracy: 1.0000 - val_loss: 8.1301e-08\n",
      "Epoch 7/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m118s\u001b[0m 19ms/step - accuracy: 0.9961 - loss: 0.0122 - val_accuracy: 1.0000 - val_loss: 9.7893e-07\n",
      "Epoch 8/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m128s\u001b[0m 21ms/step - accuracy: 0.9966 - loss: 0.0115 - val_accuracy: 1.0000 - val_loss: 9.9604e-07\n",
      "Epoch 9/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m133s\u001b[0m 21ms/step - accuracy: 0.9968 - loss: 0.0104 - val_accuracy: 1.0000 - val_loss: 2.2906e-07\n",
      "Epoch 10/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m135s\u001b[0m 22ms/step - accuracy: 0.9971 - loss: 0.0090 - val_accuracy: 1.0000 - val_loss: 4.0668e-07\n",
      "Epoch 11/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m117s\u001b[0m 19ms/step - accuracy: 0.9972 - loss: 0.0081 - val_accuracy: 1.0000 - val_loss: 1.1021e-07\n",
      "Epoch 12/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m121s\u001b[0m 20ms/step - accuracy: 0.9973 - loss: 0.0083 - val_accuracy: 1.0000 - val_loss: 1.0550e-07\n",
      "Epoch 13/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m128s\u001b[0m 21ms/step - accuracy: 0.9978 - loss: 0.0071 - val_accuracy: 1.0000 - val_loss: 5.7817e-09\n",
      "Epoch 14/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m137s\u001b[0m 22ms/step - accuracy: 0.9979 - loss: 0.0070 - val_accuracy: 1.0000 - val_loss: 5.9605e-09\n",
      "Epoch 15/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m132s\u001b[0m 21ms/step - accuracy: 0.9982 - loss: 0.0060 - val_accuracy: 1.0000 - val_loss: 1.6278e-07\n",
      "Epoch 16/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m119s\u001b[0m 19ms/step - accuracy: 0.9981 - loss: 0.0057 - val_accuracy: 1.0000 - val_loss: 5.8055e-08\n",
      "Epoch 17/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m125s\u001b[0m 20ms/step - accuracy: 0.9982 - loss: 0.0056 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 18/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m132s\u001b[0m 21ms/step - accuracy: 0.9985 - loss: 0.0049 - val_accuracy: 1.0000 - val_loss: 3.3379e-09\n",
      "Epoch 19/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m132s\u001b[0m 21ms/step - accuracy: 0.9985 - loss: 0.0050 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 20/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m120s\u001b[0m 19ms/step - accuracy: 0.9988 - loss: 0.0039 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 21/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m128s\u001b[0m 21ms/step - accuracy: 0.9983 - loss: 0.0054 - val_accuracy: 1.0000 - val_loss: 4.1783e-08\n",
      "Epoch 22/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m138s\u001b[0m 22ms/step - accuracy: 0.9982 - loss: 0.0058 - val_accuracy: 1.0000 - val_loss: 9.5367e-09\n",
      "Epoch 23/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 23ms/step - accuracy: 0.9985 - loss: 0.0049 - val_accuracy: 1.0000 - val_loss: 7.1357e-07\n",
      "Epoch 24/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m132s\u001b[0m 21ms/step - accuracy: 0.9986 - loss: 0.0044 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 25/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m121s\u001b[0m 20ms/step - accuracy: 0.9989 - loss: 0.0035 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 26/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m128s\u001b[0m 21ms/step - accuracy: 0.9988 - loss: 0.0038 - val_accuracy: 1.0000 - val_loss: 5.5149e-07\n",
      "Epoch 27/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m139s\u001b[0m 22ms/step - accuracy: 0.9987 - loss: 0.0042 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 27: early stopping\n",
      "Restoring model weights from the end of the best epoch: 17.\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step\n",
      "\n",
      "Training fold 2\n",
      "\n",
      "Generating upsampling using SMOTE...\n",
      "200000 samples after upsampling.\n",
      "Class distribution for training after upsampling: Counter({np.int64(0): 6250, np.int64(1): 6250, np.int64(2): 6250, np.int64(3): 6250, np.int64(4): 6250, np.int64(5): 6250, np.int64(6): 6250, np.int64(7): 6250, np.int64(8): 6250, np.int64(9): 6250, np.int64(10): 6250, np.int64(11): 6250, np.int64(12): 6250, np.int64(13): 6250, np.int64(14): 6250, np.int64(15): 6250, np.int64(16): 6250, np.int64(17): 6250, np.int64(18): 6250, np.int64(19): 6250, np.int64(20): 6250, np.int64(21): 6250, np.int64(22): 6250, np.int64(23): 6250, np.int64(24): 6250, np.int64(25): 6250, np.int64(26): 6250, np.int64(27): 6250, np.int64(28): 6250, np.int64(29): 6250, np.int64(30): 6250, np.int64(31): 6250})\n",
      "Finishing upsampling.\n",
      "\n",
      "Epoch 1/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m138s\u001b[0m 22ms/step - accuracy: 0.9727 - loss: 0.1509 - val_accuracy: 1.0000 - val_loss: 1.0145e-07\n",
      "Epoch 2/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m138s\u001b[0m 22ms/step - accuracy: 0.9947 - loss: 0.0174 - val_accuracy: 1.0000 - val_loss: 1.3188e-06\n",
      "Epoch 3/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m122s\u001b[0m 20ms/step - accuracy: 0.9973 - loss: 0.0094 - val_accuracy: 1.0000 - val_loss: 6.0797e-09\n",
      "Epoch 4/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m128s\u001b[0m 21ms/step - accuracy: 0.9976 - loss: 0.0088 - val_accuracy: 1.0000 - val_loss: 1.0101e-06\n",
      "Epoch 5/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m138s\u001b[0m 22ms/step - accuracy: 0.9980 - loss: 0.0063 - val_accuracy: 1.0000 - val_loss: 2.4563e-07\n",
      "Epoch 6/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m130s\u001b[0m 21ms/step - accuracy: 0.9983 - loss: 0.0056 - val_accuracy: 1.0000 - val_loss: 6.4969e-09\n",
      "Epoch 7/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m127s\u001b[0m 20ms/step - accuracy: 0.9985 - loss: 0.0048 - val_accuracy: 1.0000 - val_loss: 1.5731e-06\n",
      "Epoch 8/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m134s\u001b[0m 22ms/step - accuracy: 0.9982 - loss: 0.0056 - val_accuracy: 1.0000 - val_loss: 1.1921e-10\n",
      "Epoch 9/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 23ms/step - accuracy: 0.9985 - loss: 0.0050 - val_accuracy: 1.0000 - val_loss: 3.6359e-09\n",
      "Epoch 10/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m131s\u001b[0m 21ms/step - accuracy: 0.9983 - loss: 0.0065 - val_accuracy: 1.0000 - val_loss: 5.9605e-11\n",
      "Epoch 11/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m126s\u001b[0m 20ms/step - accuracy: 0.9987 - loss: 0.0042 - val_accuracy: 1.0000 - val_loss: 1.7285e-09\n",
      "Epoch 12/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m135s\u001b[0m 22ms/step - accuracy: 0.9988 - loss: 0.0042 - val_accuracy: 1.0000 - val_loss: 2.5274e-06\n",
      "Epoch 13/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 23ms/step - accuracy: 0.9988 - loss: 0.0037 - val_accuracy: 1.0000 - val_loss: 2.5570e-08\n",
      "Epoch 14/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m147s\u001b[0m 24ms/step - accuracy: 0.9987 - loss: 0.0035 - val_accuracy: 1.0000 - val_loss: 6.5565e-10\n",
      "Epoch 15/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m127s\u001b[0m 20ms/step - accuracy: 0.9988 - loss: 0.0042 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 16/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m129s\u001b[0m 21ms/step - accuracy: 0.9989 - loss: 0.0041 - val_accuracy: 1.0000 - val_loss: 2.0862e-09\n",
      "Epoch 17/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m137s\u001b[0m 22ms/step - accuracy: 0.9990 - loss: 0.0035 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 18/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 23ms/step - accuracy: 0.9987 - loss: 0.0040 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 19/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m128s\u001b[0m 21ms/step - accuracy: 0.9987 - loss: 0.0040 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 20/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m133s\u001b[0m 21ms/step - accuracy: 0.9990 - loss: 0.0031 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 21/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 23ms/step - accuracy: 0.9992 - loss: 0.0025 - val_accuracy: 1.0000 - val_loss: 2.2054e-09\n",
      "Epoch 22/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m145s\u001b[0m 23ms/step - accuracy: 0.9990 - loss: 0.0037 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 23/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m125s\u001b[0m 20ms/step - accuracy: 0.9991 - loss: 0.0032 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 24/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m134s\u001b[0m 22ms/step - accuracy: 0.9990 - loss: 0.0029 - val_accuracy: 1.0000 - val_loss: 1.1921e-10\n",
      "Epoch 25/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 23ms/step - accuracy: 0.9991 - loss: 0.0037 - val_accuracy: 1.0000 - val_loss: 2.3127e-08\n",
      "Epoch 25: early stopping\n",
      "Restoring model weights from the end of the best epoch: 15.\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "\n",
      "Training fold 3\n",
      "\n",
      "Generating upsampling using SMOTE...\n",
      "200000 samples after upsampling.\n",
      "Class distribution for training after upsampling: Counter({np.int64(0): 6250, np.int64(1): 6250, np.int64(2): 6250, np.int64(3): 6250, np.int64(4): 6250, np.int64(5): 6250, np.int64(6): 6250, np.int64(7): 6250, np.int64(8): 6250, np.int64(9): 6250, np.int64(10): 6250, np.int64(11): 6250, np.int64(12): 6250, np.int64(13): 6250, np.int64(14): 6250, np.int64(15): 6250, np.int64(16): 6250, np.int64(17): 6250, np.int64(18): 6250, np.int64(19): 6250, np.int64(20): 6250, np.int64(21): 6250, np.int64(22): 6250, np.int64(23): 6250, np.int64(24): 6250, np.int64(25): 6250, np.int64(26): 6250, np.int64(27): 6250, np.int64(28): 6250, np.int64(29): 6250, np.int64(30): 6250, np.int64(31): 6250})\n",
      "Finishing upsampling.\n",
      "\n",
      "Epoch 1/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m125s\u001b[0m 20ms/step - accuracy: 0.9883 - loss: 0.0533 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 2/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m125s\u001b[0m 20ms/step - accuracy: 0.9973 - loss: 0.0087 - val_accuracy: 1.0000 - val_loss: 9.5128e-08\n",
      "Epoch 3/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m132s\u001b[0m 21ms/step - accuracy: 0.9981 - loss: 0.0069 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 4/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m139s\u001b[0m 23ms/step - accuracy: 0.9984 - loss: 0.0062 - val_accuracy: 1.0000 - val_loss: 1.2279e-08\n",
      "Epoch 5/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m151s\u001b[0m 24ms/step - accuracy: 0.9988 - loss: 0.0046 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 6/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m130s\u001b[0m 21ms/step - accuracy: 0.9989 - loss: 0.0045 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 7/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m138s\u001b[0m 22ms/step - accuracy: 0.9986 - loss: 0.0048 - val_accuracy: 1.0000 - val_loss: 2.3842e-10\n",
      "Epoch 8/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m148s\u001b[0m 24ms/step - accuracy: 0.9991 - loss: 0.0032 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 9/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m129s\u001b[0m 21ms/step - accuracy: 0.9989 - loss: 0.0041 - val_accuracy: 1.0000 - val_loss: 4.0531e-08\n",
      "Epoch 10/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m136s\u001b[0m 22ms/step - accuracy: 0.9989 - loss: 0.0036 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 11/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m145s\u001b[0m 23ms/step - accuracy: 0.9991 - loss: 0.0040 - val_accuracy: 1.0000 - val_loss: 4.2081e-08\n",
      "Epoch 11: early stopping\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\n",
      "Training fold 4\n",
      "\n",
      "Generating upsampling using SMOTE...\n",
      "200000 samples after upsampling.\n",
      "Class distribution for training after upsampling: Counter({np.int64(0): 6250, np.int64(1): 6250, np.int64(2): 6250, np.int64(3): 6250, np.int64(4): 6250, np.int64(5): 6250, np.int64(6): 6250, np.int64(7): 6250, np.int64(8): 6250, np.int64(9): 6250, np.int64(10): 6250, np.int64(11): 6250, np.int64(12): 6250, np.int64(13): 6250, np.int64(14): 6250, np.int64(15): 6250, np.int64(16): 6250, np.int64(17): 6250, np.int64(18): 6250, np.int64(19): 6250, np.int64(20): 6250, np.int64(21): 6250, np.int64(22): 6250, np.int64(23): 6250, np.int64(24): 6250, np.int64(25): 6250, np.int64(26): 6250, np.int64(27): 6250, np.int64(28): 6250, np.int64(29): 6250, np.int64(30): 6250, np.int64(31): 6250})\n",
      "Finishing upsampling.\n",
      "\n",
      "Epoch 1/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m132s\u001b[0m 21ms/step - accuracy: 0.9921 - loss: 0.0300 - val_accuracy: 1.0000 - val_loss: 1.0490e-08\n",
      "Epoch 2/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m123s\u001b[0m 20ms/step - accuracy: 0.9975 - loss: 0.0090 - val_accuracy: 1.0000 - val_loss: 7.9870e-09\n",
      "Epoch 3/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m122s\u001b[0m 20ms/step - accuracy: 0.9984 - loss: 0.0062 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 4/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m126s\u001b[0m 20ms/step - accuracy: 0.9982 - loss: 0.0060 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 5/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m134s\u001b[0m 22ms/step - accuracy: 0.9986 - loss: 0.0053 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 6/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m137s\u001b[0m 22ms/step - accuracy: 0.9988 - loss: 0.0056 - val_accuracy: 1.0000 - val_loss: 1.9670e-09\n",
      "Epoch 7/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m131s\u001b[0m 21ms/step - accuracy: 0.9987 - loss: 0.0050 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 8/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m130s\u001b[0m 21ms/step - accuracy: 0.9989 - loss: 0.0039 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 9/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m139s\u001b[0m 23ms/step - accuracy: 0.9989 - loss: 0.0038 - val_accuracy: 1.0000 - val_loss: 1.4901e-09\n",
      "Epoch 10/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m125s\u001b[0m 20ms/step - accuracy: 0.9991 - loss: 0.0031 - val_accuracy: 1.0000 - val_loss: 7.1526e-10\n",
      "Epoch 11/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m128s\u001b[0m 21ms/step - accuracy: 0.9988 - loss: 0.0041 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 12/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m133s\u001b[0m 21ms/step - accuracy: 0.9990 - loss: 0.0039 - val_accuracy: 1.0000 - val_loss: 3.4123e-06\n",
      "Epoch 13/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m133s\u001b[0m 21ms/step - accuracy: 0.9989 - loss: 0.0045 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 13: early stopping\n",
      "Restoring model weights from the end of the best epoch: 3.\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\n",
      "Training fold 5\n",
      "\n",
      "Generating upsampling using SMOTE...\n",
      "200000 samples after upsampling.\n",
      "Class distribution for training after upsampling: Counter({np.int64(0): 6250, np.int64(1): 6250, np.int64(2): 6250, np.int64(3): 6250, np.int64(4): 6250, np.int64(5): 6250, np.int64(6): 6250, np.int64(7): 6250, np.int64(8): 6250, np.int64(9): 6250, np.int64(10): 6250, np.int64(11): 6250, np.int64(12): 6250, np.int64(13): 6250, np.int64(14): 6250, np.int64(15): 6250, np.int64(16): 6250, np.int64(17): 6250, np.int64(18): 6250, np.int64(19): 6250, np.int64(20): 6250, np.int64(21): 6250, np.int64(22): 6250, np.int64(23): 6250, np.int64(24): 6250, np.int64(25): 6250, np.int64(26): 6250, np.int64(27): 6250, np.int64(28): 6250, np.int64(29): 6250, np.int64(30): 6250, np.int64(31): 6250})\n",
      "Finishing upsampling.\n",
      "\n",
      "Epoch 1/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m130s\u001b[0m 21ms/step - accuracy: 0.9944 - loss: 0.0191 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 2/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m132s\u001b[0m 21ms/step - accuracy: 0.9980 - loss: 0.0084 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 3/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m138s\u001b[0m 22ms/step - accuracy: 0.9987 - loss: 0.0042 - val_accuracy: 1.0000 - val_loss: 0.0010\n",
      "Epoch 4/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m128s\u001b[0m 21ms/step - accuracy: 0.9986 - loss: 0.0049 - val_accuracy: 1.0000 - val_loss: 4.1723e-09\n",
      "Epoch 5/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m124s\u001b[0m 20ms/step - accuracy: 0.9989 - loss: 0.0042 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 6/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m127s\u001b[0m 21ms/step - accuracy: 0.9990 - loss: 0.0042 - val_accuracy: 1.0000 - val_loss: 1.9073e-08\n",
      "Epoch 7/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m130s\u001b[0m 21ms/step - accuracy: 0.9989 - loss: 0.0047 - val_accuracy: 1.0000 - val_loss: 2.9802e-08\n",
      "Epoch 8/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m136s\u001b[0m 22ms/step - accuracy: 0.9988 - loss: 0.0045 - val_accuracy: 1.0000 - val_loss: 1.7596e-06\n",
      "Epoch 9/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 23ms/step - accuracy: 0.9991 - loss: 0.0031 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 10/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m191s\u001b[0m 31ms/step - accuracy: 0.9992 - loss: 0.0031 - val_accuracy: 1.0000 - val_loss: 2.9802e-10\n",
      "Epoch 11/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m167s\u001b[0m 27ms/step - accuracy: 0.9991 - loss: 0.0027 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 11: early stopping\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step\n",
      "\n",
      "Training fold 6\n",
      "\n",
      "Generating upsampling using SMOTE...\n",
      "200000 samples after upsampling.\n",
      "Class distribution for training after upsampling: Counter({np.int64(0): 6250, np.int64(1): 6250, np.int64(2): 6250, np.int64(3): 6250, np.int64(4): 6250, np.int64(5): 6250, np.int64(6): 6250, np.int64(7): 6250, np.int64(8): 6250, np.int64(9): 6250, np.int64(10): 6250, np.int64(11): 6250, np.int64(12): 6250, np.int64(13): 6250, np.int64(14): 6250, np.int64(15): 6250, np.int64(16): 6250, np.int64(17): 6250, np.int64(18): 6250, np.int64(19): 6250, np.int64(20): 6250, np.int64(21): 6250, np.int64(22): 6250, np.int64(23): 6250, np.int64(24): 6250, np.int64(25): 6250, np.int64(26): 6250, np.int64(27): 6250, np.int64(28): 6250, np.int64(29): 6250, np.int64(30): 6250, np.int64(31): 6250})\n",
      "Finishing upsampling.\n",
      "\n",
      "Epoch 1/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m167s\u001b[0m 27ms/step - accuracy: 0.9961 - loss: 0.0130 - val_accuracy: 1.0000 - val_loss: 7.0930e-09\n",
      "Epoch 2/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m151s\u001b[0m 24ms/step - accuracy: 0.9983 - loss: 0.0052 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 3/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m165s\u001b[0m 27ms/step - accuracy: 0.9988 - loss: 0.0044 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 4/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m150s\u001b[0m 24ms/step - accuracy: 0.9989 - loss: 0.0045 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 5/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m150s\u001b[0m 24ms/step - accuracy: 0.9990 - loss: 0.0051 - val_accuracy: 1.0000 - val_loss: 6.6446e-07\n",
      "Epoch 6/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m166s\u001b[0m 27ms/step - accuracy: 0.9989 - loss: 0.0037 - val_accuracy: 1.0000 - val_loss: 4.7684e-10\n",
      "Epoch 7/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m151s\u001b[0m 24ms/step - accuracy: 0.9988 - loss: 0.0038 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 8/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m153s\u001b[0m 25ms/step - accuracy: 0.9992 - loss: 0.0029 - val_accuracy: 1.0000 - val_loss: 2.0266e-09\n",
      "Epoch 9/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m154s\u001b[0m 25ms/step - accuracy: 0.9990 - loss: 0.0037 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 10/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m151s\u001b[0m 24ms/step - accuracy: 0.9989 - loss: 0.0036 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 11/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m158s\u001b[0m 26ms/step - accuracy: 0.9992 - loss: 0.0031 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 12/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m152s\u001b[0m 25ms/step - accuracy: 0.9992 - loss: 0.0024 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 12: early stopping\n",
      "Restoring model weights from the end of the best epoch: 2.\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step\n",
      "\n",
      "Training fold 7\n",
      "\n",
      "Generating upsampling using SMOTE...\n",
      "200000 samples after upsampling.\n",
      "Class distribution for training after upsampling: Counter({np.int64(0): 6250, np.int64(1): 6250, np.int64(2): 6250, np.int64(3): 6250, np.int64(4): 6250, np.int64(5): 6250, np.int64(6): 6250, np.int64(7): 6250, np.int64(8): 6250, np.int64(9): 6250, np.int64(10): 6250, np.int64(11): 6250, np.int64(12): 6250, np.int64(13): 6250, np.int64(14): 6250, np.int64(15): 6250, np.int64(16): 6250, np.int64(17): 6250, np.int64(18): 6250, np.int64(19): 6250, np.int64(20): 6250, np.int64(21): 6250, np.int64(22): 6250, np.int64(23): 6250, np.int64(24): 6250, np.int64(25): 6250, np.int64(26): 6250, np.int64(27): 6250, np.int64(28): 6250, np.int64(29): 6250, np.int64(30): 6250, np.int64(31): 6250})\n",
      "Finishing upsampling.\n",
      "\n",
      "Epoch 1/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m156s\u001b[0m 25ms/step - accuracy: 0.9969 - loss: 0.0102 - val_accuracy: 1.0000 - val_loss: 9.9480e-08\n",
      "Epoch 2/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m160s\u001b[0m 26ms/step - accuracy: 0.9986 - loss: 0.0053 - val_accuracy: 1.0000 - val_loss: 1.4305e-09\n",
      "Epoch 3/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m154s\u001b[0m 25ms/step - accuracy: 0.9990 - loss: 0.0036 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 4/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 23ms/step - accuracy: 0.9991 - loss: 0.0036 - val_accuracy: 1.0000 - val_loss: 7.2718e-09\n",
      "Epoch 5/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m131s\u001b[0m 21ms/step - accuracy: 0.9991 - loss: 0.0033 - val_accuracy: 1.0000 - val_loss: 5.9605e-11\n",
      "Epoch 6/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m137s\u001b[0m 22ms/step - accuracy: 0.9991 - loss: 0.0027 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 7/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m134s\u001b[0m 22ms/step - accuracy: 0.9992 - loss: 0.0038 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 8/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 23ms/step - accuracy: 0.9992 - loss: 0.0029 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 9/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m145s\u001b[0m 23ms/step - accuracy: 0.9990 - loss: 0.0041 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 10/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m155s\u001b[0m 25ms/step - accuracy: 0.9992 - loss: 0.0028 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 11/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m160s\u001b[0m 26ms/step - accuracy: 0.9994 - loss: 0.0021 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 12/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m150s\u001b[0m 24ms/step - accuracy: 0.9993 - loss: 0.0025 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 13/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m179s\u001b[0m 29ms/step - accuracy: 0.9994 - loss: 0.0033 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 13: early stopping\n",
      "Restoring model weights from the end of the best epoch: 3.\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step\n",
      "\n",
      "Training fold 8\n",
      "\n",
      "Generating upsampling using SMOTE...\n",
      "200000 samples after upsampling.\n",
      "Class distribution for training after upsampling: Counter({np.int64(0): 6250, np.int64(1): 6250, np.int64(2): 6250, np.int64(3): 6250, np.int64(4): 6250, np.int64(5): 6250, np.int64(6): 6250, np.int64(7): 6250, np.int64(8): 6250, np.int64(9): 6250, np.int64(10): 6250, np.int64(11): 6250, np.int64(12): 6250, np.int64(13): 6250, np.int64(14): 6250, np.int64(15): 6250, np.int64(16): 6250, np.int64(17): 6250, np.int64(18): 6250, np.int64(19): 6250, np.int64(20): 6250, np.int64(21): 6250, np.int64(22): 6250, np.int64(23): 6250, np.int64(24): 6250, np.int64(25): 6250, np.int64(26): 6250, np.int64(27): 6250, np.int64(28): 6250, np.int64(29): 6250, np.int64(30): 6250, np.int64(31): 6250})\n",
      "Finishing upsampling.\n",
      "\n",
      "Epoch 1/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m162s\u001b[0m 26ms/step - accuracy: 0.9968 - loss: 0.0115 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 2/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m168s\u001b[0m 27ms/step - accuracy: 0.9986 - loss: 0.0048 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 3/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m174s\u001b[0m 28ms/step - accuracy: 0.9988 - loss: 0.0038 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 4/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m189s\u001b[0m 31ms/step - accuracy: 0.9992 - loss: 0.0034 - val_accuracy: 1.0000 - val_loss: 5.1856e-09\n",
      "Epoch 5/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m175s\u001b[0m 28ms/step - accuracy: 0.9993 - loss: 0.0030 - val_accuracy: 1.0000 - val_loss: 2.2054e-09\n",
      "Epoch 6/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m184s\u001b[0m 30ms/step - accuracy: 0.9990 - loss: 0.0046 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 7/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m190s\u001b[0m 31ms/step - accuracy: 0.9992 - loss: 0.0028 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 8/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m179s\u001b[0m 29ms/step - accuracy: 0.9993 - loss: 0.0024 - val_accuracy: 1.0000 - val_loss: 4.7684e-09\n",
      "Epoch 9/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m185s\u001b[0m 30ms/step - accuracy: 0.9993 - loss: 0.0027 - val_accuracy: 1.0000 - val_loss: 6.6757e-09\n",
      "Epoch 10/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m178s\u001b[0m 29ms/step - accuracy: 0.9991 - loss: 0.0031 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 11/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m192s\u001b[0m 31ms/step - accuracy: 0.9992 - loss: 0.0036 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 11: early stopping\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step\n",
      "\n",
      "Training fold 9\n",
      "\n",
      "Generating upsampling using SMOTE...\n",
      "200000 samples after upsampling.\n",
      "Class distribution for training after upsampling: Counter({np.int64(0): 6250, np.int64(1): 6250, np.int64(2): 6250, np.int64(3): 6250, np.int64(4): 6250, np.int64(5): 6250, np.int64(6): 6250, np.int64(7): 6250, np.int64(8): 6250, np.int64(9): 6250, np.int64(10): 6250, np.int64(11): 6250, np.int64(12): 6250, np.int64(13): 6250, np.int64(14): 6250, np.int64(15): 6250, np.int64(16): 6250, np.int64(17): 6250, np.int64(18): 6250, np.int64(19): 6250, np.int64(20): 6250, np.int64(21): 6250, np.int64(22): 6250, np.int64(23): 6250, np.int64(24): 6250, np.int64(25): 6250, np.int64(26): 6250, np.int64(27): 6250, np.int64(28): 6250, np.int64(29): 6250, np.int64(30): 6250, np.int64(31): 6250})\n",
      "Finishing upsampling.\n",
      "\n",
      "Epoch 1/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m200s\u001b[0m 32ms/step - accuracy: 0.9973 - loss: 0.0088 - val_accuracy: 1.0000 - val_loss: 2.0266e-09\n",
      "Epoch 2/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m198s\u001b[0m 32ms/step - accuracy: 0.9986 - loss: 0.0045 - val_accuracy: 1.0000 - val_loss: 2.1942e-05\n",
      "Epoch 3/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m192s\u001b[0m 31ms/step - accuracy: 0.9989 - loss: 0.0038 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 4/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m199s\u001b[0m 32ms/step - accuracy: 0.9992 - loss: 0.0035 - val_accuracy: 1.0000 - val_loss: 5.7711e-06\n",
      "Epoch 5/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m190s\u001b[0m 31ms/step - accuracy: 0.9990 - loss: 0.0034 - val_accuracy: 1.0000 - val_loss: 1.7881e-10\n",
      "Epoch 6/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m205s\u001b[0m 33ms/step - accuracy: 0.9990 - loss: 0.0039 - val_accuracy: 1.0000 - val_loss: 2.4676e-08\n",
      "Epoch 7/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m193s\u001b[0m 31ms/step - accuracy: 0.9993 - loss: 0.0030 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 8/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m205s\u001b[0m 33ms/step - accuracy: 0.9992 - loss: 0.0031 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 9/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m199s\u001b[0m 32ms/step - accuracy: 0.9993 - loss: 0.0031 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 10/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m202s\u001b[0m 33ms/step - accuracy: 0.9992 - loss: 0.0030 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 11/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m208s\u001b[0m 34ms/step - accuracy: 0.9992 - loss: 0.0027 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 12/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m207s\u001b[0m 33ms/step - accuracy: 0.9993 - loss: 0.0025 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 13/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m201s\u001b[0m 32ms/step - accuracy: 0.9993 - loss: 0.0024 - val_accuracy: 1.0000 - val_loss: 1.1921e-10\n",
      "Epoch 13: early stopping\n",
      "Restoring model weights from the end of the best epoch: 3.\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step\n",
      "\n",
      "Training fold 10\n",
      "\n",
      "Generating upsampling using SMOTE...\n",
      "200000 samples after upsampling.\n",
      "Class distribution for training after upsampling: Counter({np.int64(0): 6250, np.int64(1): 6250, np.int64(2): 6250, np.int64(3): 6250, np.int64(4): 6250, np.int64(5): 6250, np.int64(6): 6250, np.int64(7): 6250, np.int64(8): 6250, np.int64(9): 6250, np.int64(10): 6250, np.int64(11): 6250, np.int64(12): 6250, np.int64(13): 6250, np.int64(14): 6250, np.int64(15): 6250, np.int64(16): 6250, np.int64(17): 6250, np.int64(18): 6250, np.int64(19): 6250, np.int64(20): 6250, np.int64(21): 6250, np.int64(22): 6250, np.int64(23): 6250, np.int64(24): 6250, np.int64(25): 6250, np.int64(26): 6250, np.int64(27): 6250, np.int64(28): 6250, np.int64(29): 6250, np.int64(30): 6250, np.int64(31): 6250})\n",
      "Finishing upsampling.\n",
      "\n",
      "Epoch 1/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m208s\u001b[0m 34ms/step - accuracy: 0.9976 - loss: 0.0099 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 2/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m220s\u001b[0m 36ms/step - accuracy: 0.9991 - loss: 0.0036 - val_accuracy: 1.0000 - val_loss: 5.1856e-09\n",
      "Epoch 3/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m206s\u001b[0m 33ms/step - accuracy: 0.9988 - loss: 0.0041 - val_accuracy: 1.0000 - val_loss: 2.3097e-07\n",
      "Epoch 4/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m216s\u001b[0m 35ms/step - accuracy: 0.9993 - loss: 0.0030 - val_accuracy: 1.0000 - val_loss: 1.4305e-09\n",
      "Epoch 5/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m212s\u001b[0m 34ms/step - accuracy: 0.9993 - loss: 0.0026 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 6/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m224s\u001b[0m 36ms/step - accuracy: 0.9991 - loss: 0.0033 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 7/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m213s\u001b[0m 34ms/step - accuracy: 0.9994 - loss: 0.0026 - val_accuracy: 1.0000 - val_loss: 1.7881e-10\n",
      "Epoch 8/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m225s\u001b[0m 36ms/step - accuracy: 0.9992 - loss: 0.0031 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 9/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m213s\u001b[0m 34ms/step - accuracy: 0.9993 - loss: 0.0024 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 10/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m226s\u001b[0m 37ms/step - accuracy: 0.9993 - loss: 0.0027 - val_accuracy: 1.0000 - val_loss: 2.3425e-08\n",
      "Epoch 11/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m219s\u001b[0m 35ms/step - accuracy: 0.9992 - loss: 0.0023 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 11: early stopping\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step\n",
      "\n",
      "Time taken for training:  07:14:14\n",
      "\n",
      "\n",
      "Fold 1 - Train Accuracy 0.9995 - Test Accuracy 0.5623\n",
      "Fold 2 - Train Accuracy 0.9996 - Test Accuracy 0.7767\n",
      "Fold 3 - Train Accuracy 0.9987 - Test Accuracy 0.9429\n",
      "Fold 4 - Train Accuracy 0.9996 - Test Accuracy 0.9363\n",
      "Fold 5 - Train Accuracy 0.9994 - Test Accuracy 0.9720\n",
      "Fold 6 - Train Accuracy 0.9994 - Test Accuracy 0.9709\n",
      "Fold 7 - Train Accuracy 0.9998 - Test Accuracy 0.9720\n",
      "Fold 8 - Train Accuracy 0.9996 - Test Accuracy 0.9810\n",
      "Fold 9 - Train Accuracy 0.9998 - Test Accuracy 0.9837\n",
      "Fold 10 - Train Accuracy 0.9997 - Test Accuracy 0.9864\n",
      "\n",
      "Mean Train Accuracy: 0.9995 - Std: 0.0003 \n",
      "Mean Test Accuracy: 0.9084 - Std: 0.1297 \n",
      "\n",
      "Evaluate other metrics:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.94      0.94     13905\n",
      "           1       0.93      0.95      0.94       723\n",
      "           2       0.96      0.97      0.96      2659\n",
      "           3       0.84      0.80      0.82      1123\n",
      "           4       0.88      0.82      0.85       384\n",
      "           5       0.95      0.82      0.88        44\n",
      "           6       0.90      0.85      0.87       663\n",
      "           7       0.87      0.89      0.88        94\n",
      "           8       0.71      0.83      0.77        12\n",
      "           9       0.85      0.84      0.85       786\n",
      "          10       0.88      0.78      0.82         9\n",
      "          11       0.96      0.81      0.88        98\n",
      "          12       1.00      0.88      0.93         8\n",
      "          13       1.00      0.80      0.89        20\n",
      "          14       0.96      0.84      0.90        77\n",
      "          15       0.92      0.92      0.92        62\n",
      "          16       0.83      0.82      0.82       917\n",
      "          17       0.88      0.89      0.89       473\n",
      "          18       0.86      0.86      0.86        22\n",
      "          19       0.94      0.83      0.88       122\n",
      "          20       0.91      0.86      0.88       111\n",
      "          21       0.85      0.85      0.85       201\n",
      "          22       0.58      1.00      0.74         7\n",
      "          23       0.81      0.82      0.81        96\n",
      "          24       0.84      0.81      0.83      1045\n",
      "          25       0.84      0.83      0.83       540\n",
      "          26       0.85      0.81      0.83      1334\n",
      "          27       0.86      0.86      0.86        28\n",
      "          28       0.92      0.94      0.93        35\n",
      "          29       0.96      0.83      0.89        77\n",
      "          30       0.92      0.84      0.88        64\n",
      "          31       0.75      0.86      0.80         7\n",
      "\n",
      "    accuracy                           0.91     25746\n",
      "   macro avg       0.88      0.86      0.86     25746\n",
      "weighted avg       0.91      0.91      0.91     25746\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Build a \"time window\" structure to handle the dataset as a time series.\n",
    "new_df = load_dataset(\"V3\")\n",
    "X_array, y_array = build_time_window_structure(new_df, \"V3\")\n",
    "X_array, y_array = remove_classes_with_less_samples(X_array, y_array)\n",
    "\n",
    "# Train the CNN model.\n",
    "v1_model = create_v1()\n",
    "v1_num_epochs = 300\n",
    "v1_batch_size = 32\n",
    "v1_validation_split = 0.01\n",
    "\n",
    "training_history_v1 = train_cnn_model(v1_model, X_array, y_array, v1_num_epochs, v1_batch_size, v1_validation_split, \"v1_model_V3.keras\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e5c04d8-ecce-4f37-a589-6241f436d375",
   "metadata": {},
   "source": [
    "#### V4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "46d5bb04-8192-4d41-a718-524175becc8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Start loading CSV file...\n",
      "Finish loading CSV file.\n",
      "\n",
      "Starting build_time_window_structure function...\n",
      "\n",
      "Shape of features:  (25770, 1250)\n",
      "Quantity os samples (labels):  25770\n",
      "\n",
      "Finishing build_time_window_structure function.\n",
      "\n",
      "Remove classes with less than 6 samples.\n",
      "\n",
      "Show classes identification:\n",
      "Class: 0 - Arrhythmia code: 1\n",
      "Class: 1 - Arrhythmia code: 21\n",
      "Class: 2 - Arrhythmia code: 22\n",
      "Class: 3 - Arrhythmia code: 23\n",
      "Class: 4 - Arrhythmia code: 30\n",
      "Class: 5 - Arrhythmia code: 36\n",
      "Class: 6 - Arrhythmia code: 50\n",
      "Class: 7 - Arrhythmia code: 51\n",
      "Class: 8 - Arrhythmia code: 54\n",
      "Class: 9 - Arrhythmia code: 60\n",
      "Class: 10 - Arrhythmia code: 80\n",
      "Class: 11 - Arrhythmia code: 82\n",
      "Class: 12 - Arrhythmia code: 83\n",
      "Class: 13 - Arrhythmia code: 88\n",
      "Class: 14 - Arrhythmia code: 101\n",
      "Class: 15 - Arrhythmia code: 104\n",
      "Class: 16 - Arrhythmia code: 105\n",
      "Class: 17 - Arrhythmia code: 106\n",
      "Class: 18 - Arrhythmia code: 108\n",
      "Class: 19 - Arrhythmia code: 120\n",
      "Class: 20 - Arrhythmia code: 121\n",
      "Class: 21 - Arrhythmia code: 125\n",
      "Class: 22 - Arrhythmia code: 140\n",
      "Class: 23 - Arrhythmia code: 142\n",
      "Class: 24 - Arrhythmia code: 145\n",
      "Class: 25 - Arrhythmia code: 146\n",
      "Class: 26 - Arrhythmia code: 147\n",
      "Class: 27 - Arrhythmia code: 155\n",
      "Class: 28 - Arrhythmia code: 160\n",
      "Class: 29 - Arrhythmia code: 161\n",
      "Class: 30 - Arrhythmia code: 165\n",
      "Class: 31 - Arrhythmia code: 166\n",
      "\n",
      "Check for dataset balance:\n",
      "Class =   0   Qty =    13905   Percentage = 54.01 %\n",
      "Class =   2   Qty =     2659   Percentage = 10.33 %\n",
      "Class =  26   Qty =     1334   Percentage = 5.18 %\n",
      "Class =   3   Qty =     1123   Percentage = 4.36 %\n",
      "Class =  24   Qty =     1045   Percentage = 4.06 %\n",
      "Class =  16   Qty =      917   Percentage = 3.56 %\n",
      "Class =   9   Qty =      786   Percentage = 3.05 %\n",
      "Class =   1   Qty =      723   Percentage = 2.81 %\n",
      "Class =   6   Qty =      663   Percentage = 2.58 %\n",
      "Class =  25   Qty =      540   Percentage = 2.10 %\n",
      "Class =  17   Qty =      473   Percentage = 1.84 %\n",
      "Class =   4   Qty =      384   Percentage = 1.49 %\n",
      "Class =  21   Qty =      201   Percentage = 0.78 %\n",
      "Class =  19   Qty =      122   Percentage = 0.47 %\n",
      "Class =  20   Qty =      111   Percentage = 0.43 %\n",
      "Class =  11   Qty =       98   Percentage = 0.38 %\n",
      "Class =  23   Qty =       96   Percentage = 0.37 %\n",
      "Class =   7   Qty =       94   Percentage = 0.37 %\n",
      "Class =  14   Qty =       77   Percentage = 0.30 %\n",
      "Class =  29   Qty =       77   Percentage = 0.30 %\n",
      "Class =  30   Qty =       64   Percentage = 0.25 %\n",
      "Class =  15   Qty =       62   Percentage = 0.24 %\n",
      "Class =   5   Qty =       44   Percentage = 0.17 %\n",
      "Class =  28   Qty =       35   Percentage = 0.14 %\n",
      "Class =  27   Qty =       28   Percentage = 0.11 %\n",
      "Class =  18   Qty =       22   Percentage = 0.09 %\n",
      "Class =  13   Qty =       20   Percentage = 0.08 %\n",
      "Class =   8   Qty =       12   Percentage = 0.05 %\n",
      "Class =  10   Qty =        9   Percentage = 0.03 %\n",
      "Class =  12   Qty =        8   Percentage = 0.03 %\n",
      "Class =  22   Qty =        7   Percentage = 0.03 %\n",
      "Class =  31   Qty =        7   Percentage = 0.03 %\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1248</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)        │            <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1248</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)        │            <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1246</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)        │           <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1246</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)        │            <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">623</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">619</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)        │           <span style=\"color: #00af00; text-decoration-color: #00af00\">656</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_2           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">619</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)        │            <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">615</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)        │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,296</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_3           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">615</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)        │            <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">307</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4912</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">628,864</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_4           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,128</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv1d (\u001b[38;5;33mConv1D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1248\u001b[0m, \u001b[38;5;34m8\u001b[0m)        │            \u001b[38;5;34m32\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1248\u001b[0m, \u001b[38;5;34m8\u001b[0m)        │            \u001b[38;5;34m32\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_1 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1246\u001b[0m, \u001b[38;5;34m8\u001b[0m)        │           \u001b[38;5;34m200\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_1           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1246\u001b[0m, \u001b[38;5;34m8\u001b[0m)        │            \u001b[38;5;34m32\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d (\u001b[38;5;33mMaxPooling1D\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m623\u001b[0m, \u001b[38;5;34m8\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_2 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m619\u001b[0m, \u001b[38;5;34m16\u001b[0m)        │           \u001b[38;5;34m656\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_2           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m619\u001b[0m, \u001b[38;5;34m16\u001b[0m)        │            \u001b[38;5;34m64\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_3 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m615\u001b[0m, \u001b[38;5;34m16\u001b[0m)        │         \u001b[38;5;34m1,296\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_3           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m615\u001b[0m, \u001b[38;5;34m16\u001b[0m)        │            \u001b[38;5;34m64\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_1 (\u001b[38;5;33mMaxPooling1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m307\u001b[0m, \u001b[38;5;34m16\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4912\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │       \u001b[38;5;34m628,864\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_4           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │           \u001b[38;5;34m512\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │         \u001b[38;5;34m4,128\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">635,880</span> (2.43 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m635,880\u001b[0m (2.43 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">635,528</span> (2.42 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m635,528\u001b[0m (2.42 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">352</span> (1.38 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m352\u001b[0m (1.38 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting training...\n",
      "\n",
      "Training fold 1\n",
      "\n",
      "Generating upsampling using SMOTE...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\DeveloperTools\\python\\3.11.0\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:737: UserWarning: The least populated class in y has only 7 members, which is less than n_splits=10.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200000 samples after upsampling.\n",
      "Class distribution for training after upsampling: Counter({np.int64(0): 6250, np.int64(1): 6250, np.int64(2): 6250, np.int64(3): 6250, np.int64(4): 6250, np.int64(5): 6250, np.int64(6): 6250, np.int64(7): 6250, np.int64(8): 6250, np.int64(9): 6250, np.int64(10): 6250, np.int64(11): 6250, np.int64(12): 6250, np.int64(13): 6250, np.int64(14): 6250, np.int64(15): 6250, np.int64(16): 6250, np.int64(17): 6250, np.int64(18): 6250, np.int64(19): 6250, np.int64(20): 6250, np.int64(21): 6250, np.int64(22): 6250, np.int64(23): 6250, np.int64(24): 6250, np.int64(25): 6250, np.int64(26): 6250, np.int64(27): 6250, np.int64(28): 6250, np.int64(29): 6250, np.int64(30): 6250, np.int64(31): 6250})\n",
      "Finishing upsampling.\n",
      "\n",
      "Epoch 1/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m111s\u001b[0m 17ms/step - accuracy: 0.8706 - loss: 0.5080 - val_accuracy: 1.0000 - val_loss: 3.1178e-05\n",
      "Epoch 2/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 17ms/step - accuracy: 0.9761 - loss: 0.0761 - val_accuracy: 1.0000 - val_loss: 2.5885e-05\n",
      "Epoch 3/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 17ms/step - accuracy: 0.9879 - loss: 0.0382 - val_accuracy: 1.0000 - val_loss: 4.4484e-06\n",
      "Epoch 4/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m103s\u001b[0m 17ms/step - accuracy: 0.9919 - loss: 0.0257 - val_accuracy: 1.0000 - val_loss: 2.0296e-05\n",
      "Epoch 5/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 17ms/step - accuracy: 0.9940 - loss: 0.0192 - val_accuracy: 1.0000 - val_loss: 4.7033e-06\n",
      "Epoch 6/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 17ms/step - accuracy: 0.9947 - loss: 0.0159 - val_accuracy: 1.0000 - val_loss: 3.3353e-04\n",
      "Epoch 7/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 17ms/step - accuracy: 0.9961 - loss: 0.0122 - val_accuracy: 1.0000 - val_loss: 1.1959e-06\n",
      "Epoch 8/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 17ms/step - accuracy: 0.9965 - loss: 0.0106 - val_accuracy: 1.0000 - val_loss: 6.7120e-07\n",
      "Epoch 9/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 17ms/step - accuracy: 0.9968 - loss: 0.0104 - val_accuracy: 1.0000 - val_loss: 1.7285e-09\n",
      "Epoch 10/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 17ms/step - accuracy: 0.9971 - loss: 0.0096 - val_accuracy: 1.0000 - val_loss: 2.1458e-08\n",
      "Epoch 11/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 17ms/step - accuracy: 0.9975 - loss: 0.0081 - val_accuracy: 1.0000 - val_loss: 3.1864e-07\n",
      "Epoch 12/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 17ms/step - accuracy: 0.9973 - loss: 0.0081 - val_accuracy: 1.0000 - val_loss: 2.4164e-07\n",
      "Epoch 13/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 18ms/step - accuracy: 0.9972 - loss: 0.0098 - val_accuracy: 1.0000 - val_loss: 5.7817e-09\n",
      "Epoch 14/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 18ms/step - accuracy: 0.9980 - loss: 0.0064 - val_accuracy: 1.0000 - val_loss: 2.3246e-09\n",
      "Epoch 15/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 18ms/step - accuracy: 0.9979 - loss: 0.0066 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 16/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 18ms/step - accuracy: 0.9981 - loss: 0.0062 - val_accuracy: 1.0000 - val_loss: 8.7618e-06\n",
      "Epoch 17/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m110s\u001b[0m 18ms/step - accuracy: 0.9983 - loss: 0.0056 - val_accuracy: 1.0000 - val_loss: 5.4823e-07\n",
      "Epoch 18/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m112s\u001b[0m 18ms/step - accuracy: 0.9985 - loss: 0.0050 - val_accuracy: 1.0000 - val_loss: 1.3536e-07\n",
      "Epoch 19/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m110s\u001b[0m 18ms/step - accuracy: 0.9982 - loss: 0.0060 - val_accuracy: 1.0000 - val_loss: 1.7405e-08\n",
      "Epoch 20/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m114s\u001b[0m 18ms/step - accuracy: 0.9983 - loss: 0.0054 - val_accuracy: 1.0000 - val_loss: 8.3446e-10\n",
      "Epoch 21/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m118s\u001b[0m 19ms/step - accuracy: 0.9986 - loss: 0.0043 - val_accuracy: 1.0000 - val_loss: 6.3181e-09\n",
      "Epoch 22/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 18ms/step - accuracy: 0.9988 - loss: 0.0045 - val_accuracy: 1.0000 - val_loss: 5.7029e-07\n",
      "Epoch 23/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m111s\u001b[0m 18ms/step - accuracy: 0.9986 - loss: 0.0045 - val_accuracy: 1.0000 - val_loss: 1.4305e-09\n",
      "Epoch 24/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m111s\u001b[0m 18ms/step - accuracy: 0.9988 - loss: 0.0036 - val_accuracy: 1.0000 - val_loss: 3.3915e-08\n",
      "Epoch 25/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m111s\u001b[0m 18ms/step - accuracy: 0.9988 - loss: 0.0042 - val_accuracy: 1.0000 - val_loss: 4.8308e-07\n",
      "Epoch 25: early stopping\n",
      "Restoring model weights from the end of the best epoch: 15.\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step\n",
      "\n",
      "Training fold 2\n",
      "\n",
      "Generating upsampling using SMOTE...\n",
      "200000 samples after upsampling.\n",
      "Class distribution for training after upsampling: Counter({np.int64(0): 6250, np.int64(1): 6250, np.int64(2): 6250, np.int64(3): 6250, np.int64(4): 6250, np.int64(5): 6250, np.int64(6): 6250, np.int64(7): 6250, np.int64(8): 6250, np.int64(9): 6250, np.int64(10): 6250, np.int64(11): 6250, np.int64(12): 6250, np.int64(13): 6250, np.int64(14): 6250, np.int64(15): 6250, np.int64(16): 6250, np.int64(17): 6250, np.int64(18): 6250, np.int64(19): 6250, np.int64(20): 6250, np.int64(21): 6250, np.int64(22): 6250, np.int64(23): 6250, np.int64(24): 6250, np.int64(25): 6250, np.int64(26): 6250, np.int64(27): 6250, np.int64(28): 6250, np.int64(29): 6250, np.int64(30): 6250, np.int64(31): 6250})\n",
      "Finishing upsampling.\n",
      "\n",
      "Epoch 1/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m114s\u001b[0m 18ms/step - accuracy: 0.9748 - loss: 0.1270 - val_accuracy: 1.0000 - val_loss: 8.0466e-08\n",
      "Epoch 2/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m111s\u001b[0m 18ms/step - accuracy: 0.9951 - loss: 0.0170 - val_accuracy: 1.0000 - val_loss: 6.3530e-07\n",
      "Epoch 3/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m112s\u001b[0m 18ms/step - accuracy: 0.9972 - loss: 0.0092 - val_accuracy: 1.0000 - val_loss: 3.3975e-09\n",
      "Epoch 4/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m110s\u001b[0m 18ms/step - accuracy: 0.9977 - loss: 0.0071 - val_accuracy: 1.0000 - val_loss: 4.3791e-07\n",
      "Epoch 5/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m114s\u001b[0m 18ms/step - accuracy: 0.9981 - loss: 0.0063 - val_accuracy: 1.0000 - val_loss: 1.1206e-08\n",
      "Epoch 6/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m110s\u001b[0m 18ms/step - accuracy: 0.9985 - loss: 0.0051 - val_accuracy: 1.0000 - val_loss: 8.1817e-07\n",
      "Epoch 7/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m110s\u001b[0m 18ms/step - accuracy: 0.9982 - loss: 0.0058 - val_accuracy: 1.0000 - val_loss: 8.9407e-10\n",
      "Epoch 8/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m112s\u001b[0m 18ms/step - accuracy: 0.9985 - loss: 0.0053 - val_accuracy: 1.0000 - val_loss: 4.7272e-07\n",
      "Epoch 9/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m110s\u001b[0m 18ms/step - accuracy: 0.9987 - loss: 0.0045 - val_accuracy: 1.0000 - val_loss: 8.5710e-08\n",
      "Epoch 10/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m112s\u001b[0m 18ms/step - accuracy: 0.9987 - loss: 0.0048 - val_accuracy: 1.0000 - val_loss: 6.6362e-06\n",
      "Epoch 11/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m112s\u001b[0m 18ms/step - accuracy: 0.9987 - loss: 0.0041 - val_accuracy: 1.0000 - val_loss: 2.3842e-10\n",
      "Epoch 12/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m113s\u001b[0m 18ms/step - accuracy: 0.9986 - loss: 0.0051 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 13/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m113s\u001b[0m 18ms/step - accuracy: 0.9989 - loss: 0.0042 - val_accuracy: 1.0000 - val_loss: 3.5167e-09\n",
      "Epoch 14/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m117s\u001b[0m 19ms/step - accuracy: 0.9987 - loss: 0.0041 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 15/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m113s\u001b[0m 18ms/step - accuracy: 0.9990 - loss: 0.0031 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 16/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m114s\u001b[0m 18ms/step - accuracy: 0.9990 - loss: 0.0033 - val_accuracy: 1.0000 - val_loss: 2.5081e-07\n",
      "Epoch 17/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m113s\u001b[0m 18ms/step - accuracy: 0.9990 - loss: 0.0035 - val_accuracy: 1.0000 - val_loss: 8.2552e-08\n",
      "Epoch 18/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m113s\u001b[0m 18ms/step - accuracy: 0.9989 - loss: 0.0035 - val_accuracy: 1.0000 - val_loss: 3.7551e-09\n",
      "Epoch 19/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m114s\u001b[0m 18ms/step - accuracy: 0.9989 - loss: 0.0038 - val_accuracy: 1.0000 - val_loss: 7.1526e-10\n",
      "Epoch 20/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m117s\u001b[0m 19ms/step - accuracy: 0.9990 - loss: 0.0033 - val_accuracy: 1.0000 - val_loss: 2.9802e-10\n",
      "Epoch 21/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m117s\u001b[0m 19ms/step - accuracy: 0.9987 - loss: 0.0040 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 22/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m115s\u001b[0m 19ms/step - accuracy: 0.9991 - loss: 0.0028 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 22: early stopping\n",
      "Restoring model weights from the end of the best epoch: 12.\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\n",
      "Training fold 3\n",
      "\n",
      "Generating upsampling using SMOTE...\n",
      "200000 samples after upsampling.\n",
      "Class distribution for training after upsampling: Counter({np.int64(0): 6250, np.int64(1): 6250, np.int64(2): 6250, np.int64(3): 6250, np.int64(4): 6250, np.int64(5): 6250, np.int64(6): 6250, np.int64(7): 6250, np.int64(8): 6250, np.int64(9): 6250, np.int64(10): 6250, np.int64(11): 6250, np.int64(12): 6250, np.int64(13): 6250, np.int64(14): 6250, np.int64(15): 6250, np.int64(16): 6250, np.int64(17): 6250, np.int64(18): 6250, np.int64(19): 6250, np.int64(20): 6250, np.int64(21): 6250, np.int64(22): 6250, np.int64(23): 6250, np.int64(24): 6250, np.int64(25): 6250, np.int64(26): 6250, np.int64(27): 6250, np.int64(28): 6250, np.int64(29): 6250, np.int64(30): 6250, np.int64(31): 6250})\n",
      "Finishing upsampling.\n",
      "\n",
      "Epoch 1/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m116s\u001b[0m 19ms/step - accuracy: 0.9890 - loss: 0.0501 - val_accuracy: 1.0000 - val_loss: 1.3113e-08\n",
      "Epoch 2/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m117s\u001b[0m 19ms/step - accuracy: 0.9973 - loss: 0.0094 - val_accuracy: 1.0000 - val_loss: 4.2623e-07\n",
      "Epoch 3/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m117s\u001b[0m 19ms/step - accuracy: 0.9981 - loss: 0.0066 - val_accuracy: 1.0000 - val_loss: 4.1723e-10\n",
      "Epoch 4/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m120s\u001b[0m 19ms/step - accuracy: 0.9984 - loss: 0.0061 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 5/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m117s\u001b[0m 19ms/step - accuracy: 0.9988 - loss: 0.0044 - val_accuracy: 1.0000 - val_loss: 7.3910e-09\n",
      "Epoch 6/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m116s\u001b[0m 19ms/step - accuracy: 0.9987 - loss: 0.0048 - val_accuracy: 1.0000 - val_loss: 2.3210e-07\n",
      "Epoch 7/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m118s\u001b[0m 19ms/step - accuracy: 0.9987 - loss: 0.0044 - val_accuracy: 1.0000 - val_loss: 3.6359e-09\n",
      "Epoch 8/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m118s\u001b[0m 19ms/step - accuracy: 0.9990 - loss: 0.0040 - val_accuracy: 1.0000 - val_loss: 2.0266e-09\n",
      "Epoch 9/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m117s\u001b[0m 19ms/step - accuracy: 0.9986 - loss: 0.0047 - val_accuracy: 1.0000 - val_loss: 1.0133e-09\n",
      "Epoch 10/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m120s\u001b[0m 19ms/step - accuracy: 0.9990 - loss: 0.0033 - val_accuracy: 1.0000 - val_loss: 1.3548e-07\n",
      "Epoch 11/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m116s\u001b[0m 19ms/step - accuracy: 0.9989 - loss: 0.0041 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 12/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m120s\u001b[0m 19ms/step - accuracy: 0.9990 - loss: 0.0040 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 13/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m117s\u001b[0m 19ms/step - accuracy: 0.9989 - loss: 0.0047 - val_accuracy: 1.0000 - val_loss: 6.0797e-09\n",
      "Epoch 14/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m119s\u001b[0m 19ms/step - accuracy: 0.9992 - loss: 0.0029 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 14: early stopping\n",
      "Restoring model weights from the end of the best epoch: 4.\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\n",
      "Training fold 4\n",
      "\n",
      "Generating upsampling using SMOTE...\n",
      "200000 samples after upsampling.\n",
      "Class distribution for training after upsampling: Counter({np.int64(0): 6250, np.int64(1): 6250, np.int64(2): 6250, np.int64(3): 6250, np.int64(4): 6250, np.int64(5): 6250, np.int64(6): 6250, np.int64(7): 6250, np.int64(8): 6250, np.int64(9): 6250, np.int64(10): 6250, np.int64(11): 6250, np.int64(12): 6250, np.int64(13): 6250, np.int64(14): 6250, np.int64(15): 6250, np.int64(16): 6250, np.int64(17): 6250, np.int64(18): 6250, np.int64(19): 6250, np.int64(20): 6250, np.int64(21): 6250, np.int64(22): 6250, np.int64(23): 6250, np.int64(24): 6250, np.int64(25): 6250, np.int64(26): 6250, np.int64(27): 6250, np.int64(28): 6250, np.int64(29): 6250, np.int64(30): 6250, np.int64(31): 6250})\n",
      "Finishing upsampling.\n",
      "\n",
      "Epoch 1/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m123s\u001b[0m 20ms/step - accuracy: 0.9943 - loss: 0.0210 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 2/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m122s\u001b[0m 20ms/step - accuracy: 0.9979 - loss: 0.0070 - val_accuracy: 1.0000 - val_loss: 1.3292e-07\n",
      "Epoch 3/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m124s\u001b[0m 20ms/step - accuracy: 0.9985 - loss: 0.0046 - val_accuracy: 1.0000 - val_loss: 3.8147e-09\n",
      "Epoch 4/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m127s\u001b[0m 21ms/step - accuracy: 0.9987 - loss: 0.0043 - val_accuracy: 1.0000 - val_loss: 1.5855e-08\n",
      "Epoch 5/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m119s\u001b[0m 19ms/step - accuracy: 0.9985 - loss: 0.0047 - val_accuracy: 1.0000 - val_loss: 9.5367e-10\n",
      "Epoch 6/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m123s\u001b[0m 20ms/step - accuracy: 0.9988 - loss: 0.0037 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 7/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m125s\u001b[0m 20ms/step - accuracy: 0.9990 - loss: 0.0034 - val_accuracy: 1.0000 - val_loss: 6.5565e-10\n",
      "Epoch 8/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m125s\u001b[0m 20ms/step - accuracy: 0.9989 - loss: 0.0036 - val_accuracy: 1.0000 - val_loss: 7.1526e-09\n",
      "Epoch 9/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m121s\u001b[0m 19ms/step - accuracy: 0.9990 - loss: 0.0037 - val_accuracy: 1.0000 - val_loss: 3.6657e-08\n",
      "Epoch 10/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m125s\u001b[0m 20ms/step - accuracy: 0.9991 - loss: 0.0033 - val_accuracy: 1.0000 - val_loss: 1.0133e-09\n",
      "Epoch 11/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m127s\u001b[0m 20ms/step - accuracy: 0.9990 - loss: 0.0031 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 11: early stopping\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step\n",
      "\n",
      "Training fold 5\n",
      "\n",
      "Generating upsampling using SMOTE...\n",
      "200000 samples after upsampling.\n",
      "Class distribution for training after upsampling: Counter({np.int64(0): 6250, np.int64(1): 6250, np.int64(2): 6250, np.int64(3): 6250, np.int64(4): 6250, np.int64(5): 6250, np.int64(6): 6250, np.int64(7): 6250, np.int64(8): 6250, np.int64(9): 6250, np.int64(10): 6250, np.int64(11): 6250, np.int64(12): 6250, np.int64(13): 6250, np.int64(14): 6250, np.int64(15): 6250, np.int64(16): 6250, np.int64(17): 6250, np.int64(18): 6250, np.int64(19): 6250, np.int64(20): 6250, np.int64(21): 6250, np.int64(22): 6250, np.int64(23): 6250, np.int64(24): 6250, np.int64(25): 6250, np.int64(26): 6250, np.int64(27): 6250, np.int64(28): 6250, np.int64(29): 6250, np.int64(30): 6250, np.int64(31): 6250})\n",
      "Finishing upsampling.\n",
      "\n",
      "Epoch 1/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m127s\u001b[0m 21ms/step - accuracy: 0.9959 - loss: 0.0140 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 2/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m131s\u001b[0m 21ms/step - accuracy: 0.9984 - loss: 0.0051 - val_accuracy: 1.0000 - val_loss: 2.0862e-09\n",
      "Epoch 3/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m124s\u001b[0m 20ms/step - accuracy: 0.9985 - loss: 0.0044 - val_accuracy: 1.0000 - val_loss: 3.5226e-08\n",
      "Epoch 4/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m124s\u001b[0m 20ms/step - accuracy: 0.9989 - loss: 0.0039 - val_accuracy: 1.0000 - val_loss: 1.3185e-07\n",
      "Epoch 5/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m128s\u001b[0m 21ms/step - accuracy: 0.9988 - loss: 0.0038 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 6/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m126s\u001b[0m 20ms/step - accuracy: 0.9989 - loss: 0.0034 - val_accuracy: 1.0000 - val_loss: 2.5630e-09\n",
      "Epoch 7/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m126s\u001b[0m 20ms/step - accuracy: 0.9991 - loss: 0.0030 - val_accuracy: 1.0000 - val_loss: 1.3709e-09\n",
      "Epoch 8/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m128s\u001b[0m 21ms/step - accuracy: 0.9993 - loss: 0.0030 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 9/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m134s\u001b[0m 22ms/step - accuracy: 0.9991 - loss: 0.0033 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 10/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m127s\u001b[0m 21ms/step - accuracy: 0.9992 - loss: 0.0024 - val_accuracy: 1.0000 - val_loss: 1.1921e-10\n",
      "Epoch 11/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m127s\u001b[0m 20ms/step - accuracy: 0.9991 - loss: 0.0030 - val_accuracy: 1.0000 - val_loss: 2.1458e-09\n",
      "Epoch 11: early stopping\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step\n",
      "\n",
      "Training fold 6\n",
      "\n",
      "Generating upsampling using SMOTE...\n",
      "200000 samples after upsampling.\n",
      "Class distribution for training after upsampling: Counter({np.int64(0): 6250, np.int64(1): 6250, np.int64(2): 6250, np.int64(3): 6250, np.int64(4): 6250, np.int64(5): 6250, np.int64(6): 6250, np.int64(7): 6250, np.int64(8): 6250, np.int64(9): 6250, np.int64(10): 6250, np.int64(11): 6250, np.int64(12): 6250, np.int64(13): 6250, np.int64(14): 6250, np.int64(15): 6250, np.int64(16): 6250, np.int64(17): 6250, np.int64(18): 6250, np.int64(19): 6250, np.int64(20): 6250, np.int64(21): 6250, np.int64(22): 6250, np.int64(23): 6250, np.int64(24): 6250, np.int64(25): 6250, np.int64(26): 6250, np.int64(27): 6250, np.int64(28): 6250, np.int64(29): 6250, np.int64(30): 6250, np.int64(31): 6250})\n",
      "Finishing upsampling.\n",
      "\n",
      "Epoch 1/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m125s\u001b[0m 20ms/step - accuracy: 0.9962 - loss: 0.0121 - val_accuracy: 1.0000 - val_loss: 7.7486e-10\n",
      "Epoch 2/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m129s\u001b[0m 21ms/step - accuracy: 0.9981 - loss: 0.0057 - val_accuracy: 1.0000 - val_loss: 1.7285e-09\n",
      "Epoch 3/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 21ms/step - accuracy: 0.9987 - loss: 0.0046 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 4/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m127s\u001b[0m 21ms/step - accuracy: 0.9990 - loss: 0.0039 - val_accuracy: 1.0000 - val_loss: 4.1723e-10\n",
      "Epoch 5/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m131s\u001b[0m 21ms/step - accuracy: 0.9991 - loss: 0.0036 - val_accuracy: 1.0000 - val_loss: 2.5570e-08\n",
      "Epoch 6/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m128s\u001b[0m 21ms/step - accuracy: 0.9991 - loss: 0.0028 - val_accuracy: 1.0000 - val_loss: 8.2850e-09\n",
      "Epoch 7/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m131s\u001b[0m 21ms/step - accuracy: 0.9990 - loss: 0.0040 - val_accuracy: 1.0000 - val_loss: 1.9789e-08\n",
      "Epoch 8/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m135s\u001b[0m 22ms/step - accuracy: 0.9993 - loss: 0.0032 - val_accuracy: 1.0000 - val_loss: 4.3511e-09\n",
      "Epoch 9/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m129s\u001b[0m 21ms/step - accuracy: 0.9991 - loss: 0.0029 - val_accuracy: 1.0000 - val_loss: 1.2040e-08\n",
      "Epoch 10/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m133s\u001b[0m 21ms/step - accuracy: 0.9989 - loss: 0.0036 - val_accuracy: 1.0000 - val_loss: 1.6689e-09\n",
      "Epoch 11/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m136s\u001b[0m 22ms/step - accuracy: 0.9993 - loss: 0.0025 - val_accuracy: 1.0000 - val_loss: 4.1723e-09\n",
      "Epoch 12/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m134s\u001b[0m 22ms/step - accuracy: 0.9991 - loss: 0.0034 - val_accuracy: 1.0000 - val_loss: 1.7881e-10\n",
      "Epoch 13/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m132s\u001b[0m 21ms/step - accuracy: 0.9992 - loss: 0.0027 - val_accuracy: 1.0000 - val_loss: 1.5497e-09\n",
      "Epoch 13: early stopping\n",
      "Restoring model weights from the end of the best epoch: 3.\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "\n",
      "Training fold 7\n",
      "\n",
      "Generating upsampling using SMOTE...\n",
      "200000 samples after upsampling.\n",
      "Class distribution for training after upsampling: Counter({np.int64(0): 6250, np.int64(1): 6250, np.int64(2): 6250, np.int64(3): 6250, np.int64(4): 6250, np.int64(5): 6250, np.int64(6): 6250, np.int64(7): 6250, np.int64(8): 6250, np.int64(9): 6250, np.int64(10): 6250, np.int64(11): 6250, np.int64(12): 6250, np.int64(13): 6250, np.int64(14): 6250, np.int64(15): 6250, np.int64(16): 6250, np.int64(17): 6250, np.int64(18): 6250, np.int64(19): 6250, np.int64(20): 6250, np.int64(21): 6250, np.int64(22): 6250, np.int64(23): 6250, np.int64(24): 6250, np.int64(25): 6250, np.int64(26): 6250, np.int64(27): 6250, np.int64(28): 6250, np.int64(29): 6250, np.int64(30): 6250, np.int64(31): 6250})\n",
      "Finishing upsampling.\n",
      "\n",
      "Epoch 1/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m138s\u001b[0m 22ms/step - accuracy: 0.9973 - loss: 0.0086 - val_accuracy: 1.0000 - val_loss: 3.5763e-10\n",
      "Epoch 2/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m134s\u001b[0m 22ms/step - accuracy: 0.9986 - loss: 0.0050 - val_accuracy: 1.0000 - val_loss: 1.3518e-07\n",
      "Epoch 3/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m136s\u001b[0m 22ms/step - accuracy: 0.9988 - loss: 0.0042 - val_accuracy: 1.0000 - val_loss: 3.2783e-09\n",
      "Epoch 4/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m137s\u001b[0m 22ms/step - accuracy: 0.9990 - loss: 0.0035 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 5/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 23ms/step - accuracy: 0.9993 - loss: 0.0028 - val_accuracy: 1.0000 - val_loss: 4.7552e-07\n",
      "Epoch 6/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m136s\u001b[0m 22ms/step - accuracy: 0.9992 - loss: 0.0033 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 7/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m139s\u001b[0m 23ms/step - accuracy: 0.9991 - loss: 0.0036 - val_accuracy: 1.0000 - val_loss: 4.4703e-09\n",
      "Epoch 8/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m150s\u001b[0m 24ms/step - accuracy: 0.9990 - loss: 0.0033 - val_accuracy: 1.0000 - val_loss: 2.6226e-09\n",
      "Epoch 9/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m155s\u001b[0m 25ms/step - accuracy: 0.9991 - loss: 0.0031 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 10/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m165s\u001b[0m 27ms/step - accuracy: 0.9992 - loss: 0.0025 - val_accuracy: 1.0000 - val_loss: 1.5199e-08\n",
      "Epoch 11/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m166s\u001b[0m 27ms/step - accuracy: 0.9993 - loss: 0.0023 - val_accuracy: 1.0000 - val_loss: 2.3246e-09\n",
      "Epoch 12/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m165s\u001b[0m 27ms/step - accuracy: 0.9992 - loss: 0.0030 - val_accuracy: 1.0000 - val_loss: 8.5235e-09\n",
      "Epoch 13/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m172s\u001b[0m 28ms/step - accuracy: 0.9992 - loss: 0.0029 - val_accuracy: 1.0000 - val_loss: 5.6624e-09\n",
      "Epoch 14/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m168s\u001b[0m 27ms/step - accuracy: 0.9993 - loss: 0.0023 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 14: early stopping\n",
      "Restoring model weights from the end of the best epoch: 4.\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step\n",
      "\n",
      "Training fold 8\n",
      "\n",
      "Generating upsampling using SMOTE...\n",
      "200000 samples after upsampling.\n",
      "Class distribution for training after upsampling: Counter({np.int64(0): 6250, np.int64(1): 6250, np.int64(2): 6250, np.int64(3): 6250, np.int64(4): 6250, np.int64(5): 6250, np.int64(6): 6250, np.int64(7): 6250, np.int64(8): 6250, np.int64(9): 6250, np.int64(10): 6250, np.int64(11): 6250, np.int64(12): 6250, np.int64(13): 6250, np.int64(14): 6250, np.int64(15): 6250, np.int64(16): 6250, np.int64(17): 6250, np.int64(18): 6250, np.int64(19): 6250, np.int64(20): 6250, np.int64(21): 6250, np.int64(22): 6250, np.int64(23): 6250, np.int64(24): 6250, np.int64(25): 6250, np.int64(26): 6250, np.int64(27): 6250, np.int64(28): 6250, np.int64(29): 6250, np.int64(30): 6250, np.int64(31): 6250})\n",
      "Finishing upsampling.\n",
      "\n",
      "Epoch 1/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m177s\u001b[0m 29ms/step - accuracy: 0.9963 - loss: 0.0128 - val_accuracy: 1.0000 - val_loss: 1.2100e-08\n",
      "Epoch 2/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m181s\u001b[0m 29ms/step - accuracy: 0.9986 - loss: 0.0040 - val_accuracy: 1.0000 - val_loss: 4.6134e-08\n",
      "Epoch 3/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m177s\u001b[0m 29ms/step - accuracy: 0.9989 - loss: 0.0041 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 4/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m177s\u001b[0m 29ms/step - accuracy: 0.9991 - loss: 0.0035 - val_accuracy: 1.0000 - val_loss: 1.6391e-08\n",
      "Epoch 5/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m181s\u001b[0m 29ms/step - accuracy: 0.9990 - loss: 0.0037 - val_accuracy: 1.0000 - val_loss: 3.2453e-05\n",
      "Epoch 6/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m180s\u001b[0m 29ms/step - accuracy: 0.9993 - loss: 0.0032 - val_accuracy: 1.0000 - val_loss: 8.9407e-10\n",
      "Epoch 7/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m186s\u001b[0m 30ms/step - accuracy: 0.9994 - loss: 0.0025 - val_accuracy: 1.0000 - val_loss: 3.2187e-09\n",
      "Epoch 8/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m185s\u001b[0m 30ms/step - accuracy: 0.9993 - loss: 0.0025 - val_accuracy: 1.0000 - val_loss: 1.4901e-09\n",
      "Epoch 9/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m187s\u001b[0m 30ms/step - accuracy: 0.9992 - loss: 0.0030 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 10/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m187s\u001b[0m 30ms/step - accuracy: 0.9994 - loss: 0.0026 - val_accuracy: 1.0000 - val_loss: 1.2134e-06\n",
      "Epoch 11/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m191s\u001b[0m 31ms/step - accuracy: 0.9993 - loss: 0.0022 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 12/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m197s\u001b[0m 32ms/step - accuracy: 0.9992 - loss: 0.0030 - val_accuracy: 1.0000 - val_loss: 3.7521e-07\n",
      "Epoch 13/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m192s\u001b[0m 31ms/step - accuracy: 0.9992 - loss: 0.0037 - val_accuracy: 1.0000 - val_loss: 1.8120e-08\n",
      "Epoch 13: early stopping\n",
      "Restoring model weights from the end of the best epoch: 3.\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step\n",
      "\n",
      "Training fold 9\n",
      "\n",
      "Generating upsampling using SMOTE...\n",
      "200000 samples after upsampling.\n",
      "Class distribution for training after upsampling: Counter({np.int64(0): 6250, np.int64(1): 6250, np.int64(2): 6250, np.int64(3): 6250, np.int64(4): 6250, np.int64(5): 6250, np.int64(6): 6250, np.int64(7): 6250, np.int64(8): 6250, np.int64(9): 6250, np.int64(10): 6250, np.int64(11): 6250, np.int64(12): 6250, np.int64(13): 6250, np.int64(14): 6250, np.int64(15): 6250, np.int64(16): 6250, np.int64(17): 6250, np.int64(18): 6250, np.int64(19): 6250, np.int64(20): 6250, np.int64(21): 6250, np.int64(22): 6250, np.int64(23): 6250, np.int64(24): 6250, np.int64(25): 6250, np.int64(26): 6250, np.int64(27): 6250, np.int64(28): 6250, np.int64(29): 6250, np.int64(30): 6250, np.int64(31): 6250})\n",
      "Finishing upsampling.\n",
      "\n",
      "Epoch 1/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m202s\u001b[0m 33ms/step - accuracy: 0.9973 - loss: 0.0110 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 2/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m194s\u001b[0m 31ms/step - accuracy: 0.9990 - loss: 0.0038 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 3/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m200s\u001b[0m 32ms/step - accuracy: 0.9991 - loss: 0.0031 - val_accuracy: 1.0000 - val_loss: 1.7881e-10\n",
      "Epoch 4/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m194s\u001b[0m 31ms/step - accuracy: 0.9990 - loss: 0.0037 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 5/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m198s\u001b[0m 32ms/step - accuracy: 0.9991 - loss: 0.0029 - val_accuracy: 1.0000 - val_loss: 1.1921e-09\n",
      "Epoch 6/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m203s\u001b[0m 33ms/step - accuracy: 0.9993 - loss: 0.0027 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 7/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m199s\u001b[0m 32ms/step - accuracy: 0.9993 - loss: 0.0023 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 8/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m203s\u001b[0m 33ms/step - accuracy: 0.9992 - loss: 0.0028 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 9/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m206s\u001b[0m 33ms/step - accuracy: 0.9992 - loss: 0.0026 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 10/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m206s\u001b[0m 33ms/step - accuracy: 0.9992 - loss: 0.0028 - val_accuracy: 1.0000 - val_loss: 2.9564e-08\n",
      "Epoch 11/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m207s\u001b[0m 33ms/step - accuracy: 0.9992 - loss: 0.0023 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 11: early stopping\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step\n",
      "\n",
      "Training fold 10\n",
      "\n",
      "Generating upsampling using SMOTE...\n",
      "200000 samples after upsampling.\n",
      "Class distribution for training after upsampling: Counter({np.int64(0): 6250, np.int64(1): 6250, np.int64(2): 6250, np.int64(3): 6250, np.int64(4): 6250, np.int64(5): 6250, np.int64(6): 6250, np.int64(7): 6250, np.int64(8): 6250, np.int64(9): 6250, np.int64(10): 6250, np.int64(11): 6250, np.int64(12): 6250, np.int64(13): 6250, np.int64(14): 6250, np.int64(15): 6250, np.int64(16): 6250, np.int64(17): 6250, np.int64(18): 6250, np.int64(19): 6250, np.int64(20): 6250, np.int64(21): 6250, np.int64(22): 6250, np.int64(23): 6250, np.int64(24): 6250, np.int64(25): 6250, np.int64(26): 6250, np.int64(27): 6250, np.int64(28): 6250, np.int64(29): 6250, np.int64(30): 6250, np.int64(31): 6250})\n",
      "Finishing upsampling.\n",
      "\n",
      "Epoch 1/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m211s\u001b[0m 34ms/step - accuracy: 0.9978 - loss: 0.0079 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 2/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m212s\u001b[0m 34ms/step - accuracy: 0.9991 - loss: 0.0033 - val_accuracy: 1.0000 - val_loss: 3.4630e-08\n",
      "Epoch 3/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m214s\u001b[0m 35ms/step - accuracy: 0.9990 - loss: 0.0036 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 4/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m212s\u001b[0m 34ms/step - accuracy: 0.9991 - loss: 0.0033 - val_accuracy: 1.0000 - val_loss: 5.9837e-07\n",
      "Epoch 5/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m214s\u001b[0m 35ms/step - accuracy: 0.9992 - loss: 0.0029 - val_accuracy: 1.0000 - val_loss: 1.0309e-06\n",
      "Epoch 6/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m225s\u001b[0m 36ms/step - accuracy: 0.9992 - loss: 0.0029 - val_accuracy: 1.0000 - val_loss: 5.3644e-10\n",
      "Epoch 7/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m219s\u001b[0m 35ms/step - accuracy: 0.9992 - loss: 0.0032 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 8/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m219s\u001b[0m 35ms/step - accuracy: 0.9992 - loss: 0.0033 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 9/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m221s\u001b[0m 36ms/step - accuracy: 0.9994 - loss: 0.0024 - val_accuracy: 1.0000 - val_loss: 1.4842e-08\n",
      "Epoch 10/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m226s\u001b[0m 37ms/step - accuracy: 0.9994 - loss: 0.0023 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 11/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m225s\u001b[0m 36ms/step - accuracy: 0.9994 - loss: 0.0021 - val_accuracy: 1.0000 - val_loss: 5.9605e-10\n",
      "Epoch 11: early stopping\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step\n",
      "\n",
      "Time taken for training:  06:43:39\n",
      "\n",
      "\n",
      "Fold 1 - Train Accuracy 0.9993 - Test Accuracy 0.5942\n",
      "Fold 2 - Train Accuracy 0.9997 - Test Accuracy 0.7689\n",
      "Fold 3 - Train Accuracy 0.9996 - Test Accuracy 0.9270\n",
      "Fold 4 - Train Accuracy 0.9993 - Test Accuracy 0.9604\n",
      "Fold 5 - Train Accuracy 0.9994 - Test Accuracy 0.9717\n",
      "Fold 6 - Train Accuracy 0.9996 - Test Accuracy 0.9693\n",
      "Fold 7 - Train Accuracy 0.9996 - Test Accuracy 0.9674\n",
      "Fold 8 - Train Accuracy 0.9994 - Test Accuracy 0.9740\n",
      "Fold 9 - Train Accuracy 0.9997 - Test Accuracy 0.9798\n",
      "Fold 10 - Train Accuracy 0.9998 - Test Accuracy 0.9903\n",
      "\n",
      "Mean Train Accuracy: 0.9995 - Std: 0.0002 \n",
      "Mean Test Accuracy: 0.9103 - Std: 0.1219 \n",
      "\n",
      "Evaluate other metrics:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.95      0.94     13905\n",
      "           1       0.95      0.94      0.95       723\n",
      "           2       0.94      0.98      0.96      2659\n",
      "           3       0.82      0.79      0.81      1123\n",
      "           4       0.91      0.79      0.85       384\n",
      "           5       0.84      0.84      0.84        44\n",
      "           6       0.92      0.85      0.88       663\n",
      "           7       0.87      0.87      0.87        94\n",
      "           8       1.00      0.92      0.96        12\n",
      "           9       0.88      0.83      0.85       786\n",
      "          10       0.80      0.89      0.84         9\n",
      "          11       0.94      0.85      0.89        98\n",
      "          12       1.00      0.75      0.86         8\n",
      "          13       0.89      0.80      0.84        20\n",
      "          14       0.89      0.84      0.87        77\n",
      "          15       0.89      0.90      0.90        62\n",
      "          16       0.78      0.84      0.81       917\n",
      "          17       0.93      0.90      0.91       473\n",
      "          18       1.00      0.91      0.95        22\n",
      "          19       0.86      0.84      0.85       122\n",
      "          20       0.93      0.86      0.90       111\n",
      "          21       0.89      0.82      0.85       201\n",
      "          22       0.71      0.71      0.71         7\n",
      "          23       0.85      0.83      0.84        96\n",
      "          24       0.84      0.81      0.83      1045\n",
      "          25       0.86      0.85      0.85       540\n",
      "          26       0.83      0.81      0.82      1334\n",
      "          27       0.85      0.82      0.84        28\n",
      "          28       0.92      0.94      0.93        35\n",
      "          29       0.85      0.82      0.83        77\n",
      "          30       0.98      0.83      0.90        64\n",
      "          31       0.86      0.86      0.86         7\n",
      "\n",
      "    accuracy                           0.91     25746\n",
      "   macro avg       0.89      0.85      0.87     25746\n",
      "weighted avg       0.91      0.91      0.91     25746\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Build a \"time window\" structure to handle the dataset as a time series.\n",
    "new_df = load_dataset(\"V4\")\n",
    "X_array, y_array = build_time_window_structure(new_df, \"V4\")\n",
    "X_array, y_array = remove_classes_with_less_samples(X_array, y_array)\n",
    "\n",
    "# Train the CNN model.\n",
    "v1_model = create_v1()\n",
    "v1_num_epochs = 300\n",
    "v1_batch_size = 32\n",
    "v1_validation_split = 0.01\n",
    "\n",
    "training_history_v1 = train_cnn_model(v1_model, X_array, y_array, v1_num_epochs, v1_batch_size, v1_validation_split, \"v1_model_V4.keras\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4393b94-adff-4999-bc4a-f800ed79665e",
   "metadata": {},
   "source": [
    "#### V5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53694cdc-fc20-4759-9fc5-94ed8992946f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Start loading CSV file...\n",
      "Finish loading CSV file.\n",
      "\n",
      "Starting build_time_window_structure function...\n",
      "\n",
      "Shape of features:  (25770, 1250)\n",
      "Quantity os samples (labels):  25770\n",
      "\n",
      "Finishing build_time_window_structure function.\n",
      "\n",
      "Remove classes with less than 6 samples.\n",
      "\n",
      "Show classes identification:\n",
      "Class: 0 - Arrhythmia code: 1\n",
      "Class: 1 - Arrhythmia code: 21\n",
      "Class: 2 - Arrhythmia code: 22\n",
      "Class: 3 - Arrhythmia code: 23\n",
      "Class: 4 - Arrhythmia code: 30\n",
      "Class: 5 - Arrhythmia code: 36\n",
      "Class: 6 - Arrhythmia code: 50\n",
      "Class: 7 - Arrhythmia code: 51\n",
      "Class: 8 - Arrhythmia code: 54\n",
      "Class: 9 - Arrhythmia code: 60\n",
      "Class: 10 - Arrhythmia code: 80\n",
      "Class: 11 - Arrhythmia code: 82\n",
      "Class: 12 - Arrhythmia code: 83\n",
      "Class: 13 - Arrhythmia code: 88\n",
      "Class: 14 - Arrhythmia code: 101\n",
      "Class: 15 - Arrhythmia code: 104\n",
      "Class: 16 - Arrhythmia code: 105\n",
      "Class: 17 - Arrhythmia code: 106\n",
      "Class: 18 - Arrhythmia code: 108\n",
      "Class: 19 - Arrhythmia code: 120\n",
      "Class: 20 - Arrhythmia code: 121\n",
      "Class: 21 - Arrhythmia code: 125\n",
      "Class: 22 - Arrhythmia code: 140\n",
      "Class: 23 - Arrhythmia code: 142\n",
      "Class: 24 - Arrhythmia code: 145\n",
      "Class: 25 - Arrhythmia code: 146\n",
      "Class: 26 - Arrhythmia code: 147\n",
      "Class: 27 - Arrhythmia code: 155\n",
      "Class: 28 - Arrhythmia code: 160\n",
      "Class: 29 - Arrhythmia code: 161\n",
      "Class: 30 - Arrhythmia code: 165\n",
      "Class: 31 - Arrhythmia code: 166\n",
      "\n",
      "Check for dataset balance:\n",
      "Class =   0   Qty =    13905   Percentage = 54.01 %\n",
      "Class =   2   Qty =     2659   Percentage = 10.33 %\n",
      "Class =  26   Qty =     1334   Percentage = 5.18 %\n",
      "Class =   3   Qty =     1123   Percentage = 4.36 %\n",
      "Class =  24   Qty =     1045   Percentage = 4.06 %\n",
      "Class =  16   Qty =      917   Percentage = 3.56 %\n",
      "Class =   9   Qty =      786   Percentage = 3.05 %\n",
      "Class =   1   Qty =      723   Percentage = 2.81 %\n",
      "Class =   6   Qty =      663   Percentage = 2.58 %\n",
      "Class =  25   Qty =      540   Percentage = 2.10 %\n",
      "Class =  17   Qty =      473   Percentage = 1.84 %\n",
      "Class =   4   Qty =      384   Percentage = 1.49 %\n",
      "Class =  21   Qty =      201   Percentage = 0.78 %\n",
      "Class =  19   Qty =      122   Percentage = 0.47 %\n",
      "Class =  20   Qty =      111   Percentage = 0.43 %\n",
      "Class =  11   Qty =       98   Percentage = 0.38 %\n",
      "Class =  23   Qty =       96   Percentage = 0.37 %\n",
      "Class =   7   Qty =       94   Percentage = 0.37 %\n",
      "Class =  14   Qty =       77   Percentage = 0.30 %\n",
      "Class =  29   Qty =       77   Percentage = 0.30 %\n",
      "Class =  30   Qty =       64   Percentage = 0.25 %\n",
      "Class =  15   Qty =       62   Percentage = 0.24 %\n",
      "Class =   5   Qty =       44   Percentage = 0.17 %\n",
      "Class =  28   Qty =       35   Percentage = 0.14 %\n",
      "Class =  27   Qty =       28   Percentage = 0.11 %\n",
      "Class =  18   Qty =       22   Percentage = 0.09 %\n",
      "Class =  13   Qty =       20   Percentage = 0.08 %\n",
      "Class =   8   Qty =       12   Percentage = 0.05 %\n",
      "Class =  10   Qty =        9   Percentage = 0.03 %\n",
      "Class =  12   Qty =        8   Percentage = 0.03 %\n",
      "Class =  22   Qty =        7   Percentage = 0.03 %\n",
      "Class =  31   Qty =        7   Percentage = 0.03 %\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1248</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)        │            <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1248</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)        │            <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1246</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)        │           <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1246</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)        │            <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">623</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">619</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)        │           <span style=\"color: #00af00; text-decoration-color: #00af00\">656</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_2           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">619</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)        │            <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">615</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)        │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,296</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_3           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">615</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)        │            <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">307</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4912</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">628,864</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_4           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,128</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv1d (\u001b[38;5;33mConv1D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1248\u001b[0m, \u001b[38;5;34m8\u001b[0m)        │            \u001b[38;5;34m32\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1248\u001b[0m, \u001b[38;5;34m8\u001b[0m)        │            \u001b[38;5;34m32\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_1 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1246\u001b[0m, \u001b[38;5;34m8\u001b[0m)        │           \u001b[38;5;34m200\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_1           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1246\u001b[0m, \u001b[38;5;34m8\u001b[0m)        │            \u001b[38;5;34m32\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d (\u001b[38;5;33mMaxPooling1D\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m623\u001b[0m, \u001b[38;5;34m8\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_2 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m619\u001b[0m, \u001b[38;5;34m16\u001b[0m)        │           \u001b[38;5;34m656\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_2           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m619\u001b[0m, \u001b[38;5;34m16\u001b[0m)        │            \u001b[38;5;34m64\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_3 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m615\u001b[0m, \u001b[38;5;34m16\u001b[0m)        │         \u001b[38;5;34m1,296\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_3           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m615\u001b[0m, \u001b[38;5;34m16\u001b[0m)        │            \u001b[38;5;34m64\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_1 (\u001b[38;5;33mMaxPooling1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m307\u001b[0m, \u001b[38;5;34m16\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4912\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │       \u001b[38;5;34m628,864\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_4           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │           \u001b[38;5;34m512\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │         \u001b[38;5;34m4,128\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">635,880</span> (2.43 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m635,880\u001b[0m (2.43 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">635,528</span> (2.42 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m635,528\u001b[0m (2.42 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">352</span> (1.38 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m352\u001b[0m (1.38 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting training...\n",
      "\n",
      "Training fold 1\n",
      "\n",
      "Generating upsampling using SMOTE...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\DeveloperTools\\python\\3.11.0\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:737: UserWarning: The least populated class in y has only 7 members, which is less than n_splits=10.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200000 samples after upsampling.\n",
      "Class distribution for training after upsampling: Counter({np.int64(0): 6250, np.int64(1): 6250, np.int64(2): 6250, np.int64(3): 6250, np.int64(4): 6250, np.int64(5): 6250, np.int64(6): 6250, np.int64(7): 6250, np.int64(8): 6250, np.int64(9): 6250, np.int64(10): 6250, np.int64(11): 6250, np.int64(12): 6250, np.int64(13): 6250, np.int64(14): 6250, np.int64(15): 6250, np.int64(16): 6250, np.int64(17): 6250, np.int64(18): 6250, np.int64(19): 6250, np.int64(20): 6250, np.int64(21): 6250, np.int64(22): 6250, np.int64(23): 6250, np.int64(24): 6250, np.int64(25): 6250, np.int64(26): 6250, np.int64(27): 6250, np.int64(28): 6250, np.int64(29): 6250, np.int64(30): 6250, np.int64(31): 6250})\n",
      "Finishing upsampling.\n",
      "\n",
      "Epoch 1/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m101s\u001b[0m 16ms/step - accuracy: 0.8720 - loss: 0.5014 - val_accuracy: 1.0000 - val_loss: 6.2636e-05\n",
      "Epoch 2/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 17ms/step - accuracy: 0.9789 - loss: 0.0687 - val_accuracy: 1.0000 - val_loss: 4.3538e-05\n",
      "Epoch 3/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 17ms/step - accuracy: 0.9890 - loss: 0.0347 - val_accuracy: 1.0000 - val_loss: 2.8160e-05\n",
      "Epoch 4/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m103s\u001b[0m 17ms/step - accuracy: 0.9925 - loss: 0.0244 - val_accuracy: 1.0000 - val_loss: 3.2931e-07\n",
      "Epoch 5/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m102s\u001b[0m 17ms/step - accuracy: 0.9946 - loss: 0.0173 - val_accuracy: 1.0000 - val_loss: 9.0038e-07\n",
      "Epoch 6/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m102s\u001b[0m 17ms/step - accuracy: 0.9957 - loss: 0.0129 - val_accuracy: 1.0000 - val_loss: 1.1938e-06\n",
      "Epoch 7/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m102s\u001b[0m 16ms/step - accuracy: 0.9960 - loss: 0.0124 - val_accuracy: 1.0000 - val_loss: 1.6730e-05\n",
      "Epoch 8/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 17ms/step - accuracy: 0.9963 - loss: 0.0113 - val_accuracy: 1.0000 - val_loss: 1.5497e-09\n",
      "Epoch 9/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 18ms/step - accuracy: 0.9968 - loss: 0.0100 - val_accuracy: 1.0000 - val_loss: 6.8545e-09\n",
      "Epoch 10/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m103s\u001b[0m 17ms/step - accuracy: 0.9969 - loss: 0.0091 - val_accuracy: 1.0000 - val_loss: 6.0469e-05\n",
      "Epoch 11/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m103s\u001b[0m 17ms/step - accuracy: 0.9976 - loss: 0.0073 - val_accuracy: 1.0000 - val_loss: 1.6868e-08\n",
      "Epoch 12/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 17ms/step - accuracy: 0.9978 - loss: 0.0074 - val_accuracy: 1.0000 - val_loss: 2.8431e-08\n",
      "Epoch 13/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 17ms/step - accuracy: 0.9979 - loss: 0.0064 - val_accuracy: 1.0000 - val_loss: 7.3314e-09\n",
      "Epoch 14/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 17ms/step - accuracy: 0.9981 - loss: 0.0060 - val_accuracy: 1.0000 - val_loss: 2.6941e-08\n",
      "Epoch 15/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 17ms/step - accuracy: 0.9979 - loss: 0.0061 - val_accuracy: 1.0000 - val_loss: 3.3259e-08\n",
      "Epoch 16/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 17ms/step - accuracy: 0.9984 - loss: 0.0050 - val_accuracy: 1.0000 - val_loss: 1.0729e-09\n",
      "Epoch 17/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 17ms/step - accuracy: 0.9983 - loss: 0.0057 - val_accuracy: 1.0000 - val_loss: 3.2400e-07\n",
      "Epoch 18/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 17ms/step - accuracy: 0.9984 - loss: 0.0048 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 19/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 17ms/step - accuracy: 0.9983 - loss: 0.0055 - val_accuracy: 1.0000 - val_loss: 7.6890e-09\n",
      "Epoch 20/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 17ms/step - accuracy: 0.9982 - loss: 0.0062 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 21/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 17ms/step - accuracy: 0.9987 - loss: 0.0044 - val_accuracy: 1.0000 - val_loss: 7.1526e-10\n",
      "Epoch 22/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 17ms/step - accuracy: 0.9989 - loss: 0.0039 - val_accuracy: 1.0000 - val_loss: 1.1879e-07\n",
      "Epoch 23/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 17ms/step - accuracy: 0.9985 - loss: 0.0054 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 24/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 17ms/step - accuracy: 0.9988 - loss: 0.0038 - val_accuracy: 1.0000 - val_loss: 8.2612e-08\n",
      "Epoch 25/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 18ms/step - accuracy: 0.9988 - loss: 0.0041 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 26/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m110s\u001b[0m 18ms/step - accuracy: 0.9988 - loss: 0.0037 - val_accuracy: 1.0000 - val_loss: 3.2187e-09\n",
      "Epoch 27/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 17ms/step - accuracy: 0.9987 - loss: 0.0045 - val_accuracy: 1.0000 - val_loss: 7.0333e-09\n",
      "Epoch 28/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 18ms/step - accuracy: 0.9988 - loss: 0.0043 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 28: early stopping\n",
      "Restoring model weights from the end of the best epoch: 18.\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step\n",
      "\n",
      "Training fold 2\n",
      "\n",
      "Generating upsampling using SMOTE...\n",
      "200000 samples after upsampling.\n",
      "Class distribution for training after upsampling: Counter({np.int64(0): 6250, np.int64(1): 6250, np.int64(2): 6250, np.int64(3): 6250, np.int64(4): 6250, np.int64(5): 6250, np.int64(6): 6250, np.int64(7): 6250, np.int64(8): 6250, np.int64(9): 6250, np.int64(10): 6250, np.int64(11): 6250, np.int64(12): 6250, np.int64(13): 6250, np.int64(14): 6250, np.int64(15): 6250, np.int64(16): 6250, np.int64(17): 6250, np.int64(18): 6250, np.int64(19): 6250, np.int64(20): 6250, np.int64(21): 6250, np.int64(22): 6250, np.int64(23): 6250, np.int64(24): 6250, np.int64(25): 6250, np.int64(26): 6250, np.int64(27): 6250, np.int64(28): 6250, np.int64(29): 6250, np.int64(30): 6250, np.int64(31): 6250})\n",
      "Finishing upsampling.\n",
      "\n",
      "Epoch 1/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m121s\u001b[0m 20ms/step - accuracy: 0.9766 - loss: 0.1299 - val_accuracy: 1.0000 - val_loss: 5.3048e-09\n",
      "Epoch 2/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m111s\u001b[0m 18ms/step - accuracy: 0.9953 - loss: 0.0154 - val_accuracy: 1.0000 - val_loss: 4.2856e-08\n",
      "Epoch 3/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 18ms/step - accuracy: 0.9973 - loss: 0.0091 - val_accuracy: 1.0000 - val_loss: 2.0802e-08\n",
      "Epoch 4/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 18ms/step - accuracy: 0.9979 - loss: 0.0069 - val_accuracy: 1.0000 - val_loss: 2.0921e-08\n",
      "Epoch 5/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m111s\u001b[0m 18ms/step - accuracy: 0.9984 - loss: 0.0054 - val_accuracy: 1.0000 - val_loss: 5.9605e-11\n",
      "Epoch 6/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 18ms/step - accuracy: 0.9982 - loss: 0.0055 - val_accuracy: 1.0000 - val_loss: 2.0802e-08\n",
      "Epoch 7/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m110s\u001b[0m 18ms/step - accuracy: 0.9984 - loss: 0.0046 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 8/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m110s\u001b[0m 18ms/step - accuracy: 0.9985 - loss: 0.0047 - val_accuracy: 1.0000 - val_loss: 8.9407e-10\n",
      "Epoch 9/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m110s\u001b[0m 18ms/step - accuracy: 0.9987 - loss: 0.0039 - val_accuracy: 1.0000 - val_loss: 8.0942e-07\n",
      "Epoch 10/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m115s\u001b[0m 19ms/step - accuracy: 0.9987 - loss: 0.0041 - val_accuracy: 1.0000 - val_loss: 1.8477e-09\n",
      "Epoch 11/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m112s\u001b[0m 18ms/step - accuracy: 0.9986 - loss: 0.0047 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 12/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m114s\u001b[0m 18ms/step - accuracy: 0.9988 - loss: 0.0044 - val_accuracy: 1.0000 - val_loss: 1.1921e-10\n",
      "Epoch 13/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m115s\u001b[0m 19ms/step - accuracy: 0.9992 - loss: 0.0027 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 14/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m116s\u001b[0m 19ms/step - accuracy: 0.9987 - loss: 0.0041 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 15/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m116s\u001b[0m 19ms/step - accuracy: 0.9988 - loss: 0.0042 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 16/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m115s\u001b[0m 19ms/step - accuracy: 0.9990 - loss: 0.0033 - val_accuracy: 1.0000 - val_loss: 1.1921e-10\n",
      "Epoch 17/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m116s\u001b[0m 19ms/step - accuracy: 0.9990 - loss: 0.0036 - val_accuracy: 1.0000 - val_loss: 3.5763e-10\n",
      "Epoch 17: early stopping\n",
      "Restoring model weights from the end of the best epoch: 7.\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "\n",
      "Training fold 3\n",
      "\n",
      "Generating upsampling using SMOTE...\n",
      "200000 samples after upsampling.\n",
      "Class distribution for training after upsampling: Counter({np.int64(0): 6250, np.int64(1): 6250, np.int64(2): 6250, np.int64(3): 6250, np.int64(4): 6250, np.int64(5): 6250, np.int64(6): 6250, np.int64(7): 6250, np.int64(8): 6250, np.int64(9): 6250, np.int64(10): 6250, np.int64(11): 6250, np.int64(12): 6250, np.int64(13): 6250, np.int64(14): 6250, np.int64(15): 6250, np.int64(16): 6250, np.int64(17): 6250, np.int64(18): 6250, np.int64(19): 6250, np.int64(20): 6250, np.int64(21): 6250, np.int64(22): 6250, np.int64(23): 6250, np.int64(24): 6250, np.int64(25): 6250, np.int64(26): 6250, np.int64(27): 6250, np.int64(28): 6250, np.int64(29): 6250, np.int64(30): 6250, np.int64(31): 6250})\n",
      "Finishing upsampling.\n",
      "\n",
      "Epoch 1/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m119s\u001b[0m 19ms/step - accuracy: 0.9913 - loss: 0.0354 - val_accuracy: 1.0000 - val_loss: 2.0862e-09\n",
      "Epoch 2/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m118s\u001b[0m 19ms/step - accuracy: 0.9975 - loss: 0.0090 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 3/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m118s\u001b[0m 19ms/step - accuracy: 0.9983 - loss: 0.0053 - val_accuracy: 1.0000 - val_loss: 1.1921e-10\n",
      "Epoch 4/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m120s\u001b[0m 19ms/step - accuracy: 0.9986 - loss: 0.0049 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 5/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m121s\u001b[0m 20ms/step - accuracy: 0.9988 - loss: 0.0043 - val_accuracy: 1.0000 - val_loss: 1.0729e-09\n",
      "Epoch 6/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m123s\u001b[0m 20ms/step - accuracy: 0.9989 - loss: 0.0037 - val_accuracy: 1.0000 - val_loss: 2.9206e-09\n",
      "Epoch 7/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m121s\u001b[0m 20ms/step - accuracy: 0.9989 - loss: 0.0041 - val_accuracy: 1.0000 - val_loss: 1.5676e-08\n",
      "Epoch 8/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m123s\u001b[0m 20ms/step - accuracy: 0.9988 - loss: 0.0044 - val_accuracy: 1.0000 - val_loss: 6.4373e-09\n",
      "Epoch 9/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m123s\u001b[0m 20ms/step - accuracy: 0.9990 - loss: 0.0037 - val_accuracy: 1.0000 - val_loss: 3.2902e-08\n",
      "Epoch 10/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m123s\u001b[0m 20ms/step - accuracy: 0.9989 - loss: 0.0038 - val_accuracy: 1.0000 - val_loss: 2.4379e-06\n",
      "Epoch 11/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m124s\u001b[0m 20ms/step - accuracy: 0.9990 - loss: 0.0033 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 12/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m123s\u001b[0m 20ms/step - accuracy: 0.9990 - loss: 0.0036 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 12: early stopping\n",
      "Restoring model weights from the end of the best epoch: 2.\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step\n",
      "\n",
      "Training fold 4\n",
      "\n",
      "Generating upsampling using SMOTE...\n",
      "200000 samples after upsampling.\n",
      "Class distribution for training after upsampling: Counter({np.int64(0): 6250, np.int64(1): 6250, np.int64(2): 6250, np.int64(3): 6250, np.int64(4): 6250, np.int64(5): 6250, np.int64(6): 6250, np.int64(7): 6250, np.int64(8): 6250, np.int64(9): 6250, np.int64(10): 6250, np.int64(11): 6250, np.int64(12): 6250, np.int64(13): 6250, np.int64(14): 6250, np.int64(15): 6250, np.int64(16): 6250, np.int64(17): 6250, np.int64(18): 6250, np.int64(19): 6250, np.int64(20): 6250, np.int64(21): 6250, np.int64(22): 6250, np.int64(23): 6250, np.int64(24): 6250, np.int64(25): 6250, np.int64(26): 6250, np.int64(27): 6250, np.int64(28): 6250, np.int64(29): 6250, np.int64(30): 6250, np.int64(31): 6250})\n",
      "Finishing upsampling.\n",
      "\n",
      "Epoch 1/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m128s\u001b[0m 21ms/step - accuracy: 0.9946 - loss: 0.0185 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 2/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m127s\u001b[0m 21ms/step - accuracy: 0.9981 - loss: 0.0062 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 3/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m127s\u001b[0m 21ms/step - accuracy: 0.9985 - loss: 0.0051 - val_accuracy: 1.0000 - val_loss: 2.5577e-06\n",
      "Epoch 4/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m131s\u001b[0m 21ms/step - accuracy: 0.9987 - loss: 0.0048 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 5/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m132s\u001b[0m 21ms/step - accuracy: 0.9988 - loss: 0.0037 - val_accuracy: 1.0000 - val_loss: 2.5630e-09\n",
      "Epoch 6/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m130s\u001b[0m 21ms/step - accuracy: 0.9988 - loss: 0.0046 - val_accuracy: 1.0000 - val_loss: 1.1146e-08\n",
      "Epoch 7/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 23ms/step - accuracy: 0.9988 - loss: 0.0041 - val_accuracy: 1.0000 - val_loss: 9.3162e-08\n",
      "Epoch 8/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 23ms/step - accuracy: 0.9990 - loss: 0.0035 - val_accuracy: 1.0000 - val_loss: 1.7881e-09\n",
      "Epoch 9/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m148s\u001b[0m 24ms/step - accuracy: 0.9991 - loss: 0.0036 - val_accuracy: 1.0000 - val_loss: 5.0843e-08\n",
      "Epoch 10/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m145s\u001b[0m 23ms/step - accuracy: 0.9990 - loss: 0.0032 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 11/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m146s\u001b[0m 24ms/step - accuracy: 0.9991 - loss: 0.0029 - val_accuracy: 1.0000 - val_loss: 8.3446e-10\n",
      "Epoch 11: early stopping\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step\n",
      "\n",
      "Training fold 5\n",
      "\n",
      "Generating upsampling using SMOTE...\n",
      "200000 samples after upsampling.\n",
      "Class distribution for training after upsampling: Counter({np.int64(0): 6250, np.int64(1): 6250, np.int64(2): 6250, np.int64(3): 6250, np.int64(4): 6250, np.int64(5): 6250, np.int64(6): 6250, np.int64(7): 6250, np.int64(8): 6250, np.int64(9): 6250, np.int64(10): 6250, np.int64(11): 6250, np.int64(12): 6250, np.int64(13): 6250, np.int64(14): 6250, np.int64(15): 6250, np.int64(16): 6250, np.int64(17): 6250, np.int64(18): 6250, np.int64(19): 6250, np.int64(20): 6250, np.int64(21): 6250, np.int64(22): 6250, np.int64(23): 6250, np.int64(24): 6250, np.int64(25): 6250, np.int64(26): 6250, np.int64(27): 6250, np.int64(28): 6250, np.int64(29): 6250, np.int64(30): 6250, np.int64(31): 6250})\n",
      "Finishing upsampling.\n",
      "\n",
      "Epoch 1/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m148s\u001b[0m 24ms/step - accuracy: 0.9960 - loss: 0.0128 - val_accuracy: 1.0000 - val_loss: 3.3677e-08\n",
      "Epoch 2/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m147s\u001b[0m 24ms/step - accuracy: 0.9985 - loss: 0.0047 - val_accuracy: 1.0000 - val_loss: 8.3446e-09\n",
      "Epoch 3/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m150s\u001b[0m 24ms/step - accuracy: 0.9986 - loss: 0.0044 - val_accuracy: 1.0000 - val_loss: 2.9504e-08\n",
      "Epoch 4/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m153s\u001b[0m 25ms/step - accuracy: 0.9986 - loss: 0.0047 - val_accuracy: 1.0000 - val_loss: 8.2254e-09\n",
      "Epoch 5/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m150s\u001b[0m 24ms/step - accuracy: 0.9991 - loss: 0.0039 - val_accuracy: 1.0000 - val_loss: 1.2159e-08\n",
      "Epoch 6/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m152s\u001b[0m 24ms/step - accuracy: 0.9990 - loss: 0.0040 - val_accuracy: 1.0000 - val_loss: 5.9437e-07\n",
      "Epoch 7/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m157s\u001b[0m 25ms/step - accuracy: 0.9989 - loss: 0.0042 - val_accuracy: 1.0000 - val_loss: 1.6093e-09\n",
      "Epoch 8/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m152s\u001b[0m 25ms/step - accuracy: 0.9991 - loss: 0.0039 - val_accuracy: 1.0000 - val_loss: 1.9383e-07\n",
      "Epoch 9/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m172s\u001b[0m 28ms/step - accuracy: 0.9988 - loss: 0.0042 - val_accuracy: 1.0000 - val_loss: 1.9670e-09\n",
      "Epoch 10/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m160s\u001b[0m 26ms/step - accuracy: 0.9991 - loss: 0.0034 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 11/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m164s\u001b[0m 26ms/step - accuracy: 0.9990 - loss: 0.0034 - val_accuracy: 1.0000 - val_loss: 5.6028e-09\n",
      "Epoch 12/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m156s\u001b[0m 25ms/step - accuracy: 0.9992 - loss: 0.0026 - val_accuracy: 1.0000 - val_loss: 1.3590e-08\n",
      "Epoch 13/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m155s\u001b[0m 25ms/step - accuracy: 0.9990 - loss: 0.0029 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 14/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m157s\u001b[0m 25ms/step - accuracy: 0.9991 - loss: 0.0030 - val_accuracy: 1.0000 - val_loss: 4.8518e-08\n",
      "Epoch 15/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m160s\u001b[0m 26ms/step - accuracy: 0.9992 - loss: 0.0026 - val_accuracy: 1.0000 - val_loss: 3.2187e-09\n",
      "Epoch 16/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m159s\u001b[0m 26ms/step - accuracy: 0.9992 - loss: 0.0024 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 17/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m159s\u001b[0m 26ms/step - accuracy: 0.9992 - loss: 0.0025 - val_accuracy: 1.0000 - val_loss: 1.7285e-09\n",
      "Epoch 18/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m162s\u001b[0m 26ms/step - accuracy: 0.9992 - loss: 0.0028 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 19/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m161s\u001b[0m 26ms/step - accuracy: 0.9993 - loss: 0.0028 - val_accuracy: 1.0000 - val_loss: 1.5497e-09\n",
      "Epoch 20/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m163s\u001b[0m 26ms/step - accuracy: 0.9994 - loss: 0.0021 - val_accuracy: 1.0000 - val_loss: 9.5367e-10\n",
      "Epoch 20: early stopping\n",
      "Restoring model weights from the end of the best epoch: 10.\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step\n",
      "\n",
      "Training fold 6\n",
      "\n",
      "Generating upsampling using SMOTE...\n",
      "200000 samples after upsampling.\n",
      "Class distribution for training after upsampling: Counter({np.int64(0): 6250, np.int64(1): 6250, np.int64(2): 6250, np.int64(3): 6250, np.int64(4): 6250, np.int64(5): 6250, np.int64(6): 6250, np.int64(7): 6250, np.int64(8): 6250, np.int64(9): 6250, np.int64(10): 6250, np.int64(11): 6250, np.int64(12): 6250, np.int64(13): 6250, np.int64(14): 6250, np.int64(15): 6250, np.int64(16): 6250, np.int64(17): 6250, np.int64(18): 6250, np.int64(19): 6250, np.int64(20): 6250, np.int64(21): 6250, np.int64(22): 6250, np.int64(23): 6250, np.int64(24): 6250, np.int64(25): 6250, np.int64(26): 6250, np.int64(27): 6250, np.int64(28): 6250, np.int64(29): 6250, np.int64(30): 6250, np.int64(31): 6250})\n",
      "Finishing upsampling.\n",
      "\n",
      "Epoch 1/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m169s\u001b[0m 27ms/step - accuracy: 0.9946 - loss: 0.0186 - val_accuracy: 1.0000 - val_loss: 1.5497e-09\n",
      "Epoch 2/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m167s\u001b[0m 27ms/step - accuracy: 0.9983 - loss: 0.0059 - val_accuracy: 1.0000 - val_loss: 8.1658e-09\n",
      "Epoch 3/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m171s\u001b[0m 28ms/step - accuracy: 0.9987 - loss: 0.0047 - val_accuracy: 1.0000 - val_loss: 4.8697e-08\n",
      "Epoch 4/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m174s\u001b[0m 28ms/step - accuracy: 0.9989 - loss: 0.0041 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 5/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m170s\u001b[0m 28ms/step - accuracy: 0.9990 - loss: 0.0031 - val_accuracy: 1.0000 - val_loss: 8.6844e-08\n",
      "Epoch 6/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m173s\u001b[0m 28ms/step - accuracy: 0.9990 - loss: 0.0038 - val_accuracy: 1.0000 - val_loss: 8.9407e-10\n",
      "Epoch 7/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m178s\u001b[0m 29ms/step - accuracy: 0.9990 - loss: 0.0046 - val_accuracy: 1.0000 - val_loss: 2.4974e-08\n",
      "Epoch 8/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m176s\u001b[0m 29ms/step - accuracy: 0.9991 - loss: 0.0034 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 9/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m182s\u001b[0m 29ms/step - accuracy: 0.9991 - loss: 0.0034 - val_accuracy: 1.0000 - val_loss: 2.9802e-09\n",
      "Epoch 10/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m181s\u001b[0m 29ms/step - accuracy: 0.9992 - loss: 0.0028 - val_accuracy: 1.0000 - val_loss: 2.9802e-10\n",
      "Epoch 11/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m186s\u001b[0m 30ms/step - accuracy: 0.9991 - loss: 0.0029 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 12/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m185s\u001b[0m 30ms/step - accuracy: 0.9993 - loss: 0.0028 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 13/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m188s\u001b[0m 30ms/step - accuracy: 0.9992 - loss: 0.0032 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 14/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m187s\u001b[0m 30ms/step - accuracy: 0.9992 - loss: 0.0033 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 14: early stopping\n",
      "Restoring model weights from the end of the best epoch: 4.\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step\n",
      "\n",
      "Training fold 7\n",
      "\n",
      "Generating upsampling using SMOTE...\n",
      "200000 samples after upsampling.\n",
      "Class distribution for training after upsampling: Counter({np.int64(0): 6250, np.int64(1): 6250, np.int64(2): 6250, np.int64(3): 6250, np.int64(4): 6250, np.int64(5): 6250, np.int64(6): 6250, np.int64(7): 6250, np.int64(8): 6250, np.int64(9): 6250, np.int64(10): 6250, np.int64(11): 6250, np.int64(12): 6250, np.int64(13): 6250, np.int64(14): 6250, np.int64(15): 6250, np.int64(16): 6250, np.int64(17): 6250, np.int64(18): 6250, np.int64(19): 6250, np.int64(20): 6250, np.int64(21): 6250, np.int64(22): 6250, np.int64(23): 6250, np.int64(24): 6250, np.int64(25): 6250, np.int64(26): 6250, np.int64(27): 6250, np.int64(28): 6250, np.int64(29): 6250, np.int64(30): 6250, np.int64(31): 6250})\n",
      "Finishing upsampling.\n",
      "\n",
      "Epoch 1/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m190s\u001b[0m 31ms/step - accuracy: 0.9958 - loss: 0.0158 - val_accuracy: 1.0000 - val_loss: 1.0359e-07\n",
      "Epoch 2/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m190s\u001b[0m 31ms/step - accuracy: 0.9986 - loss: 0.0043 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 3/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m192s\u001b[0m 31ms/step - accuracy: 0.9988 - loss: 0.0042 - val_accuracy: 1.0000 - val_loss: 5.0664e-09\n",
      "Epoch 4/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m193s\u001b[0m 31ms/step - accuracy: 0.9989 - loss: 0.0037 - val_accuracy: 1.0000 - val_loss: 2.3842e-10\n",
      "Epoch 5/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m193s\u001b[0m 31ms/step - accuracy: 0.9991 - loss: 0.0032 - val_accuracy: 1.0000 - val_loss: 6.5565e-10\n",
      "Epoch 6/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m194s\u001b[0m 31ms/step - accuracy: 0.9991 - loss: 0.0031 - val_accuracy: 1.0000 - val_loss: 2.0862e-09\n",
      "Epoch 7/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m199s\u001b[0m 32ms/step - accuracy: 0.9992 - loss: 0.0031 - val_accuracy: 1.0000 - val_loss: 2.1702e-07\n",
      "Epoch 8/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m201s\u001b[0m 32ms/step - accuracy: 0.9994 - loss: 0.0023 - val_accuracy: 1.0000 - val_loss: 4.0233e-08\n",
      "Epoch 9/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m198s\u001b[0m 32ms/step - accuracy: 0.9991 - loss: 0.0027 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 10/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m199s\u001b[0m 32ms/step - accuracy: 0.9993 - loss: 0.0023 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 11/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m203s\u001b[0m 33ms/step - accuracy: 0.9993 - loss: 0.0021 - val_accuracy: 1.0000 - val_loss: 2.5630e-09\n",
      "Epoch 12/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m203s\u001b[0m 33ms/step - accuracy: 0.9992 - loss: 0.0025 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 12: early stopping\n",
      "Restoring model weights from the end of the best epoch: 2.\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step\n",
      "\n",
      "Training fold 8\n",
      "\n",
      "Generating upsampling using SMOTE...\n",
      "200000 samples after upsampling.\n",
      "Class distribution for training after upsampling: Counter({np.int64(0): 6250, np.int64(1): 6250, np.int64(2): 6250, np.int64(3): 6250, np.int64(4): 6250, np.int64(5): 6250, np.int64(6): 6250, np.int64(7): 6250, np.int64(8): 6250, np.int64(9): 6250, np.int64(10): 6250, np.int64(11): 6250, np.int64(12): 6250, np.int64(13): 6250, np.int64(14): 6250, np.int64(15): 6250, np.int64(16): 6250, np.int64(17): 6250, np.int64(18): 6250, np.int64(19): 6250, np.int64(20): 6250, np.int64(21): 6250, np.int64(22): 6250, np.int64(23): 6250, np.int64(24): 6250, np.int64(25): 6250, np.int64(26): 6250, np.int64(27): 6250, np.int64(28): 6250, np.int64(29): 6250, np.int64(30): 6250, np.int64(31): 6250})\n",
      "Finishing upsampling.\n",
      "\n",
      "Epoch 1/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m200s\u001b[0m 32ms/step - accuracy: 0.9974 - loss: 0.0085 - val_accuracy: 1.0000 - val_loss: 5.9605e-10\n",
      "Epoch 2/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m199s\u001b[0m 32ms/step - accuracy: 0.9985 - loss: 0.0051 - val_accuracy: 1.0000 - val_loss: 2.2352e-08\n",
      "Epoch 3/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m199s\u001b[0m 32ms/step - accuracy: 0.9988 - loss: 0.0043 - val_accuracy: 1.0000 - val_loss: 5.9605e-10\n",
      "Epoch 4/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m200s\u001b[0m 32ms/step - accuracy: 0.9990 - loss: 0.0038 - val_accuracy: 1.0000 - val_loss: 5.9605e-11\n",
      "Epoch 5/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m202s\u001b[0m 33ms/step - accuracy: 0.9990 - loss: 0.0032 - val_accuracy: 1.0000 - val_loss: 3.0398e-09\n",
      "Epoch 6/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m202s\u001b[0m 33ms/step - accuracy: 0.9992 - loss: 0.0028 - val_accuracy: 1.0000 - val_loss: 1.1921e-10\n",
      "Epoch 7/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m206s\u001b[0m 33ms/step - accuracy: 0.9993 - loss: 0.0028 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 8/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m205s\u001b[0m 33ms/step - accuracy: 0.9993 - loss: 0.0029 - val_accuracy: 1.0000 - val_loss: 4.4107e-09\n",
      "Epoch 9/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m207s\u001b[0m 33ms/step - accuracy: 0.9993 - loss: 0.0025 - val_accuracy: 1.0000 - val_loss: 8.7023e-09\n",
      "Epoch 10/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m209s\u001b[0m 34ms/step - accuracy: 0.9993 - loss: 0.0026 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 11/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m212s\u001b[0m 34ms/step - accuracy: 0.9992 - loss: 0.0028 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 12/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m211s\u001b[0m 34ms/step - accuracy: 0.9991 - loss: 0.0028 - val_accuracy: 1.0000 - val_loss: 5.9605e-11\n",
      "Epoch 13/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m210s\u001b[0m 34ms/step - accuracy: 0.9993 - loss: 0.0021 - val_accuracy: 1.0000 - val_loss: 2.5034e-09\n",
      "Epoch 14/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m211s\u001b[0m 34ms/step - accuracy: 0.9992 - loss: 0.0033 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 15/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m212s\u001b[0m 34ms/step - accuracy: 0.9993 - loss: 0.0026 - val_accuracy: 1.0000 - val_loss: 1.7881e-10\n",
      "Epoch 16/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m216s\u001b[0m 35ms/step - accuracy: 0.9994 - loss: 0.0022 - val_accuracy: 1.0000 - val_loss: 1.9908e-08\n",
      "Epoch 17/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m217s\u001b[0m 35ms/step - accuracy: 0.9995 - loss: 0.0020 - val_accuracy: 1.0000 - val_loss: 1.1563e-08\n",
      "Epoch 17: early stopping\n",
      "Restoring model weights from the end of the best epoch: 7.\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step\n",
      "\n",
      "Training fold 9\n",
      "\n",
      "Generating upsampling using SMOTE...\n",
      "200000 samples after upsampling.\n",
      "Class distribution for training after upsampling: Counter({np.int64(0): 6250, np.int64(1): 6250, np.int64(2): 6250, np.int64(3): 6250, np.int64(4): 6250, np.int64(5): 6250, np.int64(6): 6250, np.int64(7): 6250, np.int64(8): 6250, np.int64(9): 6250, np.int64(10): 6250, np.int64(11): 6250, np.int64(12): 6250, np.int64(13): 6250, np.int64(14): 6250, np.int64(15): 6250, np.int64(16): 6250, np.int64(17): 6250, np.int64(18): 6250, np.int64(19): 6250, np.int64(20): 6250, np.int64(21): 6250, np.int64(22): 6250, np.int64(23): 6250, np.int64(24): 6250, np.int64(25): 6250, np.int64(26): 6250, np.int64(27): 6250, np.int64(28): 6250, np.int64(29): 6250, np.int64(30): 6250, np.int64(31): 6250})\n",
      "Finishing upsampling.\n",
      "\n",
      "Epoch 1/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m225s\u001b[0m 36ms/step - accuracy: 0.9962 - loss: 0.0135 - val_accuracy: 1.0000 - val_loss: 1.1206e-08\n",
      "Epoch 2/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m224s\u001b[0m 36ms/step - accuracy: 0.9985 - loss: 0.0047 - val_accuracy: 1.0000 - val_loss: 1.6570e-08\n",
      "Epoch 3/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m223s\u001b[0m 36ms/step - accuracy: 0.9990 - loss: 0.0038 - val_accuracy: 1.0000 - val_loss: 4.6492e-09\n",
      "Epoch 4/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m224s\u001b[0m 36ms/step - accuracy: 0.9990 - loss: 0.0036 - val_accuracy: 1.0000 - val_loss: 1.7881e-10\n",
      "Epoch 5/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m226s\u001b[0m 37ms/step - accuracy: 0.9991 - loss: 0.0032 - val_accuracy: 1.0000 - val_loss: 5.7220e-09\n",
      "Epoch 6/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m226s\u001b[0m 37ms/step - accuracy: 0.9992 - loss: 0.0036 - val_accuracy: 1.0000 - val_loss: 1.1742e-08\n",
      "Epoch 7/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m227s\u001b[0m 37ms/step - accuracy: 0.9993 - loss: 0.0031 - val_accuracy: 1.0000 - val_loss: 3.4571e-09\n",
      "Epoch 8/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m230s\u001b[0m 37ms/step - accuracy: 0.9993 - loss: 0.0024 - val_accuracy: 1.0000 - val_loss: 1.6689e-08\n",
      "Epoch 9/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m230s\u001b[0m 37ms/step - accuracy: 0.9993 - loss: 0.0022 - val_accuracy: 1.0000 - val_loss: 4.1783e-08\n",
      "Epoch 10/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m234s\u001b[0m 38ms/step - accuracy: 0.9994 - loss: 0.0026 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 11/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m235s\u001b[0m 38ms/step - accuracy: 0.9994 - loss: 0.0021 - val_accuracy: 1.0000 - val_loss: 6.7412e-08\n",
      "Epoch 12/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m240s\u001b[0m 39ms/step - accuracy: 0.9993 - loss: 0.0029 - val_accuracy: 1.0000 - val_loss: 1.9670e-09\n",
      "Epoch 13/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m241s\u001b[0m 39ms/step - accuracy: 0.9994 - loss: 0.0023 - val_accuracy: 1.0000 - val_loss: 2.4259e-08\n",
      "Epoch 14/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m238s\u001b[0m 38ms/step - accuracy: 0.9992 - loss: 0.0031 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 15/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m238s\u001b[0m 38ms/step - accuracy: 0.9994 - loss: 0.0021 - val_accuracy: 1.0000 - val_loss: 3.6359e-09\n",
      "Epoch 16/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m241s\u001b[0m 39ms/step - accuracy: 0.9993 - loss: 0.0025 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 17/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m258s\u001b[0m 38ms/step - accuracy: 0.9996 - loss: 0.0016 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 18/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m241s\u001b[0m 39ms/step - accuracy: 0.9994 - loss: 0.0020 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 19/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m242s\u001b[0m 39ms/step - accuracy: 0.9995 - loss: 0.0021 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 20/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m244s\u001b[0m 39ms/step - accuracy: 0.9994 - loss: 0.0023 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 20: early stopping\n",
      "Restoring model weights from the end of the best epoch: 10.\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step\n",
      "\n",
      "Training fold 10\n",
      "\n",
      "Generating upsampling using SMOTE...\n",
      "200000 samples after upsampling.\n",
      "Class distribution for training after upsampling: Counter({np.int64(0): 6250, np.int64(1): 6250, np.int64(2): 6250, np.int64(3): 6250, np.int64(4): 6250, np.int64(5): 6250, np.int64(6): 6250, np.int64(7): 6250, np.int64(8): 6250, np.int64(9): 6250, np.int64(10): 6250, np.int64(11): 6250, np.int64(12): 6250, np.int64(13): 6250, np.int64(14): 6250, np.int64(15): 6250, np.int64(16): 6250, np.int64(17): 6250, np.int64(18): 6250, np.int64(19): 6250, np.int64(20): 6250, np.int64(21): 6250, np.int64(22): 6250, np.int64(23): 6250, np.int64(24): 6250, np.int64(25): 6250, np.int64(26): 6250, np.int64(27): 6250, np.int64(28): 6250, np.int64(29): 6250, np.int64(30): 6250, np.int64(31): 6250})\n",
      "Finishing upsampling.\n",
      "\n",
      "Epoch 1/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m254s\u001b[0m 41ms/step - accuracy: 0.9971 - loss: 0.0104 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 2/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m250s\u001b[0m 40ms/step - accuracy: 0.9989 - loss: 0.0037 - val_accuracy: 1.0000 - val_loss: 3.5167e-09\n",
      "Epoch 3/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m249s\u001b[0m 40ms/step - accuracy: 0.9990 - loss: 0.0034 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 4/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m250s\u001b[0m 40ms/step - accuracy: 0.9992 - loss: 0.0026 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 5/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m250s\u001b[0m 40ms/step - accuracy: 0.9992 - loss: 0.0026 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 6/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m251s\u001b[0m 41ms/step - accuracy: 0.9994 - loss: 0.0026 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 7/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m252s\u001b[0m 41ms/step - accuracy: 0.9995 - loss: 0.0021 - val_accuracy: 1.0000 - val_loss: 3.5763e-10\n",
      "Epoch 8/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m252s\u001b[0m 41ms/step - accuracy: 0.9994 - loss: 0.0022 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 9/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m256s\u001b[0m 41ms/step - accuracy: 0.9993 - loss: 0.0026 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 10/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m255s\u001b[0m 41ms/step - accuracy: 0.9992 - loss: 0.0028 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 11/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m257s\u001b[0m 42ms/step - accuracy: 0.9994 - loss: 0.0020 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 11: early stopping\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step\n",
      "\n",
      "Time taken for training:  08:33:10\n",
      "\n",
      "\n",
      "Fold 1 - Train Accuracy 0.9996 - Test Accuracy 0.6008\n",
      "Fold 2 - Train Accuracy 0.9994 - Test Accuracy 0.8532\n",
      "Fold 3 - Train Accuracy 0.9994 - Test Accuracy 0.9503\n",
      "Fold 4 - Train Accuracy 0.9993 - Test Accuracy 0.9705\n",
      "Fold 5 - Train Accuracy 0.9966 - Test Accuracy 0.8392\n",
      "Fold 6 - Train Accuracy 0.9998 - Test Accuracy 0.9623\n",
      "Fold 7 - Train Accuracy 0.9995 - Test Accuracy 0.9720\n",
      "Fold 8 - Train Accuracy 0.9997 - Test Accuracy 0.9584\n",
      "Fold 9 - Train Accuracy 0.9968 - Test Accuracy 0.8714\n",
      "Fold 10 - Train Accuracy 0.9994 - Test Accuracy 0.9845\n",
      "\n",
      "Mean Train Accuracy: 0.9990 - Std: 0.0011 \n",
      "Mean Test Accuracy: 0.8963 - Std: 0.1110 \n",
      "\n",
      "Evaluate other metrics:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.92      0.93     13905\n",
      "           1       0.91      0.96      0.93       723\n",
      "           2       0.94      0.97      0.96      2659\n",
      "           3       0.79      0.78      0.79      1123\n",
      "           4       0.87      0.79      0.83       384\n",
      "           5       0.95      0.82      0.88        44\n",
      "           6       0.87      0.82      0.85       663\n",
      "           7       0.87      0.87      0.87        94\n",
      "           8       1.00      0.92      0.96        12\n",
      "           9       0.84      0.84      0.84       786\n",
      "          10       0.89      0.89      0.89         9\n",
      "          11       0.91      0.84      0.87        98\n",
      "          12       0.64      0.88      0.74         8\n",
      "          13       0.73      0.80      0.76        20\n",
      "          14       0.81      0.91      0.86        77\n",
      "          15       0.88      0.92      0.90        62\n",
      "          16       0.73      0.84      0.79       917\n",
      "          17       0.75      0.89      0.82       473\n",
      "          18       0.91      0.91      0.91        22\n",
      "          19       0.74      0.84      0.78       122\n",
      "          20       0.76      0.85      0.80       111\n",
      "          21       0.76      0.84      0.79       201\n",
      "          22       0.39      1.00      0.56         7\n",
      "          23       0.85      0.85      0.85        96\n",
      "          24       0.86      0.79      0.82      1045\n",
      "          25       0.81      0.84      0.82       540\n",
      "          26       0.78      0.87      0.82      1334\n",
      "          27       0.76      0.89      0.82        28\n",
      "          28       0.79      0.94      0.86        35\n",
      "          29       0.85      0.86      0.85        77\n",
      "          30       0.93      0.84      0.89        64\n",
      "          31       0.67      0.86      0.75         7\n",
      "\n",
      "    accuracy                           0.90     25746\n",
      "   macro avg       0.82      0.87      0.84     25746\n",
      "weighted avg       0.90      0.90      0.90     25746\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Build a \"time window\" structure to handle the dataset as a time series.\n",
    "new_df = load_dataset(\"V5\")\n",
    "X_array, y_array = build_time_window_structure(new_df, \"V5\")\n",
    "X_array, y_array = remove_classes_with_less_samples(X_array, y_array)\n",
    "\n",
    "# Train the CNN model.\n",
    "v1_model = create_v1()\n",
    "v1_num_epochs = 300\n",
    "v1_batch_size = 32\n",
    "v1_validation_split = 0.01\n",
    "\n",
    "training_history_v1 = train_cnn_model(v1_model, X_array, y_array, v1_num_epochs, v1_batch_size, v1_validation_split, \"v1_model_V5.keras\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f5e894-ae42-4873-98a3-1fea5d538033",
   "metadata": {},
   "source": [
    "#### V6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "604f6dc2-b2e7-4688-9aff-c4a8874d5f82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Start loading CSV file...\n",
      "Finish loading CSV file.\n",
      "\n",
      "Starting build_time_window_structure function...\n",
      "\n",
      "Shape of features:  (25770, 1250)\n",
      "Quantity os samples (labels):  25770\n",
      "\n",
      "Finishing build_time_window_structure function.\n",
      "\n",
      "Remove classes with less than 6 samples.\n",
      "\n",
      "Show classes identification:\n",
      "Class: 0 - Arrhythmia code: 1\n",
      "Class: 1 - Arrhythmia code: 21\n",
      "Class: 2 - Arrhythmia code: 22\n",
      "Class: 3 - Arrhythmia code: 23\n",
      "Class: 4 - Arrhythmia code: 30\n",
      "Class: 5 - Arrhythmia code: 36\n",
      "Class: 6 - Arrhythmia code: 50\n",
      "Class: 7 - Arrhythmia code: 51\n",
      "Class: 8 - Arrhythmia code: 54\n",
      "Class: 9 - Arrhythmia code: 60\n",
      "Class: 10 - Arrhythmia code: 80\n",
      "Class: 11 - Arrhythmia code: 82\n",
      "Class: 12 - Arrhythmia code: 83\n",
      "Class: 13 - Arrhythmia code: 88\n",
      "Class: 14 - Arrhythmia code: 101\n",
      "Class: 15 - Arrhythmia code: 104\n",
      "Class: 16 - Arrhythmia code: 105\n",
      "Class: 17 - Arrhythmia code: 106\n",
      "Class: 18 - Arrhythmia code: 108\n",
      "Class: 19 - Arrhythmia code: 120\n",
      "Class: 20 - Arrhythmia code: 121\n",
      "Class: 21 - Arrhythmia code: 125\n",
      "Class: 22 - Arrhythmia code: 140\n",
      "Class: 23 - Arrhythmia code: 142\n",
      "Class: 24 - Arrhythmia code: 145\n",
      "Class: 25 - Arrhythmia code: 146\n",
      "Class: 26 - Arrhythmia code: 147\n",
      "Class: 27 - Arrhythmia code: 155\n",
      "Class: 28 - Arrhythmia code: 160\n",
      "Class: 29 - Arrhythmia code: 161\n",
      "Class: 30 - Arrhythmia code: 165\n",
      "Class: 31 - Arrhythmia code: 166\n",
      "\n",
      "Check for dataset balance:\n",
      "Class =   0   Qty =    13905   Percentage = 54.01 %\n",
      "Class =   2   Qty =     2659   Percentage = 10.33 %\n",
      "Class =  26   Qty =     1334   Percentage = 5.18 %\n",
      "Class =   3   Qty =     1123   Percentage = 4.36 %\n",
      "Class =  24   Qty =     1045   Percentage = 4.06 %\n",
      "Class =  16   Qty =      917   Percentage = 3.56 %\n",
      "Class =   9   Qty =      786   Percentage = 3.05 %\n",
      "Class =   1   Qty =      723   Percentage = 2.81 %\n",
      "Class =   6   Qty =      663   Percentage = 2.58 %\n",
      "Class =  25   Qty =      540   Percentage = 2.10 %\n",
      "Class =  17   Qty =      473   Percentage = 1.84 %\n",
      "Class =   4   Qty =      384   Percentage = 1.49 %\n",
      "Class =  21   Qty =      201   Percentage = 0.78 %\n",
      "Class =  19   Qty =      122   Percentage = 0.47 %\n",
      "Class =  20   Qty =      111   Percentage = 0.43 %\n",
      "Class =  11   Qty =       98   Percentage = 0.38 %\n",
      "Class =  23   Qty =       96   Percentage = 0.37 %\n",
      "Class =   7   Qty =       94   Percentage = 0.37 %\n",
      "Class =  14   Qty =       77   Percentage = 0.30 %\n",
      "Class =  29   Qty =       77   Percentage = 0.30 %\n",
      "Class =  30   Qty =       64   Percentage = 0.25 %\n",
      "Class =  15   Qty =       62   Percentage = 0.24 %\n",
      "Class =   5   Qty =       44   Percentage = 0.17 %\n",
      "Class =  28   Qty =       35   Percentage = 0.14 %\n",
      "Class =  27   Qty =       28   Percentage = 0.11 %\n",
      "Class =  18   Qty =       22   Percentage = 0.09 %\n",
      "Class =  13   Qty =       20   Percentage = 0.08 %\n",
      "Class =   8   Qty =       12   Percentage = 0.05 %\n",
      "Class =  10   Qty =        9   Percentage = 0.03 %\n",
      "Class =  12   Qty =        8   Percentage = 0.03 %\n",
      "Class =  22   Qty =        7   Percentage = 0.03 %\n",
      "Class =  31   Qty =        7   Percentage = 0.03 %\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1248</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)        │            <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1248</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)        │            <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1246</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)        │           <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1246</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)        │            <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">623</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">619</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)        │           <span style=\"color: #00af00; text-decoration-color: #00af00\">656</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_2           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">619</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)        │            <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">615</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)        │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,296</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_3           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">615</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)        │            <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">307</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4912</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">628,864</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_4           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,128</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv1d (\u001b[38;5;33mConv1D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1248\u001b[0m, \u001b[38;5;34m8\u001b[0m)        │            \u001b[38;5;34m32\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1248\u001b[0m, \u001b[38;5;34m8\u001b[0m)        │            \u001b[38;5;34m32\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_1 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1246\u001b[0m, \u001b[38;5;34m8\u001b[0m)        │           \u001b[38;5;34m200\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_1           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1246\u001b[0m, \u001b[38;5;34m8\u001b[0m)        │            \u001b[38;5;34m32\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d (\u001b[38;5;33mMaxPooling1D\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m623\u001b[0m, \u001b[38;5;34m8\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_2 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m619\u001b[0m, \u001b[38;5;34m16\u001b[0m)        │           \u001b[38;5;34m656\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_2           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m619\u001b[0m, \u001b[38;5;34m16\u001b[0m)        │            \u001b[38;5;34m64\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_3 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m615\u001b[0m, \u001b[38;5;34m16\u001b[0m)        │         \u001b[38;5;34m1,296\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_3           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m615\u001b[0m, \u001b[38;5;34m16\u001b[0m)        │            \u001b[38;5;34m64\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_1 (\u001b[38;5;33mMaxPooling1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m307\u001b[0m, \u001b[38;5;34m16\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4912\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │       \u001b[38;5;34m628,864\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_4           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │           \u001b[38;5;34m512\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │         \u001b[38;5;34m4,128\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">635,880</span> (2.43 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m635,880\u001b[0m (2.43 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">635,528</span> (2.42 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m635,528\u001b[0m (2.42 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">352</span> (1.38 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m352\u001b[0m (1.38 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting training...\n",
      "\n",
      "Training fold 1\n",
      "\n",
      "Generating upsampling using SMOTE...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\DeveloperTools\\python\\3.11.0\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:737: UserWarning: The least populated class in y has only 7 members, which is less than n_splits=10.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200000 samples after upsampling.\n",
      "Class distribution for training after upsampling: Counter({np.int64(0): 6250, np.int64(1): 6250, np.int64(2): 6250, np.int64(3): 6250, np.int64(4): 6250, np.int64(5): 6250, np.int64(6): 6250, np.int64(7): 6250, np.int64(8): 6250, np.int64(9): 6250, np.int64(10): 6250, np.int64(11): 6250, np.int64(12): 6250, np.int64(13): 6250, np.int64(14): 6250, np.int64(15): 6250, np.int64(16): 6250, np.int64(17): 6250, np.int64(18): 6250, np.int64(19): 6250, np.int64(20): 6250, np.int64(21): 6250, np.int64(22): 6250, np.int64(23): 6250, np.int64(24): 6250, np.int64(25): 6250, np.int64(26): 6250, np.int64(27): 6250, np.int64(28): 6250, np.int64(29): 6250, np.int64(30): 6250, np.int64(31): 6250})\n",
      "Finishing upsampling.\n",
      "\n",
      "Epoch 1/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 17ms/step - accuracy: 0.8635 - loss: 0.5306 - val_accuracy: 1.0000 - val_loss: 0.0056\n",
      "Epoch 2/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m103s\u001b[0m 17ms/step - accuracy: 0.9746 - loss: 0.0825 - val_accuracy: 0.9805 - val_loss: 0.0632\n",
      "Epoch 3/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m103s\u001b[0m 17ms/step - accuracy: 0.9864 - loss: 0.0436 - val_accuracy: 1.0000 - val_loss: 2.7566e-04\n",
      "Epoch 4/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m103s\u001b[0m 17ms/step - accuracy: 0.9910 - loss: 0.0288 - val_accuracy: 1.0000 - val_loss: 7.8378e-05\n",
      "Epoch 5/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 17ms/step - accuracy: 0.9932 - loss: 0.0216 - val_accuracy: 1.0000 - val_loss: 7.2398e-05\n",
      "Epoch 6/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 17ms/step - accuracy: 0.9942 - loss: 0.0180 - val_accuracy: 1.0000 - val_loss: 1.6315e-06\n",
      "Epoch 7/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 17ms/step - accuracy: 0.9949 - loss: 0.0154 - val_accuracy: 1.0000 - val_loss: 1.3083e-07\n",
      "Epoch 8/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 17ms/step - accuracy: 0.9955 - loss: 0.0139 - val_accuracy: 0.9975 - val_loss: 0.0093\n",
      "Epoch 9/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 17ms/step - accuracy: 0.9964 - loss: 0.0114 - val_accuracy: 1.0000 - val_loss: 1.5006e-06\n",
      "Epoch 10/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 17ms/step - accuracy: 0.9967 - loss: 0.0112 - val_accuracy: 1.0000 - val_loss: 1.1786e-04\n",
      "Epoch 11/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 17ms/step - accuracy: 0.9968 - loss: 0.0101 - val_accuracy: 0.9610 - val_loss: 0.1239\n",
      "Epoch 12/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 17ms/step - accuracy: 0.9972 - loss: 0.0090 - val_accuracy: 1.0000 - val_loss: 7.8070e-05\n",
      "Epoch 13/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 17ms/step - accuracy: 0.9976 - loss: 0.0072 - val_accuracy: 0.9940 - val_loss: 0.0162\n",
      "Epoch 14/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 18ms/step - accuracy: 0.9974 - loss: 0.0083 - val_accuracy: 1.0000 - val_loss: 1.1525e-04\n",
      "Epoch 15/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 17ms/step - accuracy: 0.9979 - loss: 0.0073 - val_accuracy: 1.0000 - val_loss: 9.7036e-07\n",
      "Epoch 16/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 17ms/step - accuracy: 0.9977 - loss: 0.0069 - val_accuracy: 1.0000 - val_loss: 1.8616e-05\n",
      "Epoch 17/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 17ms/step - accuracy: 0.9979 - loss: 0.0072 - val_accuracy: 0.9080 - val_loss: 0.3524\n",
      "Epoch 17: early stopping\n",
      "Restoring model weights from the end of the best epoch: 7.\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step\n",
      "\n",
      "Training fold 2\n",
      "\n",
      "Generating upsampling using SMOTE...\n",
      "200000 samples after upsampling.\n",
      "Class distribution for training after upsampling: Counter({np.int64(0): 6250, np.int64(1): 6250, np.int64(2): 6250, np.int64(3): 6250, np.int64(4): 6250, np.int64(5): 6250, np.int64(6): 6250, np.int64(7): 6250, np.int64(8): 6250, np.int64(9): 6250, np.int64(10): 6250, np.int64(11): 6250, np.int64(12): 6250, np.int64(13): 6250, np.int64(14): 6250, np.int64(15): 6250, np.int64(16): 6250, np.int64(17): 6250, np.int64(18): 6250, np.int64(19): 6250, np.int64(20): 6250, np.int64(21): 6250, np.int64(22): 6250, np.int64(23): 6250, np.int64(24): 6250, np.int64(25): 6250, np.int64(26): 6250, np.int64(27): 6250, np.int64(28): 6250, np.int64(29): 6250, np.int64(30): 6250, np.int64(31): 6250})\n",
      "Finishing upsampling.\n",
      "\n",
      "Epoch 1/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 18ms/step - accuracy: 0.9681 - loss: 0.1305 - val_accuracy: 1.0000 - val_loss: 4.4564e-06\n",
      "Epoch 2/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 18ms/step - accuracy: 0.9933 - loss: 0.0220 - val_accuracy: 1.0000 - val_loss: 7.4470e-06\n",
      "Epoch 3/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 18ms/step - accuracy: 0.9956 - loss: 0.0143 - val_accuracy: 1.0000 - val_loss: 3.1412e-08\n",
      "Epoch 4/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 17ms/step - accuracy: 0.9964 - loss: 0.0116 - val_accuracy: 0.9730 - val_loss: 0.0833\n",
      "Epoch 5/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 17ms/step - accuracy: 0.9968 - loss: 0.0103 - val_accuracy: 1.0000 - val_loss: 2.0253e-04\n",
      "Epoch 6/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 18ms/step - accuracy: 0.9968 - loss: 0.0103 - val_accuracy: 0.9920 - val_loss: 0.0145\n",
      "Epoch 7/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m110s\u001b[0m 18ms/step - accuracy: 0.9974 - loss: 0.0083 - val_accuracy: 1.0000 - val_loss: 7.6877e-06\n",
      "Epoch 8/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m110s\u001b[0m 18ms/step - accuracy: 0.9977 - loss: 0.0073 - val_accuracy: 0.9940 - val_loss: 0.0217\n",
      "Epoch 9/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 18ms/step - accuracy: 0.9978 - loss: 0.0078 - val_accuracy: 1.0000 - val_loss: 4.1092e-04\n",
      "Epoch 10/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m111s\u001b[0m 18ms/step - accuracy: 0.9980 - loss: 0.0061 - val_accuracy: 1.0000 - val_loss: 4.3210e-06\n",
      "Epoch 11/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m111s\u001b[0m 18ms/step - accuracy: 0.9981 - loss: 0.0075 - val_accuracy: 1.0000 - val_loss: 2.0688e-04\n",
      "Epoch 12/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m110s\u001b[0m 18ms/step - accuracy: 0.9982 - loss: 0.0067 - val_accuracy: 1.0000 - val_loss: 6.1420e-05\n",
      "Epoch 13/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 18ms/step - accuracy: 0.9982 - loss: 0.0057 - val_accuracy: 1.0000 - val_loss: 3.5405e-08\n",
      "Epoch 13: early stopping\n",
      "Restoring model weights from the end of the best epoch: 3.\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\n",
      "Training fold 3\n",
      "\n",
      "Generating upsampling using SMOTE...\n",
      "200000 samples after upsampling.\n",
      "Class distribution for training after upsampling: Counter({np.int64(0): 6250, np.int64(1): 6250, np.int64(2): 6250, np.int64(3): 6250, np.int64(4): 6250, np.int64(5): 6250, np.int64(6): 6250, np.int64(7): 6250, np.int64(8): 6250, np.int64(9): 6250, np.int64(10): 6250, np.int64(11): 6250, np.int64(12): 6250, np.int64(13): 6250, np.int64(14): 6250, np.int64(15): 6250, np.int64(16): 6250, np.int64(17): 6250, np.int64(18): 6250, np.int64(19): 6250, np.int64(20): 6250, np.int64(21): 6250, np.int64(22): 6250, np.int64(23): 6250, np.int64(24): 6250, np.int64(25): 6250, np.int64(26): 6250, np.int64(27): 6250, np.int64(28): 6250, np.int64(29): 6250, np.int64(30): 6250, np.int64(31): 6250})\n",
      "Finishing upsampling.\n",
      "\n",
      "Epoch 1/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m112s\u001b[0m 18ms/step - accuracy: 0.9864 - loss: 0.0469 - val_accuracy: 1.0000 - val_loss: 5.0853e-06\n",
      "Epoch 2/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m111s\u001b[0m 18ms/step - accuracy: 0.9961 - loss: 0.0125 - val_accuracy: 1.0000 - val_loss: 2.4773e-05\n",
      "Epoch 3/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m113s\u001b[0m 18ms/step - accuracy: 0.9972 - loss: 0.0097 - val_accuracy: 1.0000 - val_loss: 3.0264e-06\n",
      "Epoch 4/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m111s\u001b[0m 18ms/step - accuracy: 0.9972 - loss: 0.0094 - val_accuracy: 0.9500 - val_loss: 0.1130\n",
      "Epoch 5/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m111s\u001b[0m 18ms/step - accuracy: 0.9976 - loss: 0.0085 - val_accuracy: 0.9290 - val_loss: 0.2514\n",
      "Epoch 6/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m111s\u001b[0m 18ms/step - accuracy: 0.9975 - loss: 0.0084 - val_accuracy: 1.0000 - val_loss: 9.1444e-04\n",
      "Epoch 7/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m113s\u001b[0m 18ms/step - accuracy: 0.9981 - loss: 0.0067 - val_accuracy: 1.0000 - val_loss: 1.8287e-06\n",
      "Epoch 8/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m112s\u001b[0m 18ms/step - accuracy: 0.9982 - loss: 0.0064 - val_accuracy: 1.0000 - val_loss: 3.8835e-04\n",
      "Epoch 9/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m114s\u001b[0m 18ms/step - accuracy: 0.9981 - loss: 0.0069 - val_accuracy: 1.0000 - val_loss: 5.3326e-06\n",
      "Epoch 10/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m115s\u001b[0m 19ms/step - accuracy: 0.9985 - loss: 0.0052 - val_accuracy: 1.0000 - val_loss: 1.0120e-06\n",
      "Epoch 11/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m116s\u001b[0m 19ms/step - accuracy: 0.9983 - loss: 0.0052 - val_accuracy: 1.0000 - val_loss: 0.0055\n",
      "Epoch 12/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m114s\u001b[0m 18ms/step - accuracy: 0.9982 - loss: 0.0062 - val_accuracy: 1.0000 - val_loss: 5.2530e-06\n",
      "Epoch 13/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m113s\u001b[0m 18ms/step - accuracy: 0.9984 - loss: 0.0056 - val_accuracy: 1.0000 - val_loss: 0.0143\n",
      "Epoch 14/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m115s\u001b[0m 19ms/step - accuracy: 0.9984 - loss: 0.0055 - val_accuracy: 1.0000 - val_loss: 0.0181\n",
      "Epoch 15/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m113s\u001b[0m 18ms/step - accuracy: 0.9987 - loss: 0.0041 - val_accuracy: 1.0000 - val_loss: 5.7280e-08\n",
      "Epoch 16/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m115s\u001b[0m 19ms/step - accuracy: 0.9988 - loss: 0.0044 - val_accuracy: 1.0000 - val_loss: 2.9206e-09\n",
      "Epoch 17/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m116s\u001b[0m 19ms/step - accuracy: 0.9988 - loss: 0.0048 - val_accuracy: 1.0000 - val_loss: 4.2915e-09\n",
      "Epoch 18/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m116s\u001b[0m 19ms/step - accuracy: 0.9984 - loss: 0.0059 - val_accuracy: 1.0000 - val_loss: 1.8308e-04\n",
      "Epoch 19/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m117s\u001b[0m 19ms/step - accuracy: 0.9988 - loss: 0.0040 - val_accuracy: 1.0000 - val_loss: 1.7995e-07\n",
      "Epoch 20/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m117s\u001b[0m 19ms/step - accuracy: 0.9986 - loss: 0.0050 - val_accuracy: 1.0000 - val_loss: 3.7735e-04\n",
      "Epoch 21/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m117s\u001b[0m 19ms/step - accuracy: 0.9990 - loss: 0.0041 - val_accuracy: 0.9775 - val_loss: 0.0690\n",
      "Epoch 22/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m119s\u001b[0m 19ms/step - accuracy: 0.9988 - loss: 0.0039 - val_accuracy: 0.9710 - val_loss: 0.0677\n",
      "Epoch 23/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m120s\u001b[0m 19ms/step - accuracy: 0.9988 - loss: 0.0039 - val_accuracy: 1.0000 - val_loss: 4.3511e-09\n",
      "Epoch 24/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m119s\u001b[0m 19ms/step - accuracy: 0.9990 - loss: 0.0035 - val_accuracy: 1.0000 - val_loss: 0.0013\n",
      "Epoch 25/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m122s\u001b[0m 20ms/step - accuracy: 0.9989 - loss: 0.0042 - val_accuracy: 1.0000 - val_loss: 8.3437e-07\n",
      "Epoch 26/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m121s\u001b[0m 20ms/step - accuracy: 0.9988 - loss: 0.0043 - val_accuracy: 1.0000 - val_loss: 6.1810e-06\n",
      "Epoch 26: early stopping\n",
      "Restoring model weights from the end of the best epoch: 16.\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\n",
      "Training fold 4\n",
      "\n",
      "Generating upsampling using SMOTE...\n",
      "200000 samples after upsampling.\n",
      "Class distribution for training after upsampling: Counter({np.int64(0): 6250, np.int64(1): 6250, np.int64(2): 6250, np.int64(3): 6250, np.int64(4): 6250, np.int64(5): 6250, np.int64(6): 6250, np.int64(7): 6250, np.int64(8): 6250, np.int64(9): 6250, np.int64(10): 6250, np.int64(11): 6250, np.int64(12): 6250, np.int64(13): 6250, np.int64(14): 6250, np.int64(15): 6250, np.int64(16): 6250, np.int64(17): 6250, np.int64(18): 6250, np.int64(19): 6250, np.int64(20): 6250, np.int64(21): 6250, np.int64(22): 6250, np.int64(23): 6250, np.int64(24): 6250, np.int64(25): 6250, np.int64(26): 6250, np.int64(27): 6250, np.int64(28): 6250, np.int64(29): 6250, np.int64(30): 6250, np.int64(31): 6250})\n",
      "Finishing upsampling.\n",
      "\n",
      "Epoch 1/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m124s\u001b[0m 20ms/step - accuracy: 0.9864 - loss: 0.0599 - val_accuracy: 1.0000 - val_loss: 1.1667e-04\n",
      "Epoch 2/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m121s\u001b[0m 20ms/step - accuracy: 0.9966 - loss: 0.0111 - val_accuracy: 0.8780 - val_loss: 0.2998\n",
      "Epoch 3/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m123s\u001b[0m 20ms/step - accuracy: 0.9978 - loss: 0.0070 - val_accuracy: 0.9905 - val_loss: 0.0278\n",
      "Epoch 4/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m125s\u001b[0m 20ms/step - accuracy: 0.9980 - loss: 0.0058 - val_accuracy: 0.9975 - val_loss: 0.0083\n",
      "Epoch 5/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m138s\u001b[0m 20ms/step - accuracy: 0.9983 - loss: 0.0063 - val_accuracy: 1.0000 - val_loss: 5.2988e-08\n",
      "Epoch 6/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m124s\u001b[0m 20ms/step - accuracy: 0.9984 - loss: 0.0049 - val_accuracy: 1.0000 - val_loss: 0.0020\n",
      "Epoch 7/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m123s\u001b[0m 20ms/step - accuracy: 0.9988 - loss: 0.0049 - val_accuracy: 1.0000 - val_loss: 3.2163e-06\n",
      "Epoch 8/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m124s\u001b[0m 20ms/step - accuracy: 0.9985 - loss: 0.0047 - val_accuracy: 1.0000 - val_loss: 7.6314e-07\n",
      "Epoch 9/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m128s\u001b[0m 21ms/step - accuracy: 0.9985 - loss: 0.0052 - val_accuracy: 1.0000 - val_loss: 6.8588e-04\n",
      "Epoch 10/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m127s\u001b[0m 21ms/step - accuracy: 0.9989 - loss: 0.0040 - val_accuracy: 0.3125 - val_loss: 3.5870\n",
      "Epoch 11/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m130s\u001b[0m 21ms/step - accuracy: 0.9988 - loss: 0.0039 - val_accuracy: 1.0000 - val_loss: 3.9502e-05\n",
      "Epoch 12/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m129s\u001b[0m 21ms/step - accuracy: 0.9990 - loss: 0.0033 - val_accuracy: 1.0000 - val_loss: 3.7695e-06\n",
      "Epoch 13/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m128s\u001b[0m 21ms/step - accuracy: 0.9989 - loss: 0.0038 - val_accuracy: 0.9225 - val_loss: 0.2285\n",
      "Epoch 14/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m128s\u001b[0m 21ms/step - accuracy: 0.9989 - loss: 0.0037 - val_accuracy: 1.0000 - val_loss: 1.5950e-07\n",
      "Epoch 15/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m129s\u001b[0m 21ms/step - accuracy: 0.9990 - loss: 0.0037 - val_accuracy: 1.0000 - val_loss: 4.1186e-07\n",
      "Epoch 15: early stopping\n",
      "Restoring model weights from the end of the best epoch: 5.\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step\n",
      "\n",
      "Training fold 5\n",
      "\n",
      "Generating upsampling using SMOTE...\n",
      "200000 samples after upsampling.\n",
      "Class distribution for training after upsampling: Counter({np.int64(0): 6250, np.int64(1): 6250, np.int64(2): 6250, np.int64(3): 6250, np.int64(4): 6250, np.int64(5): 6250, np.int64(6): 6250, np.int64(7): 6250, np.int64(8): 6250, np.int64(9): 6250, np.int64(10): 6250, np.int64(11): 6250, np.int64(12): 6250, np.int64(13): 6250, np.int64(14): 6250, np.int64(15): 6250, np.int64(16): 6250, np.int64(17): 6250, np.int64(18): 6250, np.int64(19): 6250, np.int64(20): 6250, np.int64(21): 6250, np.int64(22): 6250, np.int64(23): 6250, np.int64(24): 6250, np.int64(25): 6250, np.int64(26): 6250, np.int64(27): 6250, np.int64(28): 6250, np.int64(29): 6250, np.int64(30): 6250, np.int64(31): 6250})\n",
      "Finishing upsampling.\n",
      "\n",
      "Epoch 1/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m148s\u001b[0m 24ms/step - accuracy: 0.9923 - loss: 0.0292 - val_accuracy: 1.0000 - val_loss: 3.0956e-06\n",
      "Epoch 2/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 23ms/step - accuracy: 0.9977 - loss: 0.0081 - val_accuracy: 1.0000 - val_loss: 2.7597e-08\n",
      "Epoch 3/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 23ms/step - accuracy: 0.9982 - loss: 0.0056 - val_accuracy: 1.0000 - val_loss: 1.2863e-07\n",
      "Epoch 4/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 23ms/step - accuracy: 0.9983 - loss: 0.0063 - val_accuracy: 1.0000 - val_loss: 6.4095e-05\n",
      "Epoch 5/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 23ms/step - accuracy: 0.9986 - loss: 0.0047 - val_accuracy: 0.0845 - val_loss: 7.3857\n",
      "Epoch 6/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 23ms/step - accuracy: 0.9988 - loss: 0.0041 - val_accuracy: 1.0000 - val_loss: 1.9443e-07\n",
      "Epoch 7/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m145s\u001b[0m 23ms/step - accuracy: 0.9989 - loss: 0.0038 - val_accuracy: 1.0000 - val_loss: 1.3455e-06\n",
      "Epoch 8/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m153s\u001b[0m 25ms/step - accuracy: 0.9989 - loss: 0.0042 - val_accuracy: 0.9935 - val_loss: 0.0194\n",
      "Epoch 9/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m145s\u001b[0m 24ms/step - accuracy: 0.9988 - loss: 0.0045 - val_accuracy: 1.0000 - val_loss: 1.3547e-04\n",
      "Epoch 10/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m160s\u001b[0m 26ms/step - accuracy: 0.9990 - loss: 0.0033 - val_accuracy: 0.9070 - val_loss: 0.3159\n",
      "Epoch 11/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m164s\u001b[0m 27ms/step - accuracy: 0.9992 - loss: 0.0030 - val_accuracy: 0.2990 - val_loss: 4.1127\n",
      "Epoch 12/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m149s\u001b[0m 24ms/step - accuracy: 0.9990 - loss: 0.0040 - val_accuracy: 0.5945 - val_loss: 2.0037\n",
      "Epoch 12: early stopping\n",
      "Restoring model weights from the end of the best epoch: 2.\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step\n",
      "\n",
      "Training fold 6\n",
      "\n",
      "Generating upsampling using SMOTE...\n",
      "200000 samples after upsampling.\n",
      "Class distribution for training after upsampling: Counter({np.int64(0): 6250, np.int64(1): 6250, np.int64(2): 6250, np.int64(3): 6250, np.int64(4): 6250, np.int64(5): 6250, np.int64(6): 6250, np.int64(7): 6250, np.int64(8): 6250, np.int64(9): 6250, np.int64(10): 6250, np.int64(11): 6250, np.int64(12): 6250, np.int64(13): 6250, np.int64(14): 6250, np.int64(15): 6250, np.int64(16): 6250, np.int64(17): 6250, np.int64(18): 6250, np.int64(19): 6250, np.int64(20): 6250, np.int64(21): 6250, np.int64(22): 6250, np.int64(23): 6250, np.int64(24): 6250, np.int64(25): 6250, np.int64(26): 6250, np.int64(27): 6250, np.int64(28): 6250, np.int64(29): 6250, np.int64(30): 6250, np.int64(31): 6250})\n",
      "Finishing upsampling.\n",
      "\n",
      "Epoch 1/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m156s\u001b[0m 25ms/step - accuracy: 0.9952 - loss: 0.0162 - val_accuracy: 0.9810 - val_loss: 0.0367\n",
      "Epoch 2/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m155s\u001b[0m 25ms/step - accuracy: 0.9979 - loss: 0.0066 - val_accuracy: 0.9985 - val_loss: 0.0046\n",
      "Epoch 3/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m154s\u001b[0m 25ms/step - accuracy: 0.9986 - loss: 0.0056 - val_accuracy: 1.0000 - val_loss: 8.5831e-08\n",
      "Epoch 4/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m157s\u001b[0m 25ms/step - accuracy: 0.9986 - loss: 0.0047 - val_accuracy: 1.0000 - val_loss: 5.9605e-11\n",
      "Epoch 5/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m157s\u001b[0m 25ms/step - accuracy: 0.9985 - loss: 0.0049 - val_accuracy: 1.0000 - val_loss: 7.3866e-06\n",
      "Epoch 6/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m157s\u001b[0m 25ms/step - accuracy: 0.9986 - loss: 0.0042 - val_accuracy: 0.9670 - val_loss: 0.1259\n",
      "Epoch 7/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m158s\u001b[0m 25ms/step - accuracy: 0.9990 - loss: 0.0033 - val_accuracy: 1.0000 - val_loss: 1.7197e-06\n",
      "Epoch 8/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m160s\u001b[0m 26ms/step - accuracy: 0.9988 - loss: 0.0045 - val_accuracy: 1.0000 - val_loss: 3.9448e-07\n",
      "Epoch 9/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m162s\u001b[0m 26ms/step - accuracy: 0.9988 - loss: 0.0042 - val_accuracy: 0.8940 - val_loss: 0.3662\n",
      "Epoch 10/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m163s\u001b[0m 26ms/step - accuracy: 0.9988 - loss: 0.0042 - val_accuracy: 1.0000 - val_loss: 2.6939e-04\n",
      "Epoch 11/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m161s\u001b[0m 26ms/step - accuracy: 0.9991 - loss: 0.0031 - val_accuracy: 1.0000 - val_loss: 9.4652e-08\n",
      "Epoch 12/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m163s\u001b[0m 26ms/step - accuracy: 0.9991 - loss: 0.0031 - val_accuracy: 1.0000 - val_loss: 4.3398e-07\n",
      "Epoch 13/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m168s\u001b[0m 27ms/step - accuracy: 0.9988 - loss: 0.0054 - val_accuracy: 1.0000 - val_loss: 1.1798e-06\n",
      "Epoch 14/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m168s\u001b[0m 27ms/step - accuracy: 0.9990 - loss: 0.0038 - val_accuracy: 1.0000 - val_loss: 2.7421e-06\n",
      "Epoch 14: early stopping\n",
      "Restoring model weights from the end of the best epoch: 4.\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step\n",
      "\n",
      "Training fold 7\n",
      "\n",
      "Generating upsampling using SMOTE...\n",
      "200000 samples after upsampling.\n",
      "Class distribution for training after upsampling: Counter({np.int64(0): 6250, np.int64(1): 6250, np.int64(2): 6250, np.int64(3): 6250, np.int64(4): 6250, np.int64(5): 6250, np.int64(6): 6250, np.int64(7): 6250, np.int64(8): 6250, np.int64(9): 6250, np.int64(10): 6250, np.int64(11): 6250, np.int64(12): 6250, np.int64(13): 6250, np.int64(14): 6250, np.int64(15): 6250, np.int64(16): 6250, np.int64(17): 6250, np.int64(18): 6250, np.int64(19): 6250, np.int64(20): 6250, np.int64(21): 6250, np.int64(22): 6250, np.int64(23): 6250, np.int64(24): 6250, np.int64(25): 6250, np.int64(26): 6250, np.int64(27): 6250, np.int64(28): 6250, np.int64(29): 6250, np.int64(30): 6250, np.int64(31): 6250})\n",
      "Finishing upsampling.\n",
      "\n",
      "Epoch 1/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m179s\u001b[0m 29ms/step - accuracy: 0.9946 - loss: 0.0188 - val_accuracy: 1.0000 - val_loss: 2.3842e-10\n",
      "Epoch 2/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m170s\u001b[0m 28ms/step - accuracy: 0.9981 - loss: 0.0063 - val_accuracy: 0.9790 - val_loss: 0.0632\n",
      "Epoch 3/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m169s\u001b[0m 27ms/step - accuracy: 0.9986 - loss: 0.0047 - val_accuracy: 1.0000 - val_loss: 0.0014\n",
      "Epoch 4/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m172s\u001b[0m 28ms/step - accuracy: 0.9985 - loss: 0.0056 - val_accuracy: 1.0000 - val_loss: 9.5367e-10\n",
      "Epoch 5/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m176s\u001b[0m 28ms/step - accuracy: 0.9989 - loss: 0.0036 - val_accuracy: 1.0000 - val_loss: 6.9906e-07\n",
      "Epoch 6/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m176s\u001b[0m 28ms/step - accuracy: 0.9990 - loss: 0.0033 - val_accuracy: 1.0000 - val_loss: 4.8755e-06\n",
      "Epoch 7/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m179s\u001b[0m 29ms/step - accuracy: 0.9989 - loss: 0.0039 - val_accuracy: 1.0000 - val_loss: 2.1362e-07\n",
      "Epoch 8/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m180s\u001b[0m 29ms/step - accuracy: 0.9988 - loss: 0.0037 - val_accuracy: 0.9730 - val_loss: 0.1226\n",
      "Epoch 9/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m181s\u001b[0m 29ms/step - accuracy: 0.9991 - loss: 0.0035 - val_accuracy: 1.0000 - val_loss: 0.0029\n",
      "Epoch 10/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m184s\u001b[0m 30ms/step - accuracy: 0.9989 - loss: 0.0039 - val_accuracy: 1.0000 - val_loss: 9.1255e-08\n",
      "Epoch 11/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m184s\u001b[0m 30ms/step - accuracy: 0.9991 - loss: 0.0035 - val_accuracy: 0.9915 - val_loss: 0.0124\n",
      "Epoch 11: early stopping\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step\n",
      "\n",
      "Training fold 8\n",
      "\n",
      "Generating upsampling using SMOTE...\n",
      "200000 samples after upsampling.\n",
      "Class distribution for training after upsampling: Counter({np.int64(0): 6250, np.int64(1): 6250, np.int64(2): 6250, np.int64(3): 6250, np.int64(4): 6250, np.int64(5): 6250, np.int64(6): 6250, np.int64(7): 6250, np.int64(8): 6250, np.int64(9): 6250, np.int64(10): 6250, np.int64(11): 6250, np.int64(12): 6250, np.int64(13): 6250, np.int64(14): 6250, np.int64(15): 6250, np.int64(16): 6250, np.int64(17): 6250, np.int64(18): 6250, np.int64(19): 6250, np.int64(20): 6250, np.int64(21): 6250, np.int64(22): 6250, np.int64(23): 6250, np.int64(24): 6250, np.int64(25): 6250, np.int64(26): 6250, np.int64(27): 6250, np.int64(28): 6250, np.int64(29): 6250, np.int64(30): 6250, np.int64(31): 6250})\n",
      "Finishing upsampling.\n",
      "\n",
      "Epoch 1/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m193s\u001b[0m 31ms/step - accuracy: 0.9970 - loss: 0.0102 - val_accuracy: 0.7090 - val_loss: 0.9593\n",
      "Epoch 2/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m189s\u001b[0m 31ms/step - accuracy: 0.9986 - loss: 0.0051 - val_accuracy: 1.0000 - val_loss: 8.8238e-05\n",
      "Epoch 3/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m190s\u001b[0m 31ms/step - accuracy: 0.9987 - loss: 0.0047 - val_accuracy: 0.9365 - val_loss: 0.2493\n",
      "Epoch 4/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m190s\u001b[0m 31ms/step - accuracy: 0.9989 - loss: 0.0041 - val_accuracy: 1.0000 - val_loss: 6.2703e-08\n",
      "Epoch 5/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m188s\u001b[0m 30ms/step - accuracy: 0.9987 - loss: 0.0041 - val_accuracy: 1.0000 - val_loss: 1.5229e-06\n",
      "Epoch 6/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m190s\u001b[0m 31ms/step - accuracy: 0.9991 - loss: 0.0035 - val_accuracy: 1.0000 - val_loss: 1.1265e-08\n",
      "Epoch 7/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m190s\u001b[0m 31ms/step - accuracy: 0.9991 - loss: 0.0037 - val_accuracy: 0.9690 - val_loss: 0.1441\n",
      "Epoch 8/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m191s\u001b[0m 31ms/step - accuracy: 0.9988 - loss: 0.0052 - val_accuracy: 1.0000 - val_loss: 1.4413e-04\n",
      "Epoch 9/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m192s\u001b[0m 31ms/step - accuracy: 0.9992 - loss: 0.0029 - val_accuracy: 0.9875 - val_loss: 0.0421\n",
      "Epoch 10/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m195s\u001b[0m 32ms/step - accuracy: 0.9991 - loss: 0.0032 - val_accuracy: 0.9925 - val_loss: 0.0199\n",
      "Epoch 11/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m197s\u001b[0m 32ms/step - accuracy: 0.9991 - loss: 0.0033 - val_accuracy: 0.9515 - val_loss: 0.2253\n",
      "Epoch 12/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m197s\u001b[0m 32ms/step - accuracy: 0.9991 - loss: 0.0034 - val_accuracy: 0.9560 - val_loss: 0.1043\n",
      "Epoch 13/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m200s\u001b[0m 32ms/step - accuracy: 0.9992 - loss: 0.0032 - val_accuracy: 1.0000 - val_loss: 2.8085e-07\n",
      "Epoch 14/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m202s\u001b[0m 33ms/step - accuracy: 0.9994 - loss: 0.0023 - val_accuracy: 0.9935 - val_loss: 0.0176\n",
      "Epoch 15/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m205s\u001b[0m 33ms/step - accuracy: 0.9991 - loss: 0.0031 - val_accuracy: 1.0000 - val_loss: 1.4195e-06\n",
      "Epoch 16/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m213s\u001b[0m 34ms/step - accuracy: 0.9992 - loss: 0.0032 - val_accuracy: 1.0000 - val_loss: 8.0478e-05\n",
      "Epoch 16: early stopping\n",
      "Restoring model weights from the end of the best epoch: 6.\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step\n",
      "\n",
      "Training fold 9\n",
      "\n",
      "Generating upsampling using SMOTE...\n",
      "200000 samples after upsampling.\n",
      "Class distribution for training after upsampling: Counter({np.int64(0): 6250, np.int64(1): 6250, np.int64(2): 6250, np.int64(3): 6250, np.int64(4): 6250, np.int64(5): 6250, np.int64(6): 6250, np.int64(7): 6250, np.int64(8): 6250, np.int64(9): 6250, np.int64(10): 6250, np.int64(11): 6250, np.int64(12): 6250, np.int64(13): 6250, np.int64(14): 6250, np.int64(15): 6250, np.int64(16): 6250, np.int64(17): 6250, np.int64(18): 6250, np.int64(19): 6250, np.int64(20): 6250, np.int64(21): 6250, np.int64(22): 6250, np.int64(23): 6250, np.int64(24): 6250, np.int64(25): 6250, np.int64(26): 6250, np.int64(27): 6250, np.int64(28): 6250, np.int64(29): 6250, np.int64(30): 6250, np.int64(31): 6250})\n",
      "Finishing upsampling.\n",
      "\n",
      "Epoch 1/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m216s\u001b[0m 35ms/step - accuracy: 0.9954 - loss: 0.0159 - val_accuracy: 1.0000 - val_loss: 0.0013\n",
      "Epoch 2/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m213s\u001b[0m 34ms/step - accuracy: 0.9985 - loss: 0.0049 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 3/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m213s\u001b[0m 34ms/step - accuracy: 0.9987 - loss: 0.0053 - val_accuracy: 1.0000 - val_loss: 4.8280e-09\n",
      "Epoch 4/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m217s\u001b[0m 35ms/step - accuracy: 0.9988 - loss: 0.0037 - val_accuracy: 1.0000 - val_loss: 2.2883e-05\n",
      "Epoch 5/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m221s\u001b[0m 36ms/step - accuracy: 0.9990 - loss: 0.0036 - val_accuracy: 1.0000 - val_loss: 4.6684e-05\n",
      "Epoch 6/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m219s\u001b[0m 35ms/step - accuracy: 0.9991 - loss: 0.0028 - val_accuracy: 1.0000 - val_loss: 3.9499e-07\n",
      "Epoch 7/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m219s\u001b[0m 35ms/step - accuracy: 0.9990 - loss: 0.0031 - val_accuracy: 1.0000 - val_loss: 1.9845e-06\n",
      "Epoch 8/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m216s\u001b[0m 35ms/step - accuracy: 0.9992 - loss: 0.0030 - val_accuracy: 1.0000 - val_loss: 3.9867e-05\n",
      "Epoch 9/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m216s\u001b[0m 35ms/step - accuracy: 0.9993 - loss: 0.0026 - val_accuracy: 1.0000 - val_loss: 1.2690e-07\n",
      "Epoch 10/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m218s\u001b[0m 35ms/step - accuracy: 0.9991 - loss: 0.0031 - val_accuracy: 1.0000 - val_loss: 1.8623e-05\n",
      "Epoch 11/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m219s\u001b[0m 35ms/step - accuracy: 0.9991 - loss: 0.0029 - val_accuracy: 0.9995 - val_loss: 0.0062\n",
      "Epoch 12/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m220s\u001b[0m 35ms/step - accuracy: 0.9992 - loss: 0.0029 - val_accuracy: 1.0000 - val_loss: 8.4633e-07\n",
      "Epoch 12: early stopping\n",
      "Restoring model weights from the end of the best epoch: 2.\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step\n",
      "\n",
      "Training fold 10\n",
      "\n",
      "Generating upsampling using SMOTE...\n",
      "200000 samples after upsampling.\n",
      "Class distribution for training after upsampling: Counter({np.int64(0): 6250, np.int64(1): 6250, np.int64(2): 6250, np.int64(3): 6250, np.int64(4): 6250, np.int64(5): 6250, np.int64(6): 6250, np.int64(7): 6250, np.int64(8): 6250, np.int64(9): 6250, np.int64(10): 6250, np.int64(11): 6250, np.int64(12): 6250, np.int64(13): 6250, np.int64(14): 6250, np.int64(15): 6250, np.int64(16): 6250, np.int64(17): 6250, np.int64(18): 6250, np.int64(19): 6250, np.int64(20): 6250, np.int64(21): 6250, np.int64(22): 6250, np.int64(23): 6250, np.int64(24): 6250, np.int64(25): 6250, np.int64(26): 6250, np.int64(27): 6250, np.int64(28): 6250, np.int64(29): 6250, np.int64(30): 6250, np.int64(31): 6250})\n",
      "Finishing upsampling.\n",
      "\n",
      "Epoch 1/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m230s\u001b[0m 37ms/step - accuracy: 0.9977 - loss: 0.0088 - val_accuracy: 1.0000 - val_loss: 1.0736e-06\n",
      "Epoch 2/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m227s\u001b[0m 37ms/step - accuracy: 0.9988 - loss: 0.0045 - val_accuracy: 1.0000 - val_loss: 6.8283e-05\n",
      "Epoch 3/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m226s\u001b[0m 37ms/step - accuracy: 0.9988 - loss: 0.0038 - val_accuracy: 1.0000 - val_loss: 0.0142\n",
      "Epoch 4/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m226s\u001b[0m 37ms/step - accuracy: 0.9989 - loss: 0.0037 - val_accuracy: 1.0000 - val_loss: 1.1545e-07\n",
      "Epoch 5/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m227s\u001b[0m 37ms/step - accuracy: 0.9989 - loss: 0.0043 - val_accuracy: 1.0000 - val_loss: 5.9605e-11\n",
      "Epoch 6/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m261s\u001b[0m 36ms/step - accuracy: 0.9992 - loss: 0.0031 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 7/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m230s\u001b[0m 37ms/step - accuracy: 0.9990 - loss: 0.0033 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 8/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m234s\u001b[0m 38ms/step - accuracy: 0.9991 - loss: 0.0028 - val_accuracy: 1.0000 - val_loss: 7.0983e-06\n",
      "Epoch 9/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m233s\u001b[0m 38ms/step - accuracy: 0.9993 - loss: 0.0024 - val_accuracy: 1.0000 - val_loss: 5.2094e-08\n",
      "Epoch 10/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m234s\u001b[0m 38ms/step - accuracy: 0.9993 - loss: 0.0021 - val_accuracy: 1.0000 - val_loss: 1.8076e-05\n",
      "Epoch 11/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m232s\u001b[0m 38ms/step - accuracy: 0.9992 - loss: 0.0032 - val_accuracy: 1.0000 - val_loss: 1.6453e-04\n",
      "Epoch 12/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m237s\u001b[0m 38ms/step - accuracy: 0.9994 - loss: 0.0022 - val_accuracy: 1.0000 - val_loss: 2.8014e-09\n",
      "Epoch 13/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m237s\u001b[0m 38ms/step - accuracy: 0.9993 - loss: 0.0023 - val_accuracy: 1.0000 - val_loss: 2.5703e-05\n",
      "Epoch 14/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m240s\u001b[0m 39ms/step - accuracy: 0.9994 - loss: 0.0027 - val_accuracy: 1.0000 - val_loss: 3.3723e-07\n",
      "Epoch 15/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m241s\u001b[0m 39ms/step - accuracy: 0.9994 - loss: 0.0024 - val_accuracy: 0.9680 - val_loss: 0.0982\n",
      "Epoch 16/300\n",
      "\u001b[1m6188/6188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m243s\u001b[0m 39ms/step - accuracy: 0.9992 - loss: 0.0030 - val_accuracy: 1.0000 - val_loss: 3.5727e-06\n",
      "Epoch 16: early stopping\n",
      "Restoring model weights from the end of the best epoch: 6.\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step\n",
      "\n",
      "Time taken for training:  07:35:41\n",
      "\n",
      "\n",
      "Fold 1 - Train Accuracy 0.9975 - Test Accuracy 0.6016\n",
      "Fold 2 - Train Accuracy 0.9975 - Test Accuracy 0.7868\n",
      "Fold 3 - Train Accuracy 0.9994 - Test Accuracy 0.7313\n",
      "Fold 4 - Train Accuracy 0.9994 - Test Accuracy 0.9150\n",
      "Fold 5 - Train Accuracy 0.9993 - Test Accuracy 0.9581\n",
      "Fold 6 - Train Accuracy 0.9993 - Test Accuracy 0.9553\n",
      "Fold 7 - Train Accuracy 0.9991 - Test Accuracy 0.9724\n",
      "Fold 8 - Train Accuracy 0.9993 - Test Accuracy 0.9526\n",
      "Fold 9 - Train Accuracy 0.9994 - Test Accuracy 0.9767\n",
      "Fold 10 - Train Accuracy 0.9993 - Test Accuracy 0.9670\n",
      "\n",
      "Mean Train Accuracy: 0.9990 - Std: 0.0007 \n",
      "Mean Test Accuracy: 0.8817 - Std: 0.1233 \n",
      "\n",
      "Evaluate other metrics:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.94      0.92     13905\n",
      "           1       0.94      0.91      0.93       723\n",
      "           2       0.92      0.96      0.94      2659\n",
      "           3       0.80      0.69      0.74      1123\n",
      "           4       0.85      0.68      0.76       384\n",
      "           5       0.92      0.75      0.82        44\n",
      "           6       0.83      0.81      0.82       663\n",
      "           7       0.88      0.79      0.83        94\n",
      "           8       0.91      0.83      0.87        12\n",
      "           9       0.84      0.77      0.80       786\n",
      "          10       0.58      0.78      0.67         9\n",
      "          11       0.89      0.71      0.79        98\n",
      "          12       0.67      0.75      0.71         8\n",
      "          13       0.88      0.70      0.78        20\n",
      "          14       0.92      0.71      0.80        77\n",
      "          15       0.90      0.89      0.89        62\n",
      "          16       0.84      0.67      0.74       917\n",
      "          17       0.93      0.84      0.89       473\n",
      "          18       0.90      0.82      0.86        22\n",
      "          19       0.83      0.75      0.79       122\n",
      "          20       0.87      0.73      0.79       111\n",
      "          21       0.81      0.76      0.78       201\n",
      "          22       0.50      0.86      0.63         7\n",
      "          23       0.88      0.76      0.82        96\n",
      "          24       0.79      0.73      0.76      1045\n",
      "          25       0.80      0.77      0.78       540\n",
      "          26       0.75      0.82      0.78      1334\n",
      "          27       0.91      0.75      0.82        28\n",
      "          28       0.97      0.80      0.88        35\n",
      "          29       0.80      0.74      0.77        77\n",
      "          30       0.94      0.77      0.84        64\n",
      "          31       1.00      0.71      0.83         7\n",
      "\n",
      "    accuracy                           0.88     25746\n",
      "   macro avg       0.85      0.78      0.81     25746\n",
      "weighted avg       0.88      0.88      0.88     25746\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Build a \"time window\" structure to handle the dataset as a time series.\n",
    "new_df = load_dataset(\"V6\")\n",
    "X_array, y_array = build_time_window_structure(new_df, \"V6\")\n",
    "X_array, y_array = remove_classes_with_less_samples(X_array, y_array)\n",
    "\n",
    "# Train the CNN model.\n",
    "v1_model = create_v1()\n",
    "v1_num_epochs = 300\n",
    "v1_batch_size = 32\n",
    "v1_validation_split = 0.01\n",
    "\n",
    "training_history_v1 = train_cnn_model(v1_model, X_array, y_array, v1_num_epochs, v1_batch_size, v1_validation_split, \"v1_model_V6.keras\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
